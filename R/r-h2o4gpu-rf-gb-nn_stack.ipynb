{"metadata":{"kernelspec":{"name":"ir","display_name":"R","language":"R"},"language_info":{"name":"R","codemirror_mode":"r","pygments_lexer":"r","mimetype":"text/x-r-source","file_extension":".r","version":"4.0.5"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# September Tabular Competition\n\n\nThe dataset for this month incorporates more than 1.45 million rows of data. Since some of that data is missing - 936218 elements from the test set and 1820782 from train set, there is an option to fill those NA values. To underline how these missing values impact the dataset as a whole:\n\n\n$N_{joint}$ = $\\texttt{number of elements in the joint set} $\n\n$m_{joint}$ = $\\texttt{number of missing elements in the joint set} $\n\n\\begin{equation}\n    \\frac{m_{joint}}{N_{joint}} \\,= \\frac{sum(is.na(test)) + sum(is.na(train))}{118 * nrow(train + test)} \\, = \\frac{2757000}{171264374} \\, = 0.016098 \\\\\n\\end{equation}\n\n\nMissing cells make up a little more than 1.6% of joint datasets. Due to the nature of how missing cells affect whole rows, trimmed dataset has only 545157 rows (37.5% of original size).\n\nGeneral idea for this notebook is preprocessing, exploring missing values (how many missing values are in a row), filling them and creating a neural network / rf for classification.\n\nSome features in the dataset consist of huge numbers and are in need of transformation before imputing missing values.\n\n\n\n\nSince the dataset is massive, control of RAM usage is very important.","metadata":{}},{"cell_type":"code","source":"#Loading in libraries\npacman::p_load(tidyverse, dplyr, keras, tensorflow, ggplot2, ggthemes, fastDummies, janitor, cowplot,\n                repr, BBmisc, devtools, reticulate, Rcpp,  cvAUC, tictoc)\n\n#Loading in data\ndf_train <- read.csv(\"../input/tabular-playground-series-sep-2021/train.csv\")\ndf_test <- read.csv(\"../input/tabular-playground-series-sep-2021/test.csv\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-27T22:40:16.756571Z","iopub.execute_input":"2021-09-27T22:40:16.758458Z","iopub.status.idle":"2021-09-27T22:43:26.507306Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"py_discover_config()","metadata":{"execution":{"iopub.status.busy":"2021-09-27T22:43:26.510506Z","iopub.execute_input":"2021-09-27T22:43:26.537158Z","iopub.status.idle":"2021-09-27T22:43:27.842791Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"python:         /miniconda/envs/r-reticulate/bin/python\nlibpython:      /miniconda/envs/r-reticulate/lib/libpython3.7m.so\npythonhome:     /miniconda/envs/r-reticulate:/miniconda/envs/r-reticulate\nversion:        3.7.11 (default, Jul 27 2021, 14:32:16)  [GCC 7.5.0]\nnumpy:          /miniconda/envs/r-reticulate/lib/python3.7/site-packages/numpy\nnumpy_version:  1.18.5\n\npython versions found: \n /miniconda/envs/r-reticulate/bin/python\n /usr/bin/python3\n /usr/bin/python"},"metadata":{}}]},{"cell_type":"markdown","source":"'h2o4gpu' package requires an installed python module. First the package is downloaded using a  *install_github()* function from devtools and then python module is installed using a *py_install()* function from the reticulate package.","metadata":{}},{"cell_type":"code","source":"#Loading in h2o4gpu package\ndevtools::install_github(\"h2oai/h2o4gpu\", subdir = \"src/interface_r\")\nlibrary(h2o4gpu)","metadata":{"execution":{"iopub.status.busy":"2021-09-27T22:43:27.845638Z","iopub.execute_input":"2021-09-27T22:43:27.846881Z","iopub.status.idle":"2021-09-27T22:44:41.719470Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"Downloading GitHub repo h2oai/h2o4gpu@HEAD\n\n'/usr/bin/git' clone --depth 1 --no-hardlinks --recurse-submodules https://github.com/h2oai/cub /tmp/RtmpziXP6j/remotesf6fdf36ef/h2oai-h2o4gpu-aaf7795/src/interface_r/../../cub\n\n'/usr/bin/git' clone --depth 1 --no-hardlinks --recurse-submodules https://github.com/h2oai/xgboost /tmp/RtmpziXP6j/remotesf6fdf36ef/h2oai-h2o4gpu-aaf7795/src/interface_r/../../xgboost\n\n'/usr/bin/git' clone --depth 1 --no-hardlinks --recurse-submodules https://github.com/h2oai/py3nvml /tmp/RtmpziXP6j/remotesf6fdf36ef/h2oai-h2o4gpu-aaf7795/src/interface_r/../../py3nvml\n\n'/usr/bin/git' clone --depth 1 --no-hardlinks --recurse-submodules https://github.com/h2oai/scikit-learn.git /tmp/RtmpziXP6j/remotesf6fdf36ef/h2oai-h2o4gpu-aaf7795/src/interface_r/../../scikit-learn\n\n'/usr/bin/git' clone --depth 1 --no-hardlinks --recurse-submodules https://github.com/google/googletest.git /tmp/RtmpziXP6j/remotesf6fdf36ef/h2oai-h2o4gpu-aaf7795/src/interface_r/../../tests/googletest\n\n'/usr/bin/git' clone --depth 1 --no-hardlinks --recurse-submodules https://github.com/h2oai/LightGBM.git /tmp/RtmpziXP6j/remotesf6fdf36ef/h2oai-h2o4gpu-aaf7795/src/interface_r/../../LightGBM\n\n'/usr/bin/git' clone --depth 1 --no-hardlinks --recurse-submodules https://github.com/NVIDIA/nccl.git /tmp/RtmpziXP6j/remotesf6fdf36ef/h2oai-h2o4gpu-aaf7795/src/interface_r/../../nccl\n\n'/usr/bin/git' clone --depth 1 --no-hardlinks --recurse-submodules https://github.com/h2oai/xgboost.git /tmp/RtmpziXP6j/remotesf6fdf36ef/h2oai-h2o4gpu-aaf7795/src/interface_r/../../xgboost_prev\n\n","output_type":"stream"},{"name":"stdout","text":"\n\u001b[32m✔\u001b[39m  \u001b[90mchecking for file ‘/tmp/RtmpziXP6j/remotesf6fdf36ef/h2oai-h2o4gpu-aaf7795/src/interface_r/DESCRIPTION’\u001b[39m\u001b[36m\u001b[39m\n\u001b[90m─\u001b[39m\u001b[90m  \u001b[39m\u001b[90mpreparing ‘h2o4gpu’:\u001b[39m\u001b[36m\u001b[39m\n\u001b[32m✔\u001b[39m  \u001b[90mchecking DESCRIPTION meta-information\u001b[39m\u001b[36m\u001b[39m\n\u001b[90m─\u001b[39m\u001b[90m  \u001b[39m\u001b[90mchecking for LF line-endings in source and make files and shell scripts\u001b[39m\u001b[36m\u001b[39m\n\u001b[90m─\u001b[39m\u001b[90m  \u001b[39m\u001b[90mchecking for empty or unneeded directories\u001b[39m\u001b[36m\u001b[39m\n\u001b[90m─\u001b[39m\u001b[90m  \u001b[39m\u001b[90mbuilding ‘h2o4gpu_0.3.3.tar.gz’\u001b[39m\u001b[36m\u001b[39m\n   \n\r","output_type":"stream"},{"name":"stderr","text":"Installing package into ‘/usr/local/lib/R/site-library’\n(as ‘lib’ is unspecified)\n\n\nAttaching package: ‘h2o4gpu’\n\n\nThe following object is masked from ‘package:keras’:\n\n    fit\n\n\nThe following object is masked from ‘package:base’:\n\n    transform\n\n\n","output_type":"stream"}]},{"cell_type":"code","source":"#Installing python module for h2o4gpu\npy_install(\"h2o4gpu\",\n          pip = TRUE)","metadata":{"execution":{"iopub.status.busy":"2021-09-27T22:44:41.722754Z","iopub.execute_input":"2021-09-27T22:44:41.724174Z","iopub.status.idle":"2021-09-27T22:45:13.945490Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Data exploration","metadata":{}},{"cell_type":"code","source":"#Piechart\ndf_pie <- data.frame(claim = c(sum(df_train$claim), (length(df_train$claim) - sum(df_train$claim))), claim_group = c(1,0))\n\ndf_pie <- df_pie %>%\n    mutate(claim_group = as.factor(claim_group))\n\n#Label positions\ndf_pie<- df_pie %>% \n  arrange(desc(claim_group)) %>%\n  mutate(prop = claim / sum(df_pie$claim) *100) %>%\n  mutate(ypos = cumsum(prop)- 0.5*prop)\n\ndf_pie <- df_pie %>%\ncbind(roundup = round(df_pie$prop, digits = 2))\n\n\noptions(repr.plot.width = 9, repr.plot.height = 7)\n# Basic piechart\nggplot(df_pie, aes(x =\" \", y = prop, fill = claim_group)) +\n  geom_bar(stat=\"identity\", width=2, color=\"white\") +\n  coord_polar(\"y\", start=0) +\n  theme_void() + \n  geom_text(aes(y = ypos), label = paste(as.character(df_pie$roundup), \"%\", sep=\" \"), color = \"white\", size = 8) +\n  scale_fill_brewer(palette = \"Pastel1\") +\n theme(legend.key.size = unit(1, 'cm'), \n        legend.key.height = unit(1, 'cm'), \n        legend.key.width = unit(1, 'cm'), \n        legend.title = element_text(size = 12), \n        legend.text = element_text(size = 10))\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-09-27T22:45:13.948934Z","iopub.execute_input":"2021-09-27T22:45:13.950561Z","iopub.status.idle":"2021-09-27T22:45:14.449713Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAABDgAAANICAIAAACt98iXAAAABmJLR0QA/wD/AP+gvaeTAAAg\nAElEQVR4nOzdZ5hkZ2Hm/eecip1zmBw1ozAajTQSSCAkYUyQsQHjxWCTzDrwshgwr8Hv5bT2\nmsWYl2UBB2wTdmVswBHsJRuMskajCZocuqdzrpzjOc+zHzrOTKfqru5zTtX/d/GhXV3hrrG6\nqu56kqaUEgAAAABgJ7rVAQAAAADgRhQVAAAAALZDUQEAAABgOxQVAAAAALZDUQEAAABgOxQV\nAAAAALZDUQEAAABgOxQVAAAAALZDUQEAAABgOxQVAAAAALZDUQEAAABgOxQVAAAAALZDUQEA\nAABgOxQVAAAAALZDUQEAAABgOxQVAAAAALZDUQEAAABgOxQVAAAAALZDUQEAAABgOxQVAAAA\nALZDUQEAAABgOxQVAAAAALZDUQEAAABgOxQVAAAAALZDUQEAAABgOxQVAAAAALZDUQEAAABg\nOxQVAAAAALZDUQEAAABgOxQVAAAAALZDUQEAAABgOxQVAAAAALZDUQEAAABgOxQVAAAAALZD\nUQEAAABgOxQVAAAAALZDUQEAAABgOxQVAAAAALZDUQEAAABgOxQVAAAAALZDUQEAAABgOxQV\nAAAAALZDUQEAAABgOxQVAAAAALZDUQEAAABgOxQVAAAAALZDUQEAAABgOxQVAAAAALZDUQEA\nAABgOxQVAAAAALZDUQEAAABgOxQVAAAAALZDUQEAAABgOxQVAAAAALZDUQEAAABgOxQVAAAA\nALZDUQEAAABgOxQVAAAAALZDUQEAAABgOxQVAAAAALZDUQEAAABgOxQVAAAAALZDUQEAAABg\nOxQVAAAAALZDUQEAAABgOxQVAAAAALZDUQEAAABgOxQVAAAAALZDUQEAAABgOxQVAAAAALZD\nUQEAAABgOxQVAAAAALZDUQEAAABgOxQVAAAAALZDUQEAAABgOxQVAAAAALZDUQEAAABgOxQV\nAAAAALZDUQEAAABgOxQVAAAAALZDUQEAAABgOxQVAAAAALZDUQEAAABgOxQVAAAAALbjtjoA\nAFQWpYRRVMWiMAxhmsIwhFFUhiFMQxjT/ysKKZVhzF5fCsOc+VlKYRr63lu0rdtNqZ7rCbp0\noWva9C9duq7Pfrnk1nVNEx6X7tY1l0t365rHpbtcmlvXXLrucWlul+7WNV3XNvnZAwBQLhQV\nAChFsaDyeVHIi3xeFfIinxeFgsrnRCEvCgVVyItCYZ2PoAr56XoRz6z3rtwuzed2ed261+3y\nefTZn3Wfx+Wb/WGdDwEAwAahqADATZQSuZzKZkQ2o7JZkcuobFZkMyqbEXMjIU5gmMowjXR+\nySvomub3umq8rlqvu8brqvG6amZ/mBvJAQDAEhQVANVNKZVJi1RKpZMilVLplMpmRC4rlLI6\n2WaQSmXyRiZvhMWNbcbncdV4XXVed73fXef31PvddT63i7lkAIDNQlEBUE0KBZVOqVRSpOea\nSVooaXUsO8oXzXzRjKWvm35W63XX+d31fne9b6a9+Jk8BgDYGBQVAJVrerQkEVeJmIrHVTIu\ncjmrMzlbpmBkCkYwMX+J26U31njq/e4Gv6ep1ttc62EFPwCgLCgqACqIkiqdFvGoisdUMqkS\nsfUvbcfyDFNGUvlIambmmKaJep+nqdbTUOOp93ta6rxeN/vgAwDWgqICwNlUJi2iETX9v2S8\nStaW2JZSIpkrJnPFuUvqfO6WOm9Lnbel3tfg97BEHwCwShQVAE5jmioeU9GIioZVLCLyS+9p\nBRtI54103hiNZIQQbpfWXOttqfdNVxePi8EWAMCSKCoAnKBQUJGQCodULKLicZa/O5RhqlAy\nH0rOdMsGv6elzttS721r8NV6eT8CAFyHNwYAdmUYKhpWoaAKBZnTVZGmJ4kNh9NCiFqfu73B\n197ga2vw+1jWAgCgqACwFylVNKLCQRUOqmiUkZPqkckbw3ljOJQWQjTWeNoafO0N/rZ6n9vF\nohYAqFIUFQDWU+mUmppQoYCKhIVpWh0HFktki4lscSCQ0jTRXOttb/B3Nvmb67xUFgCoKhQV\nABaRUkVCKjCpApMqnbY6DexIKRFNF6LpQu9kwuvWO5v8XY01HY0+N6vwAaAKUFQAbK58Xk6X\nk1BAGIbVaeAYBUOOhjOj4YymaW313q6mms4mf52PdzEAqFi8xAPYDCqZUJPjKjCp4jGWxWM9\nlJrZOuziqKj3u6cbS2udjxNaAKDCUFQAbCCVSqqJMTU+qlJJq7OgAqVyRiqX7JtKet16Z6N/\nS0ttZ6OfxgIAlYGiAqD86CfYZAVDjkYyo5GMz+Pa0lyztaWmpd5HYQEAR6OoACgblUyoiTE1\nMUY/gVXyRXMwmBoMpvwe15aWmi3NNBYAcCqKCoB1y2bl2IgaG6afwD5yRXMgkBoIzDSWHW11\njTUeq0MBAEpAUQGwVqapJsfl6LAKB1kfD9uaaywNfs/2ttrtrbU+j8vqUACAlVFUAJRMxWNq\neECOj7K/MBwkmSteHotfGU90Nvp2tNV1Nvl11t0DgI1RVACslsqk1eiwGhtWmYzVWYA1UkpN\nxXNT8ZzXrW9tqd3RVttU67U6FABgERQVACuRUk2Oy+EBFQ5ZHQUom4Ihp5fdN/g9O9rrdrTV\nejjwHgDshKICYGm5nBwbloP9Ipe1OgqwUZK54qXR2JWxeFezf09HQ2s9AywAYAsUFQA3UUqF\nAnKoXwWmWCWPKiGVmohmJ6LZ5jrvrva6rS21Lp0VLABgJYoKgAUKeTkypIYHVSZtdRTAGrF0\nIZYuXBqL72ir29VeV+fjjRIArMHrLwAhhFDxqBzoUxNjQkqrswDWKxqyfyo5MJVsb/Tv7qjr\nbKpheAUANhlFBah2KhSQg31qatLqIIDtKCGCiVwwkav1ufd01O9sr2M+GABsGooKUK2kVBOj\n8loPx8kDK8rkjYujsd7JxI62uj2d9X6OjASAjUdRAapPIS9HhuRgn8jlrI4COEnBkH1TycFg\nantr7d6uBpavAMCG4kUWqCIqnVL91+TYsDBNq7MATmVKNRRKD4fSXU01e7vqW+t9VicCgMpE\nUQGqgkrEZO9VNTXBdsNAWSghJuPZyXi2uc57S3dDV1ON1YkAoNJQVIAKpxJxee2qmhizOghQ\nmWLpwom+cEONZ19Xw7bWWtbaA0C5UFSAiqWiEdl3le28gE2QzBbPDEb6ppL7uhq2tdRq9BUA\nWDeKClCBVCQse6+oUMDqIEB1masrt3Q3bmnh6BUAWBeKClBRVDgke6+ocNDqIED1SmaLpwfC\nDROe/VsatrYwGQwA1oiiAlQIFYvIKxdVOGR1EABCCJHMFV8ciPROJA5sadzaUmt1HABwHooK\n4HgqlZQ9l1kuD9hQKmecHoj0TiYPdDduaWFnMAAoAUUFcLJcVvZekSNDbDoM2FkyWzw1EG4P\n+W7b1tRU67U6DgA4A0UFcKZiQV7rkUP9HN0IOEUomX/6SmBrS+2tWxtrOdUeAFbCCyXgNKYp\nB/tkX48oFq2OAqBk49HMRCy7q73ulu4Gn8dldRwAsC+KCuAcSsnRIdlzWeRyVkcBsHZKqcFg\najSc3tvVsLezwe1iYzAAWARFBXAGFYvKi+dULGJ1EADlYUjVM5EYCqUPbmnc0V5HWQGAG1BU\nANvLZc2rl9TosNU5AJRfvmieG44OhdKHdjS31LHOHgDmUVQAG5tejnLtqjAMq6MA2EDxTOHZ\nq4HtrbW3bWti4QoATKOoADalJsbMyxdENmN1EACbZDSSmYznbulu2NNZr2vMBQNQ7SgqgO2o\nRFxePKcinDEPVB3DlJfH4sOh9B3bmzub/FbHAQArUVQAOzGK8solOTzAAY5ANUvnjRf6Qp1N\n/kPbmzlxBUDV4uUPsAsVmDQvnBHZrNVBANhCIJ57Mjm1r6thf3cDM8EAVCGKCmADuax58Zya\nHLc6BwB7MaXqmUiMRTKHd7a0NfisjgMAm4qiAlhKKTnUL69eYl8vAEtJ541jvcHtrbW3b2/2\nunWr4wDAJqGoAJZRsag8/6JKxK0OAsABRiOZYDJ/aEfzluYaq7MAwGagqABWMAx59aIcYtE8\ngBLki+ap/nBXk//QjpYaL8etAKhwFBVgs6nApHn+jMixaB7AWkzFc+Hk5IGtTXs66lljD6CC\nUVSATWQY8vJ5OTxodQ4AzmZIdWk0NhHN3LWrtd7PWzmAysSrG7BJVChgnnuRk+YBlEs0XXj6\nytSBLY17uxoYWQFQeSgqwMYzTdl7Rfb3siIFQHmZUl0ei0/Fc3ftaqnjaEgAlYUXNWBjqXBI\nnjulMgykANgokVT+6SuB27c17WyvszoLAJQNRQXYMKYpr1yUg31W5wBQ+QxTnhuOTsazh3e2\n+D1sCAagEnBuFLAhVDRiPv0ftBQAmykQzz15eWoswhAugErAiApQfnKwT166IJS0OgiAqlM0\n5IuDkal47vDOZreLryMBOBhFBSirXNY8c1KFQ1bnAFDVxqOZaDp/9+621nqv1VkAYI34rgUo\nGzU1YTz9Y1oKADvIFsxjvYGeiQTbDQJwKEZUgHKQprxyUQ6wIgWAjSgleiYS4VT+7t2trLAH\n4DiMqADrpZIJ85knaCkA7CmczD91eSoQz1kdBABKQ1EB1kUODZjPPqGSCauDAMCSCoZ8oS90\ncTQmmQcGwDmY+gWslWGYZ0+pyXGrcwDAqgwEUpFU/p49bZxhD8ARGFEB1kKlkuazT9BSADhL\nPFN8+srURCxrdRAAWBlFBSiZGh81n3lcpZJWBwGAkhmmOtUfvjgaU0wDA2BvDP4CpVBKXr0k\n+3qszgEA6zIQSCUyxXv2tPrYDQyAXTGiAqxaLmsee4qWAqAyhFP5p68EoumC1UEAYHEUFWBV\nVDhoPP24ikasDgIAZZMrmsd6ggOBlNVBAGARTP0CVib7euXVi4L53AAqjlTq4mgskS3euaNZ\n1zWr4wDAPIoKsCzTNM+eUhNjVucAgA00Ek4nsoWje9tqvXwwAGAXTP0ClpbLmceepqUAqAbx\nTPGZK4FwKm91EACYQVEBFqcSMeO5J1Q8anUQANgkBUMe7w2NhjNWBwEAIZj6BSxKTYyZZ08J\n07Q6CABsKqnUmaFIPFu4fXszC1YAWIuiAtxI9vXIKxetTgEAlhkIpNJ5457drW4XMy8AWIYX\nIGAB0zRPHaelAEAgnnuuJ5gtMLAMwDIUFWBWLmsee0pNjludAwBsIZEtPnM1EMtwIiQAa1BU\nACGEUPGY8cwTKh6zOggA2Ei+aB7rCY5Hs1YHAVCNKCqAUKGg+fzTIp+zOggA2I4p1emB8LXJ\npNVBAFQdFtOj2qmxYfPsi0JJq4MAgH1dGY/niuYdO9gKDMDmoaigqsnBPnnxnNUpAMABBoOp\nXNG8Z3errtNWAGwGpn6hWiklL5ylpQDA6k3Gssf7QobJEDSAzUBRQVWS0nzxhBzqtzoHADhM\nOJl/9mowV2TbYgAbjqKC6lMsmMefURNjVucAAEdK5orPXg2kcobVQQBUOIoKqkw2Yz73lIqE\nrc4BAA6WLZjP9QSiaY5YAbCBKCqoIiqdMo49pVJssgkA61Uw5PO9wak4G7sD2CgUFVQLlUqa\nzz8tshxbBgDlYUp1sj80GslYHQRAZWJ7YlQFFY+ZLzwrCsxSAIByUkqcHYoopXa01VmdBUCl\noaig8qlI2DxxTBhFq4MAQAVSSpwdihqm2tNZb3UWABWFooIKp8Ih8+QxYbA7DQBsoIujMSXU\n3s4Gq4MAqBysUUElU4FJ88RztBQA2ASXRuOXx+JWpwBQORhRQcVSE2PmiyeF4gRlANgkfVNJ\nIcRt25qsDgKgElBUUJnk6LA8d1ooZXUQAKgufVNJpdTt25utDgLA8Zj6hQqkaCkAYJ3+QOr8\nSMzqFAAcj6KCSqMmxkxaCgBYaiiYOjcctToFAGejqKCiqMlx88UTtBQAsNxwKH1xlHEVAGtH\nUUHlUJPj5mlaCgDYxUAgdWmUfcAArBFFBRVCBafMF0+wxxcA2Ep/INkzkbA6BQBHoqigEqhg\nwDx5XEhaCgDYTs9E4tpk0uoUAJyHogLHU6GAefJ5IU2rgwAAFndlPD59xAoArB5FBc6mwkFa\nCgDY3+Wx+FAwZXUKAE5CUYGDqVjUPPm8MGkpAOAA50diw+G01SkAOAZFBU6l0mnz5DFhGFYH\nAQCs1vmh6EQ0a3UKAM5AUYEz5bLy+DMin7c6BwCgBEqIFwcjwUTO6iAAHICiAgcyiuaJYyqb\nsToHAKBkUqlTA5F4pmh1EAB2R1GB05imeeKYSnCCGAA4lWHKF64F03nm7gJYDkUFjqKkefoF\nFQlbnQMAsC55Qx6/FsoX2Q0FwJIoKnAS8+xpFZi0OgUAoAwyeeP4tZBhclYvgMVRVOAY8tJ5\nNTZidQoAQNkkssVTAxGplNVBANgRRQXOIPt75cA1q1MAAMosmMidGYzSVADcjKICB1CT4/LK\nRatTAAA2xHg0c3k0ZnUKALZDUYHdqXjMPHNSMDEAACpXfyA1FExZnQKAvVBUYG+5rHnyeWGy\nLQwAVLgLozEOggSwEEUFNmYY5oljIpe1OgcAYMMpJU4NRJJZDoIEMIOiArtSyjxzgoMdAaB6\nGKY80RfOG2xYDEAIigpsS148p6Y4MgUAqkumYJzqD7FhMQBBUYE9yYFrcqjf6hQAAAtEUoUz\ng1GrUwCwHkUFtqOmJuXlC1anAABYZjya6Z1MWJ0CgMUoKrAXlUqaZ06wGTEAVLmr44mJKJup\nAFWNogI7MQx56rgwDKtzAACs9+JQJJ4pWJ0CgGUoKrANpcwXT6hU0uocAABbkFKd7A8X2AQM\nqFYUFdiF7L2iAmzzBQCYly2YpwfCTAcGqhNFBbagpibktatWpwAA2E4ome+ZYGE9UI0oKrCe\nSqfMM6dYQA8AWFTvZGIixsJ6oOpQVGA1w5AnnxdG0eocAAD7OjsYSeZ4pwCqC0UFFjPPnGQB\nPQBgeYZUp/rDhsnCeqCKUFRgJdl7RU1NWJ0CAOAAqZxxZogT64EqQlGBZVQ4JHuvWJ0CAOAY\nk7HstUkG4YFqQVGBRQp5TqAHAJTq6ng8lMxbnQLAZqCowApKmWdOilzO6hwAAIdRQpwZjHAK\nJFANKCqwgOzrUcGA1SkAAI6UK5pnhiJWpwCw4Sgq2GwqFpU9LE0BAKxdIJ4bCKSsTgFgY1FU\nsLmKRfP0C0IxZA8AWJdLY/FYumB1CgAbiKKCTWWePSWyGatTAAAcTyl1ejDCySpABaOoYPPI\ngWucmgIAKJdM3jg3zMkqQMWiqGCTqFhUXrlodQoAQEUZj2ZHwmmrUwDYEBQVbArTlGdOCskA\nPQCgzC6MxFK5otUpAJQfRQWbQV46r9JszwIAKD9TqtMDEckJwkDFoahgw6lgQA4PWJ0CAFCx\nEtliz0TC6hQAyoyigg1WLJrnTlsdAgBQ4fomk5FU3uoUAMqJooKNZZ5/UeSyVqcAAFQ4JcSZ\noaghmQAGVA6KCjaQGhtRE2NWpwAAVIVM3rgyFrc6BYCyoahgw+Sy5sWzVocAAFSRwWAqmMhZ\nnQJAeVBUsFHMs6dEkf0iAQCb6uxQtMhx9UBFoKhgQ8iBPhUKWp0CAFB1ckXz4kjM6hQAyoCi\ngvJT6ZS8yiH0AABrjEYykzH2cQEcj6KC8pPnzwjTtDoFAKB6nR+OFgwmgAHORlFBmcnhARVm\n0hcAwEp5Q14aZQIY4GwUFZRVPievMOkLAGC90UiGHcAAR6OooJzMC2fZ6QsAYBPnhjkCEnAw\nigrKRk2Mqclxq1MAADAjWzB7JxJWpwCwRhQVlIlRNC+dtzoEAADX6Q8k45mC1SkArAVFBeUh\nL50XOfaCBADYi1Li7FBUKSaAAc5DUUEZqHBQjgxZnQIAgEUkssWBQMrqFABKRlHBupmmPPei\n1SEAAFjS1YlEpmBYnQJAaSgqWC/Z16MyaatTAACwJFOq88McqwI4DEUF66IyadnXa3UKAABW\nEEzkxqMZq1MAKAFFBesiL54T0rQ6BQAAK7s0GudYFcBBKCpYOxWcUoFJq1MAALAquaJ5bZJj\nVQDHoKhgraSUF89ZHQIAgBL0T6XSeVbVA85AUcEayb4elWa3RwCAk0ilLo2yqh5wBooK1iSb\nlX09VocAAKBkU/FcIJ6zOgWAlVFUsBbmpXPCZA09AMCRLozGJGfVA7ZHUUHJVHBKTY5bnQIA\ngDXK5I3+KWYvA3ZHUUGJFGvoAQCOd20ykSsyNQCwNYoKSiOHBlhDDwBwOkOqy6Nxq1MAWA5F\nBaUwDHntqtUhAAAog7FoJpouWJ0CwJIoKiiBvHZV5PNWpwAAoDwujzGoAtgXRQWrlsvKwT6r\nQwAAUDaRVH6KrYoBu6KoYLXMq5fYkhgAUGEuj8XYqRiwJ4oKVkUl4mpsxOoUAACUWSpnjETS\nVqcAsAiKClZFXr4g+MYJAFCJesYTpuQ9DrAdigpWpoJTKhSwOgUAABsiVzQHAuy8D9gORQUr\nUUpeuWh1CAAANtC1qUTekFanAHAdigpWoMZGVILdGwEAlcww1bXJhNUpAFyHooJlKSl7r1gd\nAgCADTcUTGfyhtUpAMyjqGA5cmRYZdgLBQBQ+aRSPRMMqgA2QlHB0pRUfVetDgEAwCYZi2ZS\nOQZVALugqGBJcnhIZTJWpwAAYJMoJVipAtgHRQVLkFL29VgdAgCATcWgCmAfFBUsTg4PiCzD\nKQCA6sKgCmAfFBUsRpqyr9fqEAAAWIBBFcAmKCpYhBwaELms1SkAALCAUqKXQRXABigquInJ\ncAoAoKqNM6gC2ABFBTeSQ/0in7M6BQAAlmFQBbADigquJ6UcuGZ1CAAALMagCmA5igquI8eG\nRY7hFABAtVNKXJtiUAWwEkUFCyil+hlOAQBACCHGItlswbQ6BVC9KCqYp6YmVCppdQoAAGxB\nKTUYTFmdAqheFBXMk/1s9gUAwLyhUKpoSqtTAFWKooIZKhxU0YjVKQAAsBHDVMOhtNUpgCpF\nUcEMzk4BAOBm/YGUlMrqFEA1oqhACCFUIq6CU1anAADAdvJFcyyasToFUI0oKhCC4RQAAJbW\nN5VkSAXYfBQVCJXJqIlRq1MAAGBTqZwRiHPIGLDZKCoQaqhPKL4qAgBgSX1TbN8PbDaKStUz\nTTkybHUIAABsLZLKx9IFq1MA1YWiUu3k2LAo8soLAMAKBjj8EdhcFJVqp4YGrI4AAIADTESz\n+aJpdQqgilBUqpoKB1UibnUKAAAcQCo1EmafYmDzUFSqmhzstzoCAACOMRRKsfsMsGkoKlUs\nm1FTE1aHAADAMbIFcyqRtToFUC0oKtVLDg+wKzEAACUZCqatjgBUC4pKtZJSDg9ZHQIAAIcJ\nJnKpnGF1CqAqUFSqlBofFYW81SkAAHCeoRD7FAObgaJSpeQQy+gBAFiLkXDaMJk7DWw4iko1\nUsmEikWtTgEAgCMZphqLsk8xsOEoKtVIjQxaHQEAAAcbZvYXsPEoKtVHSjk2anUIAAAcLJ4p\nJrJFq1MAFY6iUnXU1ATL6AEAWKeRMPsUAxuLolJ15Ai7EgMAsF5jkYzkODJgI1FUqkwuq0IB\nq0MAAOB4BUMG4jmrUwCVjKJSXeTIEKfRAwBQFsz+AjYURaW6qNFhqyMAAFAhAolcvmhanQKo\nWBSVKqJCQZXhux8AAMpDKTEa4UAVYKNQVKoIy+gBACiv0TBFBdgoFJWqYZpqatzqEAAAVJRk\nrhhLF6xOAVQmikq1UJPjwmQeLQAAZcaSemCDUFSqhRznNHoAAMpvPJrlQBVgI1BUqkOxyPEp\nAABshKIpw8m81SmACkRRqQpyYkxIaXUKAAAq03g0a3UEoAJRVKqCYt4XAAAbZiKWlZLZX0CZ\nUVSqQD6nIiGrQwAAULEMUwaTOatTAJWGolL55PioYJEfAAAbidlfQNlRVCof874AANhoU7Gs\nyewvoKwoKhVOZdIqFrU6BQAAFc6QKpBg9hdQThSVCqfGx6yOAABAVRiPZqyOAFQUikqFU1Pj\nVkcAAKAqBOI5g9lfQPlQVCpaLse8LwAANocpVSDOknqgbCgqlUxOTVgdAQCAKjIVY5kKUDYU\nlUqmKCoAAGyiqUROcSQAUCYUlcplGCoctDoEAABVxDBlOFWwOgVQISgqFUsFJoWUVqcAAKC6\nTLFMBSgTikrFYoEKAACbbyrOMhWgPCgqFUpJFZyyOgQAAFUnkzeSuaLVKYBKQFGpTCocEkVe\nJQEAsAB7fwFlQVGpTOz3BQCAVVimApQFRaUyyalJqyMAAFClYulCvmhanQJwPIpKBVKppMhm\nrE4BAECVUkIEEsz+AtaLolKBVChgdQQAAKpagL2/gHWjqFQgFaSoAABgpVAyzwn1wDpRVCqO\nlCoSsjoEAABVrWjKeJYj6oF1oahUGhWNCMOwOgUAANUulMhbHQFwNopKpWGBCgAAdhBKskwF\nWBeKSqWhqAAAYAeRVMGUrFMB1o6iUlkKBRWPWR0CAAAIqVQkxewvYO0oKhVFhQKCTUYAALCH\nUJKiAqwdRaWiqFDQ6ggAAGAGy1SA9aCoVBQWqAAAYB+JTLFgSKtTAE5FUakguazKZqwOAQAA\nZigGVYB1oKhUDhXmnEcAAOyFZSrAmlFUKoeKhq2OAAAArhNNcT49sEYUlcqhIhQVAADsJZkr\nFlmmAqwJRaVSFIsqlbQ6BAAAuFE0zaAKsBYUlQqhImFOUAEAwIYiaZapAGtBUakQKsJKegAA\n7IgRFWBtKCoVgpX0AADYUyxdkMx6AEpHUakIpqniMatDAACARZhSJbJFq1MAzkNRqQQqFhWS\nHUUAALApNikG1oCiUgmY9wUAgJ1FUqynB0pGUakEKh61OgIAAFgS6+mBNaCoVAIWqAAAYGe5\nopkpGFanAByGouJ8hbzIZq0OAQAAlhPPsJ4eKA1FxfEYTgEAwP4SFBWgRBQVx1MxFqgAAGB3\n8QzLVIDSUFQcjxEVAADsj6IClIqi4ngUFQAA7C9vyFzRtDoF4CQUFYfL5+8OU6YAACAASURB\nVEWOlfQAADgA6+mBklBUnI0TVAAAcApmfwEloag4G/O+AABwikSWERWgBBQVZ1MJigoAAM7A\niApQEoqKwyUSVicAAACrki2YBUNanaJy/OjRXZqmHUuutv59++4uTdMG82xp4BgUFSczTZXN\nWB0CAACsFrO/gNVzWx0Aa6dSSaGU1SkAAMBqJbPF9gaf1Smq1Cv/9bkrOWO712V1EKwWRcXJ\nUkmrEwAAgBKkcoyoWKZu176DS/wql8n5av3apsYRQohMuFDb5i3vfeaLps9TIWWMqV8OplIs\nUAEAwElSOcPqCA5TiF/6o/e+5cC2Dp+3bvv+o+/9vS8Ei0uu80kNPfGRd/7MwW0dfo+nvqnz\nnoff9LlvXpj77fce2LpwjcpTv3CLpmmp4e++4cjOmroaj69+372v/dKzk0LmvvaxX7tzZ5ff\n4+vae9eH//RHpQWOnf/td79+e0ejv7H9vkff9cRY+jP7Wuo63jL928d/dq/uqhVC/PMf/ecd\n7XX3fOTE9OXF1NVPfuAXD+3qrvH42rr3/NTbP/xE//z30f96R4emaXHzunk07+yqr2n5yemf\njzb4Ou74195/+9Tde1r8XrevvvXQK97459+5VFJyG2JExcFUkhEVAACchBGVkhSSL7zqlkee\nDeXvuP9Vb3+0e/DED77w8fd+899fHDn+l76bhj+ywW/deeubh/La0df+zDt2tacDfT/41rc+\n/PS3Jp8d/8QDXUs9xKN3/9z59vvf95u/kB144rFv/Pv7XnVf70/XfPYH+lvf8c5H8v2PfeXf\nPvuhV/sfCn7iSPtqAhuZS6+79f4nAtm7XvHoa3bVnH7ym685ePzlntwNn7iP/8lr3v4/rr3h\n595936PbhBBG5vxrDjzwxER6++GX/eyrbgldffHfv/65H37jG1++cP5d+xpX+W+VnvzikZ/7\nvtm469VvetQVvvr0c9/+4M98+/Rfn/9fv3r7Ku/BhigqTsaICgAAjpI3ZMGQXjdTWlblb970\n5meC2Q/+w8XP/fz0p23zi7948Ne+/lfv+Y/f/dpPbr/hyuc//juDOeNtX7369V88MH1J+Oyn\n24985G9/+8wnnnjtUg9xdfcHhl74ZJNLE0K89s17fuGbg5/53q3PDL34kna/EOJDr3rTLe/4\nt29+7uon/veqisr3fvkNj09lfuWLJ774K/cKIWRh/EMvO/Lnp3K1C2+tCj/9Oc/p8at31Hum\nL/i3t7/hiYn0az7+/R/8zkzOa9/6/YNv/PgHXvWRdw1+YTWPK4TIRr7bdvg9zzz317fWeYQQ\nkQv/eM99b//K+x/5f98+fqjWqR/4+TtxLGmqDFt+AQDgMOk8s79Wxchc+sAT4837fme2pQgh\nXO/8s0/df//9xrPhm6+/7dW//9hjj/35z++fu6T51rcIIfLB7DKP8uv/8nvTLUUI8dCHbhVC\nHPrI16ZbihBi+6O/JoTITi53D3OUGf/lfxms7/6V6ZYihNC9W//4m79/49WU+ZIvfn6upSgz\n/qvfHva3vu47vz3fpvb/zMc+d3dHYuiLf79s+Bt8+nt/Nt1ShBCth37+/3zsqFkMfuhbQ6u/\nB7txasECW34BAOBEyVyxpa7M66crUmr8L/JS3f6O/7TwQn/bzx479rOLXn/b63/+3UIoMzNw\nuad/cHCwv+/pb31+xUe5t3H+/xeeZo8QovORzrlLdE/L6gNnpv42WDT3P/LOhRc2bH9fq+c3\nctdf8y33dczfKviPUUPueuA33ddPZnvNBw6I9wS+ei3+to6a1Ty6t/6ed2+tW3jJ/ne+V3z0\neO+X+8Vb963+WdgKRcWxWKACAIADsZ5+lfLRISFE422rXaRhZK784fs++Pm//3G0YGq6p3vX\n/iP3PSJE/wo3u2mti6avcfevYvaKEKJu73VtQWju3T73leuvucM3vyuXmR8SQjTccuPTnH7i\nqZGMeGBVj+6pvXEtiqfuLiFEZnSR0SenYOqXUyn2JgYAwIFYT79KnsZWIURmeLUT3X/3gQc/\n/pUfvvI3/sczZ6+l8vnx/kvf+dr/3MiAN3J5twgh0oPp6y+WowXzhmsurEIu3y4hRLL3xs91\nqWspIUTt1iWHU5LmdbufFTM37vE1fYmvrXkV2W2KouJUKp2yOgIAACgZIyqrVN/9K5qm9f/N\n9xdeWEgec+l6511fveHKRubi/38u3LzvU//yyd94+eF9tW5NCCGLwc2LK0Rt57v8ujb5+NcX\nXpie+HLgpqJy3a3a39Ls1gPHPnPDlf7jz64KId56oGnukrgx30zMXP8PY/mF1y+kTv/d5HWl\nbuAf/koIsffde0p8HjZCUXGsLCvpAQBwnmzeMCWrTFfmbXroDw61Ri79f7/7rb7Zy9Q/f/iX\npVIv/b2bpkNpbl3TjEyvMftPK4vBP3//m4UQQizXE8rI5dvxxdftSE381fu/cmY2Q+C//tyN\ni+lvoLmbv/DojmzkO2/81ONzF/Z/9w/f/0KgceevvKuzVghR0+kTQnz8x+Mzv1aF//3BN2TM\nG8+T+fCjv9GXnanBgRe+8obfOq67mz/9VgcXFdaoOJXKpFe+EgAAsBklRKZgNPg9VgdxgN/6\n0d/+0/43feKNB3/w8Ovuua175PQPvn98tPXQe77+5hs/fLtrDn785V2//cwXDjwU+flH7shO\n9T3zf74xvusNO3yXJ4f+4BOfC//2h35tEwK/7Z++99U7H/jLX7r35Fd++siumlOPf2ew8R13\n1n2p392wzK3e9PV/e2jfy77zWz+x5x8fefjoLaGrp7//5GnNt+vzj396+gpHPv4L2oOf/tIb\nDoV+6ZdubzFPPv7PPzgVOtrgvbjgTrwN9+4f+7tDu5561StfqoeuPv7kibRUv/CnT7ykwcE7\nNzCi4kyGIQoFq0MAAIC1yOY36Tt+p6vpfPSFq0999B2PRi4/99gX/+7UZPM7P/qZi6e+VO9a\nZL37R390/GPvfaPo+ffPfPpPnzg/8eBvfmXo+a8/9pE31MmeT/zJX29OYHft7d++dOH33/0z\nqZ6n/vYbP25/6EOnj//ZWMF0ebctcytP3V0/6j358fe/tXbq4j98+bFnL4Ze/bYP/ejS+bfv\nnVlh3/WyTx177A9efseWx7/2+f/+qb/49zP59332yd/bcd36e2/dkSevHf/PL+984Qf//P1j\nl7e/5HX/8xtnvvrrd23gs914mmKLWwdSibj59I+tTgFgQ+h3HNZ37zOl+t6ZMauzANgQh3Y0\n7+6otzoFyu/F54/l9bb7X3Jg7hIjc8FTd+f2V3535MePluMRZHBkwNWxu9XvWnjp0QZfT/27\nkhNfLMdD2AgjKs7EvC8AABwru+zqajjXV9/6upe//KVnUvMbu53+y18XQjzyh0fK9Ah6x459\nN7SUCsYaFUdigQoAAM6VLbDxl9MoueIWCJqm/eZXPvCnP/HHrzj0yv/yntdva/JcO/X9v/rq\nU+33/Jcvv2LL5sSsMBQVZ8qw5RcAAE6VYUTFaa584RW3/T/PLX+d2vb/lA7+09Uf7PqtP/7S\n33/+k+Nxo3v37e/+nb/42H99r3eNZ0hWO9aoOJL5wnMqOGV1CgAbgjUqQMXzufVXH95qdQrA\n7lij4kxM/QIAwLHyxsrziABQVBxIKZXNWh0CAACsXYZlKsBKKCoOVCgIydxWAAAcjKNUgBVR\nVJxH5XNWRwAAAOvCiAqwIoqKA1FUAABwuFyRERVgBRQV51E5igoAAM6WL0qrIwB2R1FxIEZU\nAABwuDwjKsBKKCoOxIgKAAAOlzcoKsAKKCrOw2J6AACcjqlfwIrcVgdA6SgqAAA4XN6gqKxO\nPqeiUQsfX2tpET6/hQGqGUXFgSgqAAA4nFKqaEiPm7ktK1DRqHnqeQsDuI7er3VvsTBANePP\nw3lUPm91BAAAsF45lqkAy6KoOE2xKExe1wAAcLwCy1SAZVFUHEYVGE4BAKASsPEXsDyKitMU\ni1YnAAAAZcDGX8DyKCpOUyxYnQAAAJQBIyrA8igqTsOICgAAFcEwldURAFujqDiMoqgAAFAR\niiZTv4DlUFScxqCoAABQCRhRAZZHUXEaRlQAAKgIhmREBVgORcVpKCoAAFQEg6lfwLIoKg6j\nmPoFAEBFKDL1C1gWRcVpGFEBAKAiMKICLI+i4jScowIAQEVgMT2wPIqK0xiG1QkAwHk0TWia\n1SGA60mlpKKrAEuiqDiMMjnFFgBKoGmu+w9ve/3d23/qrm6rs1Sgw7dvec3hrQ/vq7M6iFMx\nqFJJgi987a0/9dD2lvp9d7381/7b3xT4/+26ua0OgBJJigoAlGD77o5292oHU7x+347Wms5G\nX53X5XVpxaKZLRQD0exYJJsuxwdKt9e7q722vcHX4HN5XZppynzBiKTyE+FUMLvccoV7D2/r\nXt2zGLw4fiFf8soHj9+3v7O+s9Fb69GlIZOZ/EQ4NRBbYbJxTVPrTr9LCHVxOFPqI2Ja0ZRe\nN98aV4JM4Jt3PPgu48DrP/rfP5O6+N1P/Lf3nAx0n/6L11qdy9koKk7DwrvKpnldDzwgNKFO\nPyezDt04QdMffo3udavYBfPEkNVhUO1qmprvalnVO52muQ7sat3f6lvYBnxet8/rbq6vuWWb\n0Tsc6YmsY5Wgpu/e1np7p3/hZ1Ld7fK4XfW1vp2djbFo4vRQIrP4a7zW7trAiWtdXa1Ht9XO\nBXN5XK1Nta1NtXuSqef7YktEEkLTj+yuEUJkYtGxIl8dr5Ep+aerEN/6xQ/G3HtfeP6fj9R7\nhPjVo+Ztb/nrN5/+ZOyeeo/V0RyMouI0HA5VCter36B5Xau5pnz82zKzRDHQ3NrWXfrWbaKh\nTvN7RT6n0nE1MSpHxkTZ32AabtdaWoUQYhUfSsrz7JZR16Hv3aN1tGk+ryjmVTykRvrkZHSF\nW3Ud1etrhZLy3GjJjwiUle7yPrCnfpXXvO+2jg7vdX93Ss0va9F094HdnQ3uwKnAmrqKpt9x\noGtP3XJ/sM0tjQ/VeZ6+FE7f9DKvu3yrHhMqWXNn233baqZ/Ng0zkTXcPk+DVxdC1DbUP3hA\n/MeV2KJD+Q0dbW0uTSn54lB2o8JVAcUalcqgjI8+M9n5sn84MltLXvsH75df+MDvPz3xnUd3\nWhvN0SgqTsPUrxLommfd4+lN211HjmgLvw7x12r+Wq1ti37ggDz3gpxKrfchFtAPbVn9dcvw\n7Jam7TvqOrhTzH028tVonTu0zh1aqE+eOK+Wamiax3XXNiGEmjyl8vy3CosdvKW9djV/JZp+\n+GD7XEtJJ9NXp9KhVLEglcfj7miu3b+lodGtCSG2bO84kJnsSZX833bH9o65llLM5a5OpAKp\nQtaQLperqc6/q7txa51LCOH21jywr/5HvTe+qrjc/ukflDTODCeWf6x0KYMbust3/7YaIYRS\nsnco1BuZmVTvrfEf2dfa6dW9tfX3tqeOh27cx0XT3fdu9QkhkoFwlFUW68CASmXIxx8fyRsP\nv/euuUvqtr6v3vWhvn8cERSVdaCoOArDKSXxdKx3l5+m3e6XHRH6EnfibdKPvkqc/pGcTK/r\nUeYfbr/e4l/tldf/7Jam7bnfdetsZSpkVTIlahu1Gp8QQmvf53qZZjx7Viz25qrtfqnm0YUq\nyHPjG5QNWKWmrvZ9tboQQkmhLVtXGjrats9OyApOhI9PzI8PFIvGeDAxEc7cdWvndr8uhLZ/\nX2v/uaBRyodL3eW9r2Pmy458Ovl4T3zu5oZhhuPpcDwd3dN9R4tbCOFvaN7jSQ9cXzY8tTM3\nN83sWKScq0EatzRPj9WMDQR6YvNtpJDNnbwc+onDnX5NtG5tFqHQDTds3dpWpwsljZPj+TLm\nqULs+lUZiqkzQojG/Q0LLnPtr3GPn1tpGgKWRVFxFIpKSTxdMz/IjHnu0gpXLtz0Fale53rg\nrpmWogx57aIam1CZvPDXalt2uw7uF7omNF2/55Xqx99XufVtG625tK379cO3lXCTdT675e65\n03VbtxBCqLw8c1yOh2cub+x2Hb1Pq3WLpr2uHf3mcPLGG+p1+sF2IYQaeF4Z/LcKK7l9tQ9s\n9QshlDTOjssj271LX1e7Z+vMbwuZxMKWMkdJ49zVUPvhTr8mdJfvSKPrZLyEvyl/Y9NcUTrf\nl1i05AwOhQ40d3s0IYTY2e0ZGLlugpm/cWY0xsiXuRVsbXELIZQsno3d+CImzcLZhPHSJrfL\n7W92abEFwya6y3dvh0cIER4LZ/iYvT70lMogjYgQoqPhuuUoW72ukcwKQ6BYHkXFUZj3VQqt\nqWnmp8KYGhsp+eaHHtBcuhBCKCmP/1CGczO/yKZU/wVjKuJ+6CVC14Tmcd13i/H05dLzubWu\nLVp9g2hs0to7S53Htc5nt9w9H7hzeqxGvfiknFgwWJSYNJ951v2TDwld0249JIaP3XjDW+/X\nXJowU/JqpIx5gFJpmn7kQMv0QMHQQDDqbl3mym5vQ8PsqOnQwJIzOaVZOB0uvqzdI4Ro21Yr\n4jcV9aX5m1yzd5KfXGIsRkljqCj3e3UhhK/BLcR1RaWubub1IZ8o87tAgyaEEKaZXzRWLiFF\nkxBCNOpi4TqVLbtaPJqQZv5kyKF7ftiIZO5XRdDczUKIUPq6wj9eMHXfqpbJYSnsiOcobPlV\nko7ZaVSZGyctrMxV79o+8+Kiho7Nt5Q56XHzYmDm58Zb9ZrSO79e7zp6r37woL6ley2rTdbz\n7JalddcKIYSZMCdumtJWjJjBrBBCeLq1G/bTdLe5djcIIdTl55dcwQJsiu4dHdNjE5lY9MJK\nQx/+xrn5lurasrv6JqdmRjM8/kZfKfMu9fm9MZb705ib7aXdNFOtc3YpfabcRSWphBDC5Vr8\nCfnqZ5KkFvzDuD21R5rdQojxwUhJU+CwKKZ+VQZv/T1CiOR1X3ao/pzRfKjFqkiVgaLiJIoR\nlVJoLTPTOVSw5B1ptI7DMytAlJRXF28CauTk3Cdy7UDzGlOu1Xqe3Qr3PP2hqhhc/Neh3HVX\nm7vVXfcKTRPF4CJTwoBN5G9oOtruEUJIs/D84Mrrx9z+mf+SlVzhPF1pzo1yaFtL2YQrn5y5\nY93la1pqzZum7fDM/Kp409bkHbN/bsFy7wI8ETOEEJruOXLTJs66y3u4yS2EkGY+smDe1+59\nzZoQRiF9tpT5b1gKPaUy+Joe3ul3931pfiZ2ZuqxhCH3vY2V9OvC1C9HYY1KKea37g3eNB6y\n4m0Pzn4Fkru25HILVZCxvKvVL4TQOvcLUeLIhkyazz5x/aP6XC97YLUJ1/HslqdMpXmE8HQs\n/utW39zV5i/073B11woh1JnTy35lDGwsTfe8ZN/MWGhPb2jJA0AWMOcHMlzaskMemja/uXCn\nXx8orvZjejqcMLdNn4Oi3bOz9vHF6lN7d3v9bIcZHr+uqCzcm3jKUL7amgNddW11Hr9b15TK\nG2YilQ/EMyOxtZyCHR+PG+3tbk1s3d2ZFqHe6MydePy+u/a1Te+ZFhmPzV3fW9N4a60uhOjv\nj/O3XhaMqFQIzfsnD3b/0tO/N5R/3S6fSwhx/LOf1V31f/TIVquTORtFxVF4OSuBNjefag0r\n3TX/zJ+GCkwsd72BpGj1CyGEt1vTtdKmPClTxa7fDERb9ZZf63t2y1OTGbG7UbgaXVvqbpz9\n5W7Wu2qFEKIYWNjf9KN3CiFEdsgMcDo1rLR/f3ujrgkhEoHQtdXUFCEK6Zm+oWnaLo82uPSQ\nhb+pZu5nt18XydUWFWnmnhvLvGJ7rRCirrXlYd19cSIVys7c3Ovz7upqOtg+8xVANBC+lrsu\nue6a3ZtYmfv3dO5uXnjai1br0mt9nu62+luzufODkYllj7dfNNvz47kHt/k1TT+wp3PfDiOe\nMVxeT6N/ZhCnkEmdCM4Xp1v31Qsh8plEz+r+ebEiikrFeOPffrZp51sfePT9f/LB18fOfucj\nnzx/9we+ex+nPa4PRcVRNmw72grkbp/951KyKEXTFn3vbq2lWfN5hTRUPieiQTU1KicXW/bt\nqp9fgLHYFkBzVDwixPTIg6Z5XWXvDEtaz7Nbieq5IHY9IDRNu/sRXTsmx2fvpL7Tde9LNF0T\nQqie8/M3aLxVb/YJIeTJC+t8WsB6NLS3Hax3CSGMYvbY2GpHGguZhBI1039Oe3fUDPYvXrY1\nTb9z2/wHDleJb57xQOQZ03zJzgavJhqaG+5vbpCGmTWVy+Xyzw6XKKVGxyNnp258zfHWeGcz\nuPY0L3lkpLfGf8+t3T19U70lrmOJTYVOaK1Ht9bqQrjc7tbG+eeWSaSe759fRV/T2LLTqwsh\nLvczw7Ns6CkVo7b7584/+eVf/vAfv/9t/8vTuucdv/vYX/7R66wO5XgUFVQo79zuvXn9yEP6\nlgXb/uheze0VdY3a9n16Yso8c0Ilr58R7ume+1Fllu0eRlCIg9M/ah5dlHkS1tLW8+xWVJwy\nr0y5busWmle/+2H9joyKJ0Vto1Y3+3VyvN8cmttvUdPvvUUIIeKXZGJNJ3YD5eDy1DywY/rg\nQnX+amT1SzmULF5MmocaXEKI2ubWO1qLFyM3/slomn7L3q62BeuydE/JXxvFwvFnpfbKPTMz\n03S3q+76d+DJseDZxY699zVdt5o0GEoMRrKxnFEwhcftaq737exs7K5zTec8sK8zcWFyqsSl\nLFOTkR/G0vs767savTUelzTNZDo/Hk4Nxhbk0bTDu2uFENl4ZHQts8ywOEZUKknXA+/+9vPv\ntjpFRaGooDJpDbOr23W/vmXpKVWNXa5XvEa+8B8ytKBkuBrnflTL77RmLtjfw7vkN51lt65n\ntwqq/5ip3+s6sENoQnhrtY7a+V8Fr8mTF+Yn8nfcrde4hVDmqb7SngNQRpp254GW6RlR4fHg\nWIkfo0cGYrfc2Ta979We3V2Njcn+UCaSNQwpvB53a2PNnq6G1uu3xVKlLiPX9D3bW2/vWG56\n55btnQ/WJU4NJrLXx6+rn3ltUcq8eC0wuGDKWaFoBKJGIJretrXj7m6fEELTXIf31P2wZ8l9\nlpdSzOUvD+eX2We9vq2tw60pJc8MlnkDjyqnMVcCWBpFxVF4OVu96z4QKDXSI0dHVTIjDCV8\nfq21U99zUGuuEUIIzavf94j68Q9VfvbtX19wNtzyH3jUgq8bfZu4h956nt3qqGsnjclhfc9u\nraNN8/mEkVexoBrpk5ML1tVobtfd24UQYuqUym7WtDfgJh1bO7b7dCFEIZs6PlXyyJ5pZJ/p\nTzy8t3F6ElZba0Nba8PNVwtPhRs726aHUsyS9uXV9EO3dO2e7RvxeLo/lImkizlTulyuhlrv\nltb6Pa0+TYjmlsaHajzPXAmnF3xDkhyPnXEJIUQxk5taYgnK2Hiwpn7LrfUuIYSvvrnTnQ6U\ndedgTXPdu90nhEgFw2Hzunuub6jb217TXu/1u7Vi0YylcqPB5ESaDcFWi/d1YBkUFVSo2c2p\nhMzJF56U4QXzznNpNT5gjg9qB1/m2t8phBB6jevobuO52TGBhfuHLj8ov/BrVd/mjais69mt\nXiogzweW+b2286WaxyVU0TwzVvKdYxlMBSmFt7bhJV1eIYRS5uneNW5FlY0nHu8xj+5pbvUu\n8rlRKTkyET03mfvpuUmXpdSA7h0dsy1F9Q8GLi2YWmYYZjSRjSayg5GGV+xr8mjC4695YH/D\nj3rmF4HEY5n4Kh5lcDB566GZsdZ9ja5ApJzfHbRsba/XNSWNk+P5+Us1ff+ujltb55fu+Lzu\nrtb6rtb6UDD6wkia5farwTeQwDIoKo7C69nq9Z4zpyeUxyeWWKSh1NVnZeuj+vS2XS2HNc+g\nmt5v1FzwBr/8v/mC7UrFUrsYb4T1PLty0Wv129qFEGro+A07OGvtO7WdO7SWZs3nErmcikzJ\nwR4VY7rIqtFTSnFkf+P0X+nYYDC0jmGEfDr93IVMR1vd1qaatlq31+0SSuaLRjieHQ2lInml\n6fOfyPOr3lxLd/nuaZ+5YTIQunTTAphpmUTyuRHfwzv9Qgh/fdM+b7qvUNpLilFIpWTT9B7H\nNc0uUb6ioru893Z6hBCR8euGevbu6by1eeZTRLFoJHKypsZT69aEEO0dLQ8K9dQI2wCujKlf\nwDIoKqhMamJkNVeTZ3r0nzg8/bPeWWOOpYQQQi5Y0bH8O4i2YJLYsmdal9e6nl2ZaAdeqrl0\nYabllQUHyGhe/a6X69sWHH9ZU6dt2+vatlcNnjEvDfARfHWUYELIqs0dP7J9T/f2PctdU9Pd\nP33P9tn/S3379M0jgSoYTgXDi/+luNzz2xNPrnoZjL+xcW5W6PmJ5aalJcOR/I6t02thtne5\n+0ZKnsMWMsX0UfJufzlnonbtbPVqQpr5kwv2KfY3NN/e7BZCSKNwtj88lpr5HqShqf7ePU11\nutbY0bo3mOvPMayyAv7SgWVwMj2qW7Z/frl89+ynEDn/3f8N56/fSK+f/7nERSCbYdFnVxbu\nVtfeJiGE6nl+4cmP+j2vnG8puZQKh1Vh5mtdbfcR1x3bb7wfLGp66hefX2zGWzd72qlSU+Zq\ni4qvYfbII1mMLH8rJSdmR4R89WuZSjrXCco4edDlqbm7xS2EmBiKLtxLbMfO2ulHOnMlONdS\nhBDJeOq53pl5a3t3rv5sqOrFgAqwDEZUHIXXs/JTqig1ly6E0Opnh0eKk0LcOf2j5neLZZaJ\ne9rm78iGRWXRZ1cO2p33Ck0TRtgcSMxf2naX3l0rhBCFiHnyhIrOzPrQOvfp9xzSXLq26z59\nYEqmS9wuuQqxRsWWOrpnZnAZ+URZV6ovx1Pra3IJIYRSZji1wmyultnvHgurPoxyRbv2NutC\nGMXMmdh1j77fqwshCrn4+E2DS/l0YrjYsNOjeWvqhWD21wo0vpMAlkZRAWbJ2a8jzZQyZz7f\niy6/iC69t2/93AEmShZsWFQWkOWbgOHb5tpSJ4RQZ08unMqlH94hmOYjvAAAIABJREFUhBDK\nNJ95duEmYCrQZz7vcb/8NiGEfrhLHhstWxJAiCfPrbCXQ11rx0M7vUIIJY3vn5u6+Qp3Hexq\ncWlCiHQ4fGJq8SKtu7y3zc6nio2XsOAqn5CiXQghNN3T5tLCywyqaPqW2cMf87NNw1/fdP/2\nma0CfnhmYpkZZy53TcvsCHAwVJ4FKh5/w+11LiHEYF9s4SNrunv6oYrZxf+5Aoba6dF0F2dy\nr4xvIIFlMPXLSVhyt1pNbVp7h9beobXUrnjduUPoVWS+kMx9zta2dC532z2ze5gWJoXcrK9Y\n1/3s1kk/elhoQuRGzMmF35Vqeo1bCCFS5xfZqjh2RU6PODXuL1eMSqZYo1ICU6oV/rdghGrh\n5XMXTkm93u+u97s7upuWmnHVtb11ukQoZZ5PlFADcsnE3CMd2upb5pr1rfOHtYwFZh4iE01P\n/6BprukV7UvZurNp+gcli71lWjJ3cF+DEKKQTVzJXHeHShrThctTs3ikNrc2fbWyxKhsvLED\ny6CoOIprEzfAdTKt9U7XSx90vfRB1/0PrfAm4N02/1F+aH4Freqd3Q605sDSy1R0vWVmBraa\n6Fln5tVb/7Nbl/pbpp+1PPV/27uz5biufE/Ma+2dcyKRmEeCBGdSFFmax1KpfbrDYfvc9J1f\nwY5+AN86/BruCEfYj9Dnynb0OY7usM+prlk1SCVVSUXNAyWKFAeRRG5fECQ1cACIYe298/tC\nUSFRUOlHAQnsX671X+uN7/161rsdprh06b7/3OawSj7cnRj1ZuvX/rrw/mYZyPLOSyv3GasY\nzkw9O7u5AeGrjy9c3U4LGG18+8e7g+bzs0/M3H8TZmdi4pU7Ex23btw78mvj5pXzd5ZRplfu\nHnP8Q/MLMz+5cwDXlx99uc2L6e+vM5heb2chhDf/cvnHf/edG6MQQqszXP7Rgc6t7mC9GUMI\nN67d5x/kB2z9godQVColt1VvS4qP3tv8s6ybH37Yk3F29snNP9u4/N3ZieKzP25uaorN7ODk\nff7JEMLsT+7VgL/c/+l8L+z8d7cT2fMnQwjh0luji99+72+Mrm6uA0ze/z9XbOYhhDDazZPH\namuzqHh82Sc3r1965075mF6ae/3Y9EK/2c5jzLKJXufUofnX1ifufOSVn9/vQsmFU8t///SB\n2388+6MTt/7216/u/N/HI+sLPz0ytTLZaucxhJDn2XDQPXVo7l+fmGptrtiM/vj922D+9M7X\nt5cvYsyePLH03IHBbK9xe3mn0cjnpvpPHV968cDm+uqNa9/8l8935cUezx3uhRCuXfrq/P02\nnL1//urtTE+dml/p36tPE4P+yyc3D4x+9/yuLeTWmRc6PJgH30rJFMut+fa90bWzt3cixVM/\nzS7+36P7bXyKR57fHP4OoXjr5987OffWlxufXLk9iRFPvRI/+j9/OCufD/LnDm7++aU/jfbz\nXvad/+4e29xTWa8ZQjH6xY9XkIrR1VtZvxkmzsbu+z/c/TV5LOs0Qgjh8tu7kQN22Z/f/mL2\nzPx0I4YQBpP9Fyb7P/6YWzeu//NbF+87Rh/jvZ25P37sHN26/p//fPG1k1PdGEIIU1MTz0xN\n/OijQgihKEbv/PWz89/fuHXz+jf/6W+N1w5N5DGEEJcWhksLwxBC8aN/183r1/75ra93ZdC/\nPzu70IhFUfzu3ftPw1+/fPFPX3dODxt5o/XMyeUzN25evj7qdJsTzc2fU5e++PKdLd82M87y\nTFOBB/LgWykx2v21RaP/8vs7Z7w2s5f+6/yJE3E4EbIYQgytblw6lL/4b/LTd07LvfTuxns/\nXBIpfver4vYu9qyTv/6zbOHe3SBx9kD++r/aXE4pbo1+8c6PA8Sf/reN/+7f3v4j39UTt3bl\nd/c4Yp4/fTCEED7/9X2L2eiN9zc/7Kevxql7pyHHufX8lTObH/O7T3YhSe05nnjfjTZu/POf\nvvjowZu6vvji63/8wxdfb/lU4h+4cfWbf/r95x8+9Niu61ev//JPn7z19X0+5psvL/7j219+\n8f07Sb77BVIUxWdffP1Pf7pwaTeG5WLMn11rhxCuXLjw+YN/y3/562dvfbW5etNuNecm23db\nyoXPL/7n88772pKGogIPZkWlavI8bJT7dKmSuPzuxm8n8p8cCzGEmMfDZ/LDZx7wkR9t/H+/\nu8+Cw60vNn7+p8aLT4QYQnMqe/6/ym5cL67fiJ1eaN174Yx++0/3X06JcQ9nJHf+u9u+eOCF\n2MpDcWvj1w+4bvLCb0efLmWLvdCayV/9b8LVS8WVb8NgGDubPa04/4vR5W3fYTeOzKiksHHz\n21+9+dF7UxNrM93ZfrPdyEYbG9du3Lp46fr7F658tePx9I2b3/76z5+81e8emO7MTbR6rbyV\nx9HG6NtbG19d/vbzr69+eOlhW7auf3P1n/94dWqyuzTszk802828ncebt0bXb9z84tL1jy9c\n+WrLd1A+0nBpdjKLxWjjlx88dO9WMXr73U8/vtA/Mteb7zfbjXjr1sbFy9ff//zyx1f8nNqq\n3F6JR4nT0/mzL6UNkPDfPuYUlaqxorJlxYdv3Lr6dX7u7AOvECk2ivNvbvzh7Qc+F154a+Pn\nN7Knz8ZWHkIIrU5sfWfQduPK6Lf/Mvo4zbToLvzutiXrZmcWQgjF+X8pbj7wiW30y38MT/00\nWxmGEEJvMn7nWLLib7/Z+MMDGg73413W3XLlwuf/cGGrH/zlxW++vLjtSapP//TRP2ztI69e\nufbnK9ce+/CNi5euXbx07c3H/ce3+m/5+LN/+HirH/zNpSu/u3RlL+PUXOPh1woTQmh34tJy\n6hCkoahUTMxy77Vuw1fnN/6f9+P8SlxcjtNTsdMJzSzc+La4ern4/NPig7/d5yDd7yu+eHfj\nP34aD6xlK8uh14vtZrhxvbj2TfHx+8X7HzzkkX0/7Ph3t3Xx2Isxz8Lo6uhPnz/s44obo1//\nx+L9Q/HQWjY9DK1G+PZa8eWno/fevnv/I49mRQXGhq1f8BCx8BOxUjb+0z8Wly6mTgHsoWz9\naHbmXAjhH37lckyoub9/+oCrVOBB7IysmtynDOrO+0cwHrLMTc7wMLZ+VY2rVKD+FBUYC/Z9\nbcW3Nze+upLyIJbpfqvdNCGchqfeqml4qUDdWVGB8dBw5NcWfHXlxi/+uuUDMfbAc0dml75z\n5j77ySukYmJzl2/kAEpHT4HxkDvyCx5KUakaRQXqT1OBsWDrFzycolI1igrUncMYYUw0HJAD\nD+UVUjWtZuoEwB7TU2A8tBoew+BhvEIqxowKjIHNpmLOFuqtrajAQ3mFVE1LUYG6u7P1K/Mt\nGmqt5SRPeCg/BavGigrU3t0ZFd+hodZs/YKH8wqpmGhFBWrv3ooKUGeKCjycV0jVWFGBseHk\nUqg3RQUeziukavI85La0Qq3d3foVfYuGOjOjAg/np2AFWVSBeru79cuKCtSaFRV4OK+Q6ont\nduoIwF5SVGAMxBiaigo8lFdIBXW6qRMA+0FPgRpr5t6LgEdQVKondjqpIwB76d6KiscYqK22\nAZWaGt345H/99/9H6hQ1oahUkBUVqLd7w/SKCtRWp6Wo1NMH/9e/+x//3f+UOkVNKCoVpKhA\nzZlRgfrrKiq1c+vK5//vf/j3f//f/4fUQeqjkToA22brF9TcvQUVTQVqq9NUVGrl8vn/ZWr9\nfx4VRQghdz7rLrGiUkFWVKDe7mz90lOgxqyo1Ex/+X/43Rtv/P73v//f/+166iz1YUWleqKi\nAjV3p6ikTQHspW7LM1itZM2FM2cWQggXp9ohfJs6Tk1YUamgRiM0fHeD+nLqF4wBKyrwSIpK\nJVlUgTozowJjwIwKPJKiUk3m6aHGzKhA3bUamfse4ZEUlUqK3V7qCMDeUVSg5uz7gq1QVKqp\n10+dANgzhXtUoOa6TbOm8GiKSiXF/kTqCMCeuXszvXO/oKZcSw9boahUkxUVGANO/YK66rWt\nqMCjKSqVZEUF6swwPdTdhKJSX6/+b2/e+vaj1ClqQlGppkYjtNqpQwB7Q1GBuut3FBV4NEWl\nqiyqQH3dvZleU4EaijH2zKjAFigqldU3pgL1VNy78DFpDmBv9Nu561xhKxSVqorm6aGubP2C\nWuu3m6kjQDUoKlVl6xfUVmHrF9TZhAEV2BpFpbJs/YLauruioqhADfUd+QVbo6hUlRUVqK27\nN9OnjQHsDUd+wRb5OVhZjWbodlOHAPaAYXqoNZeowBYpKhUWB8PUEYC9oKlAbTXy2G46mxi2\nRFGpsDiYTB0B2ANO/YL6mug48gu2SlGpMEUF6smMCtTXZFdRga3yc7DKFBWopTs7v5xODPWj\nqMDWKSoVFicGIfMZhPpxjwrU1mS3lToCVIbH3CrLMocUQw2ZUYH6muw68gu2yqul4gaT4fKl\n1CGAXXV3RkVTgXrptRuN3HvE2zPdbz13ZDZtgIT/9jGnqFRbHEwWj/4ooJr0FKiXoQGV7Ws3\n86UpF8eNKbW+2uKkq1Sgjooi6ClQOybpYVsUlWpzQjHU0+2iYusX1MtkT1GBbVBUKq7bC612\n6hDAbrOiAnXkyC/YFkWl8uLUdOoIwJ6wogJ10syzbitPnQKqRFGpPEUFaqhwTAbUjX1fsF2K\nSuUpKlBHt2dUUqcAdo9TbmG7FJXKU1SghsyoQO0oKrBdikr1NVux108dAthVTv2C2pnqKSqw\nPYpKLVhUgZopQrCiAjXSazfaTZP0sD2KSh3Y/QW1o6lArdj3BY9BUakDRQXqxowK1IuiAo9B\nUamDOJwK0acSasSMCtTLlKIC2+fpthayPA4mU4cAdk1h5xfUSJbFya5LVGDbFJWaiDOzqSMA\nu0dTgRqZ6jUzC6SwfYpKTcTZudQRgF1kRgXqY7rfTh0BKklRqYk4M+cWa6gPMypQIwZU4PEo\nKnXRasWJQeoQwC5x6hfUyOyEFRV4HIpKfcQZu7+gXjQVqL7JbrPV8LgFj8Mrpz6MqUB9WFGB\nupgdWE6Bx6So1IcVFaiPzaKiqkDlzSkq8LgUlRppt42pQE04nhhqIcYwY0AFHpeiUit2f0Gd\n6ClQdcNuq5l71oLH5MVTK3Z/QU1srqioKlBtBlRgJxSVWrGiAjVhmB5qwYAK7ISiUi/tThxM\npg4B7JgZFai+GN1JDzuiqNRNnF9MHQHYOSsqUHnT/VYj9zqGx6eo1E1cUFSg+iyoQPXNDjqp\nI0C1KSp1E6dnQ6OROgWwM7Z+QfUtTCoqsCOKSu1kWZydTx0C2KEidQBgR5qNbKrXSp0Cqk1R\nqSFjKlB5bqaHiluY7DhgHHZIUakhYypQea5RgYpbGNr3BTulqNRQ7PZifyJ1CmAnbP2CCosx\nzJukhx1TVOopLiyljgDsQKGoQIVN99uthkcs2CmvonqK8wupIwA7YEYFqmzRvi/YDYpKPcXZ\nuZDnqVMAj+t2UdFToJocTAy7QlGpqSx39hcA7L9uKx90m6lTQB0oKrWVLa2kjgA8LjMqUFmW\nU2C3KCq1FReWQubzC9Vk6xdU1uKwmzoC1IQH2fpqNl1RDxVlPQUqqpFnc5Pt1CmgJhSVOotL\ny6kjAI/F1i+opqVhJ7MYCrtEUamzbHHF3hGopM3jiYGKWZ627wt2jaJSa+12nJ5JHQJ4DLdX\nVFQVqJJGHl1ID7tIUam56OwvqKLbW7/0FKiUxWE3y7xuYdcoKjXnkGKoJD0FKmh5yr4v2E2K\nSt11e3E4lToEsE2G6aFqGlmcd4MK7CpFpf7i0mrqCMB2KSpQMQvDbm7fF+wqRaX+4uqB1BGA\nbbKiAlXjvC/YdYpK/cVuL87Mpk4BbIcZFaiUPIsL9n3BblNUxkJcWUsdAdgWTQWqZNG+L9gD\nispYyFYOhMznGqrD1i+olAMzvdQRoIY8vI6HZjPOL6YOAWyZogLV0W7mzvuCvaCojIts1e4v\nqBj7SKASVmd60csV9oCiMi7i4lJoNlOnALbGigpUh31fsEcUlbGR5W6ph8rYLCrepIWyG3Sb\nk13vA8KeUFTGSLT7CyrDigpUw5rlFNgzisoYiTNzoeM6KqiCwvHEUAExhlVFBfaMojJOYswO\nWFSBKrCgAlUwP+i0m3nqFFBbisp4iWvrwdEkUAFFsKACpXdg1nIK7CFFZbzEXj/OzqVOATyK\nU7+g9Bp5tji0oRr2kKIydrK19dQRgEfRU6D0Vqe7eWbhE/aQojJ24tJKaLVTpwAeyooKlN7B\nuYnUEaDmFJXxk2XZgYOpQwAPp6hAqU33W8Oe61Ngbykq4ygeXE8dAXgoKypQbofmLafAnlNU\nxlHsTxiph1JTVKDEmnm2PGWMHvacojKmjNQDwONZm+0bo4d9oKiMqbi8Glqt1CmA+yusqECJ\nHZrvp44AY0FRGVdZlq0aqYeyulNUvGkLZTM3aPfbjdQpYCwoKuMrHjrilnooqbsrKl6kUDLG\n6GHfKCrjK/b7cWExdQrgYbKgqECJtJv50rCTOgWMC0VlrGXrx1JHAO7n7tYv36ShTA7O9qN1\nTtgvfgaOtTg3HyeHqVMAP3KnqHgggvLIYlw3Rg/7SFEZd3H9aOoIwI/dKSqm6aE0Vmd67Wae\nOgWMEUVl3GWra6HdTp0C+L57KyqKCpTF4QVj9LCvFJWxl2Uuf4TSuXPolwUVKIn5yc5kt5k6\nBYwXRYWQrR8xsQvlcndFRVGBcjhiOQX2ncdTQmh34vJq6hDAd90tKpoKpDfoNucnnUoM+01R\nIYQQssPOKYYyubf1S1GB9CynQBKKCiGEEIdTcXYudQrgDscTQ2m0G9nqdC91ChhHigqbsqMn\nU0cA7jCjAqWxPj+ROdcCUlBU2BTnF+LUdOoUwG13b6b3eAQp5Vk8NG/fF6ShqHBPdvRE6ghA\nCMHWLyiLtdl+q+FhCdLw2uOeuLQSB5OpUwD3huld+AgJxRiPLg5Sp4DxpajwPdmR46kjAOHe\n8cS2fkE6B2d73VaeOgWML0WF74mra7HXT50Cxt6drV++R0MqMcajS5ZTICU/BPm+GOMRd6pA\nasW9vV9AEgdmer1WI3UKGGuKCj+Ura2Hjvt3oRTs/IIkYgzHLKdAaooKP5JlLqqHxO7do6Kp\nQAKr071+23IKJKaocB/ZwcOh1UqdAsbYveOJFRXYbzEE0ylQBooK99NouFMFUrpbVHyThn23\nPN0bdJqpUwCKCg+QrR8JnW7qFDC2XPgIyZhOgZJQVHiALM+OWVSBRO4c+pWZUYH9tTzdnexa\nToFSUFR4oOzgujtVIImisKICCcQYTi4PU6cANikqPFjM4vFTqUPAeLp76lfaGDBe1mb7Ex2H\nfUFZKCo8TLa6Fids1YV9d/e+R00F9kuWxRPLk6lTAPcoKjxUjNmJ06lDwPgprKjAfjs8P9Fp\n5qlTAPcoKjxCXF6Nk1OpU8CYcY8K7K9mnh1btIMAykVR4dGykxZVYJ9ZUYF9dXRx0Gx4KIJy\n8Zrk0eLCUpyZTZ0CxomtX7CP2s388MJE6hTADykqbEl2+mzqCDBObP2CfXRieTLPvNagdBQV\ntiROTcfVtdQpYOxYUYG91ms11mZ7qVMA96GosFX5qTMhdxwK7AsXPsJ+ObU6zLwlAKWkqLBl\nnW52+FjqEDAe7s2oeH6CPTQz0V6Z7qZOAdyfosI2ZEdPhHYndQoYA/cufEwaA2othnDmwDB1\nCuCBFBW2o9FwVDHsi+LRHwLszNpcf9hrpU4BPJCiwvZkBw7FofsfYY/d2fpl6zzskUYeTy5P\npk4BPIyiwjbFmJ1+MnUIGANFEWz9gj1zYmmy3XRCDJSaosK2xdn5uLCUOgWMBfeowF7otRvr\nbniE0lNUeBzZE2dD5osH9lIxCsH5xLAnzhyYsq8Sys+zJo8j9ieyI8dTp4BaK0LQU2APzA3a\ni0MnWEIFKCo8puzYydjrp04B9bU5o6KqwG6KMZw54EgYqAZFhceV59mTP0kdAmpOTYHddXh+\nMOg2U6cAtkRR4fHF+cW4uJw6BdSUU79gt3Vb+YkVRxJDZSgq7Eh+5lzIHe8Ie+B2UUmdAurk\nzNpUI/OqgspQVNiZbi87djJ1CKglMyqwm5amukvDbuoUwDYoKuxUduR4nBikTgG1c+dyemDn\n8iw+cWCYOgWwPYoKO5Zl2dmnUoeA2jGjArvn1Mqw12qkTgFsj6LCLogzc3FlLXUKqBfHE8Mu\nmew21+fdQw/Vo6iwO/IzZ0OrlToF1MftjV9qCuxQDOHswWmVH6pIUWGXtNr5mXOpQ0CNFK6m\nh11waH5iuu99NKgkRYVdE1fW4tJK6hRQF44nhh3rNPNTLk6BylJU2E35kz8JTTf+wq4xowI7\ncfbgdCP3qANV5dXLrmp3stNnU4eAWrCiAjuzNttfHHZSpwAen6LCLsvWDsX5xdQpoPrcowI7\n0GnmLk6BqlNU2H352adCw3H1sDPuUYEdOHdoumnTF1Sc1zB7oNvLTp1JHQKqzgHF8JgOzvUX\nJm36gspTVNgT2aEjcW4hdQqostunE+spsE2dZn561aYvqANFhb2S2QAGO2GYHh7LT2z6grrw\nSmavxF4/e8IJYPC4DNPD9h2a68/b9AV1oaiwh7K19bi8mjoFVJS9X7A93VZ+enUqdQpg1ygq\n7K38yadC25tbsH23e0rqFFAVMYSnDs00ci8aqA9FhT3WauVPPZs6BFSQ44lhO44uDWYH7dQp\ngN2kqLDn4txCtn40dQqoHDMqsFVTvdbJ5cnUKYBdpqiwH7LTT8ZJh0XCdjj1C7amkcWnD89E\n649QO4oK+yLLsqeeC1meOgdUR2FIBbbk7MHpfttp+FBDigr7JA4ms5OnU6eAiomaCjzUgZne\n6kwvdQpgTygq7J/s8LE4N586BVSEe1TgUXrtxpNrziOG2lJU2Ecx5k8957Ri2BKnfsFDxRie\nXp9puIQe6svLm/3V7uRPP+/hCx7Nigo81Mnl4XS/lToFsIcUFfZbnJ3LThhWgS3R6eG+5gbt\no0uD1CmAvaWokEB29EScX0ydAspt89QvVQV+qNPMnz4867UBtaeokEKM+dPPha5zWuDB3KMC\n9xNjfObwTLvhAQbqz+ucRJqt/JkXQuYrEB7EjArcx5kDw5mJduoUwH7wmEgycWo6O/lE6hRQ\nVnoK/MjKdG99fiJ1CmCfKCqklB05HpdWUqeAUnI8MXzfRKdx7uB06hTA/lFUSCw/90zs9VOn\ngNIpHE8M39HI4nNHZhu57g5jRFEhtWYze/bF0GikzgFlY5ge7vnJoZmJTjN1CmBfKSqkFyeH\n+U+eTZ0CSmZzRUVVgXBkcbA83U2dAthvigqlEJdWsuOnUqeAMtFTIIQQwsxE+/TKZOoUQAKK\nCmWRnTgdl1dTp4DysPULQq/VeO7IbHSsBIwlRYUSyc89EwfeNoMQwt2tXzC+Gnl8/uhsy92O\nMK68+CmTRiN7/uXQaqXOASWgqDDeYghPr88MugboYXwpKpRL7Pbyp593eQS48JExd2p1uDg0\nQA9jTVGhdOLcQnbqydQpIDkzKoyvAzO9o4uD1CmAxBQVyig7ciweOJg6BSR1e+uXpsL4me63\nzh1yAz2gqFBW+dmn4+x86hSQjhkVxlK3lT93ZDazARhQVCivLMuffdEhYADjo5HF54/OtZt5\n6iBAKSgqlFizmb/wSugYpmQsFWZUGC8xhqcPz0465gu4Q1Gh3Drd/LmXQqOROgfsO1u/GDNn\n16YXh53UKYASUVQouziccmAx42izqPjKZyycWJ48ONdPnQIoF0WFCogLS9mTT6VOAcCeWJ3p\nnVg2kQj8kKJCNWQH17Mjx1OngH1k6xfjYWGy89ShmdQpgDJSVKiM7PSTcXUtdQrYL+5RYQwM\ne61njsza2wvcl6JCleTnXK7C+HDqFzXXazdeODrbyHyZA/enqFApWZ4/91KccmMxY8DOL2qt\n1checGUK8FCKClXTaOTPvxInBqlzwB4zo0J95Vl84ejcRMfR88DDKCpUUKuVv/jT2OulzgF7\nSlGhnrIYnzsyO9VvpQ4ClJ2iQjV1OtmLPw1tV4NRX1ZUqKMY47NHZuYnffcGHk1Roapir5+/\n+Gpoek+OmtJTqJ0YwlPr04vDbuogQDUoKlRYHEzmL7wSGnY5U0dWVKidswenV6ft2gW2SlGh\n2uLUdP7sSyHzlUz9KCrUyhOrw4Nz/dQpgCrxeEflxbn5/OnngwvDqJfCigo1cmpleGTRaY3A\n9igq1EFcWtFVqBtFhbo4vjR5bElLAbZNUaEm4vKqrkKt3OkpNjZSaevzEydXJlOnACrJD0Dq\n405X8VVNPWw2legbNZW1Pj/x5NpU6hRAVfn5R63E5dX86eesq1AHd7Z+WVGhog4vaCnAjvgB\nSN3YA0ZNFHdXVKB6jiwOzhzQUoAdUVSoIV2FOsl8IVM1RxcHT6wOU6cAKk9RoZ50FSrv7qlf\nxq6olKOLg9NaCrAbXOlNbcXl1bwoNn7zy1CMUmeB7bs7o6JuUx0nlidPLDvjC9gd3qijzuLK\ngfy5F0Oepw4C26eoUDUntRRgVykq1FxcWMqffyU0LB5SVXoKlXBqZXhcSwF2laJC/cXZufyl\n10KrnToIbMfdU7+MWlFuMYazB6fdPQ/sOkWFsRCHU/nLr4VON3UQ2DJFhSrIYnzm8OyhuX7q\nIEANKSqMizgxaLzys9ifSB0EtsiMCmXXyOLzR2eXp7wHBOwJRYVx0u3lL/8sTjo3kyq4ezqx\nFRVKqdnIXjw+Pz/ZSR0EqC1FhTHTbucvvRanZ1LngEe5t/UrbQ64j24rf/XEwnS/lToIUGeK\nCuOn2cxfeDXOL6bOAQ91t6ikjQE/Mug0Xz25MNFxmiKwtxQVxlKjkT//cnZwPXUOeIg7MyqG\nVCiTqX7r5RPznab7qYA9p6gwrmLMzj6dnTlnYw0ldXdGxZoKpbEw7Lx8fL7V8PAA7Afrtoy1\nbP1obHc2fvPLMNpInQV+wIwK5bI+P3HmwJQvSGDfKCqMu7iEklZbAAAPm0lEQVS8mrdaG7/8\nl3DzZuos8B2G6SmNGMMTq1OHFxzvDuwrq7cQ4ux8/srrodtLHQS+o3CPCqWQZ/G5I7NaCrD/\nFBUI4fZ1kK++HodTqYPAHXeKinO/SKjTzF85Mb84dKUjkICiAne0O/lLrzm2mLK4t6KiqJDG\nZLf56smFYc9lKUAaigp8x+1ji48cT50D7tFTSGJ+svPKifluyzHEQDKG6eH7YsxOPxknJzd+\n9xtHgZGSYXrSOTjXP7s27WsPSEtRgfuIqwfzweTGL/4lXLuaOgtj6+7N9J4W2T9ZjGfWpg7N\n9VMHAbD1Cx4gTk41Xn09Ts2kDsKYujtL711t9k27mb98Yl5LAUpCUYEHa3fyl1/L1g6lzsFY\nsvWL/TXdb/3s1MJ03+g8UBa2fsFDZVl27pkwNTP6/W++c1ws7L3C1i/2z8G5/pNrU46YA0pF\nUYFHyw6ux25349f/xe317KO7KyqeHdlDWYxPrk0dtN0LKB9bv2BL4vxi/tO/cyMk++fuPSpp\nY1BrnWb+8ol5LQUoJz8BYatir5e/8np26HDqIIwHw/TssZmJ1muGUoASs/ULtiPLsiefinML\nG7/9VbhlGxh7SlNhDx1emDi9OjSUApSZogLbFpdW8onB6Fc/Ly5fSp2F+rq79cuTJLuq2cie\nOjS9OOymDgLwCLZ+weOIE4P81X/l5GL20L1Tv2DXTPdbPzu1qKUAlWBFBR5Xnmfnnomzcxtv\n/CZsbKROQ+3cPQ1bU2GXHF6YeGJ16Bw5oCoUFdiRuHowH0yOfvXz4sqV1FmoGfeosGtajeyp\nQzMLw07qIADbYOsX7FScnMp/+ne2gbHL3EzPLpmdaP/s9KKWAlSOFRXYDY1Gdu6ZuLC08btf\nh5s3UqehFgoXPrILbPcCqktRgV0Tl1Ya0zMbv/1V8fmnqbNQH54weTy9VuOp9emZiXbqIACP\nydYv2FXtTv7CK9mZcyHz4mLHiiKYpeexLE93Xzu9oKUAlWZFBXZftn40zs6PfvOL4tLXqbNQ\nZUURYrRph21pNbJzB6eXphxADFSeN31hT8TBZP7q69nho6mDUGnFoz8EvmN+svOz04taClAP\nVlRgz2R59sS5OLew8cZvwvVrqdNQQUUIIWRWVNiCPIunVoaHFyZSBwHYNVZUYG/FhaXG6//a\n4cU8jsKKClsy3W/97PSilgLUjBUV2HuNZnbumbh8YOONX4drV1OnoTqKIoSQWVDhwbIsnlye\nPLIwsPAG1I+iAvskzi80Xv83o7ffHP31be+UszW+TniYmYn2Tw5N99t+lAP15Lsb7KM8z06d\niQtLo9/9qrjyTeo0lF4RggsfuZ9Gnp1eHR6a66cOArCHFBXYb3FmNn/t7yyt8GjuUeF+Foed\nJ9emu608dRCAvaWoQAp3l1be+HXxzeXUaSir2z1WU+GOdjM/vTo8MNNLHQRgPygqkMzm0spf\n3x6981bY2EgdhxKyosI9y9Pds2vTrYbjOoFxoahAUlmWHTsZV9ZGf/ht8dknqdNQMre3fplR\nGXv9duPswem5QTt1EIB9pahAerHXy59/ufjsk9Hvf1s4v5g7ihCiFZXxlmfx+NLkkcUJ934C\nY0hRgbKIC0v56/ObO8FGo9RxKAEzKuNtear7xIEpQ/PA2FJUoEzyPDt+Kq4cGP3+t8UXn6VO\nQ2pO/RpX/XbjybWp+clO6iAAKSkqUDqxP5G/+GrxwfmNN/8Qvr2eOg7pmFEZP3kWjy4Oji0N\n7PUCUFSgpOKBg42lldFf/jx69x1ngo0zj6vjY3Wmd3p12Gna6wUQgqICpdZoZCefyA6ub7z5\nh+KjD1KnYd+ZURkbw17zzIGpmQnnegHco6hA6XV7+dPPF4ePjf70RvHlhdRp2EdmVMZAt5Wf\nXBmuzvR8ogF+QFGBaohT0/nLPys++2T0h98WVx1hPCZuFxVPsPXUyLPjS4P1+Yk88ykGuA9F\nBaokLizls/Oj9/4yeuetcOtW6jjsMVu/airGuDbbO7kybLtmHuDBFBWomjzPjp7IDhwavfPW\n6Py7blypMz2ljhaHnScOTPXbfv4CPIJvlFBN7XZ25lx25Njo7TdHH5zffOudmvFprZepfuuJ\n1aGJeYAtUlSgyrq97Nwz8eiJ0Vt/LD7+MHUadpt7VOpi2GudWJ5cHLrAEWAbFBWovNifyJ95\nobh8afT2m+pKvdj7VXmT3eaJlcmlYTd1EIDqUVSgJuJgMn/mheLil6M3/1hc+Dx1HHaDnlJl\ng27zxPLk8pSKAvCYFBWolTg1k7/00+Lzz0bvvOnSlcpzj0o1TXSax5YGq9M9u/YAdkJRgRqK\n8wv5/ELx5YXRO28Vn3+aOg6Py/HEVdNrNY4tDdZm+yoKwM4pKlBbcWY2f+GV4uuvRu/8ufj0\nY0dIVZALHytjsts8ujhYsYoCsHsUFai5OJzOn32xuHxp9Ne3iw/fV1eqxCerCmYH7WOLg/lJ\nJ3oB7DJFBcZCHEzmP3m2OHG6ePed0d/eC6ON1InYgts7v7xDX0oxhKWp7tHFwVS/lToLQD0p\nKjBGYrcXnziXHTk+eu+vo/Pvhps3UyfioayolFIW48p099jSYKLTTJ0FoM4UFRg/nW526kx2\n/NTog/PFu+8UV75JHYgHcepXuTTyuDbbP7o46DTz1FkA6k9RgXGV59mhw+HQ4eKzT0bv/qX4\n4rPUgfgRKyql0W83Ds71D871m3mWOgvAuFBUYNzFhaV8Yam48k3xt7+Ozr8XNoyvlEs0pJJO\nDGFh2F2f75uVB9h/igoQQgixPxGfOJcdOzk6/97ob38N16+nToQVlZSajezgbP/QfL/X8oMS\nIA3ff4HvaLWzYyezoyeKzz4ZnX+v+PxTz8op+Y+fwkSncWhu4uBcP8+sZQGkpKgAPxJjXFzO\nF5fDtauj9/82ev9v4fq11JnGUlEExxPvlzyLS1PdQ3P9mYl26iwAhKCoAA/T7WUnTmfHTxUX\nPh+df6/45CPv8e+nwpFf+2Kq11qb669Mdw3KA5SKogI8SoxxbiGfW7DAst/Uwr3UamSrM72D\ns/1B13UoAGWkqABbdneB5eKXxQfnRx99EG7dSp2p1gr3qOy+GMLsoH1gtr881TWFAlBmigqw\nTTHG6dk4PZs9ca745KPRh+8XX3zmvf89sflf1cP07hh0mqszvQOzPdc1AlSCogI8rjyPq2v5\n6lr49vroow+KD98vvr6YOlPNGKbfBb12Y2W6uzrds8ULoFoUFWDH2p3s8LFw+FjxzeXiw/PF\nhx8U166mzlQL1ql2oNPMl6e7q9O9qX4rdRYAHoeiAuyaODGIJ8+Ek2eKry8Wn3xYfPxRceWb\n1KGqTE/ZvmaeLQ47y9O9hcmOxSiASlNUgN0Xh1NxOBVOnimuXik++6T4+MPiywupQ1WRprJV\nnWa+OOwsTXXnBu2ooADUgqIC7KHY68f1o2H9aHH1SvHxR8UnHxYXv0odqjps/XqUQbe5NOwu\nTnWmevZ3AdSNogLsh9jrx6PHw9Hj4dq10WcfF599Wlz4PGxspM5VbkUIhul/JMYwM9FeHHaX\nhp1e208xgNryLR7YX91uduhIOHQkjDaKCxeKzz8tPvvEKMsDWFG5p9XI5gbthWF3cbLTbLhC\nHqD+FBUgkSyP8wtxfiE8cba4eqX4/FPLLD809lu/shinJ1rzg87cZHvYa1lbAhgrigqQXuz1\n43eXWb74rLjwRXHp4rg/qY/rb3/Qac5NtucHndlB2+XxAGNLUQHK5O4ySwjh1q3iyy+KC18U\nX35RfD2WpaUowtjcS99t5TMT7blBe36y4+Z4AIKiApRXoxEXluLCUghjXlrqWVViCINuc2ai\nPd1vzUy0uy3lBIDvUVSAKvhBafnqy+Lrr4qvviwufhVufJs63J6pXR/LsjjVa81MtGb67emJ\nVjM3Ew/AAykqQNU0Gve2h4VQXLsavvqyuPhVcfGr4tLFWs3iV7+oxBgGneaw1xr2Nv83c9wy\nAFujqADVFru90O3FlQMhhFCMikuXiotfha8vFpe/Li5fqnhvuX2RSuoU2/HdZjLVaw26TdPw\nADweRQWokZjF4VQcTm3+ZVEUV6+ES18Xly8Vly+FS18XV68kzbdNVVhQaTfziU5j0GkOus1h\ntznZbWaaCQC7QVEB6ivG2J8I/Ym4vLr5K7duFd9cKi5dCpcvFVe+CVe+Ka5dLe8Oq/Kd+tVp\n5oNu83Yzuf2/7l4EYI8oKsA4aTTi1Eycmrn3K8WouHo1XPmmuPJNuHKluHq7vVwrR3tJliHG\n0GnmvXaj12r0Wnmv3ei1G4NOo2H8HYD9oqgA4y1mm6su3/3F0ai4djVcuxquXy+uXQ3fXi+u\nXQvXrxXXr4UbN/Yv296XpTyL7WbeaWadZqPXznutRq/d6LXybiuPpt4BSEpRAfiRbLO9hB/v\nvBptFNeuhevXw/WrxY0b4caNcPNG8e234eaNcONGcfNGuHEjjEa7E2PHPaWZZ81G1sxju5G3\nmlmnmbcbebuZdZp5q5F3WnnDPAkAZaWoAGxHlj+ww9x161a4eaO4cSPcvBE2NsJoVNy8GUaj\nsHEr3LoVRqNw62axMQqjjXDr5n3ayK1boRiFEEKne/sX5gbtEEKWxdsnaN2+fiSL9/6ykcdm\nnjXyrPmdP7FNC4BKi0Up9mEDAADc4/02AACgdBQVAACgdBQVAACgdBQVAACgdBQVAACgdBQV\nAACgdBQVAACgdBQVAACgdBQVAACgdBQVAACgdBQVAACgdBQVAACgdBQVAACgdBQVAACgdBQV\nAACgdBQVAACgdBQVAACgdBQVAACgdBQVAACgdBQVAACgdBQVAACgdBQVAACgdBQVAACgdBQV\nAACgdBQVAACgdBQVAACgdBQVAACgdBQVAACgdBQVAACgdBQVAACgdBQVAACgdBQVAACgdBQV\nAACgdBQVAACgdBQVAACgdBQVAACgdBQVAACgdBQVAACgdBQVAACgdBQVAACgdBQVAACgdBQV\nAACgdBQVAACgdBQVAACgdBQVAACgdBQVAACgdBQVAACgdBQVAACgdBQVAACgdBQVAACgdBQV\nAACgdBQVAACgdBQVAACgdBQVAACgdBQVAACgdBQVAACgdBQVAACgdBQVAACgdBQVAACgdBQV\nAACgdBQVAACgdBQVAACgdBQVAACgdBQVAACgdBQVAACgdBQVAACgdBQVAACgdBQVAACgdBQV\nAACgdBQVAACgdBQVAACgdBQVAACgdBQVAACgdBQVAACgdBQVAACgdBQVAACgdBQVAACgdBQV\nAACgdBQVAACgdBQVAACgdBQVAACgdBQVAACgdBQVAACgdBQVAACgdP5/JP0aIBvbxJQAAAAA\nSUVORK5CYII="},"metadata":{"image/png":{"width":540,"height":420}}}]},{"cell_type":"markdown","source":"Target label is nearly evenly split.","metadata":{}},{"cell_type":"markdown","source":"# Exploring missing data\nSince distributions of train and test sets are similar, they will be joined before filling missing values.","metadata":{}},{"cell_type":"code","source":"#Joining datasets\ndf_joint <- df_train %>%\n    dplyr::select(-claim)\n\ndf_joint <- df_joint %>%\n    rbind(df_test)\n\ndf_joint <- df_joint %>%\n    dplyr::select(-id)\n\n\n#Checking how many cells are missing per column\nmissing_array <- c()\nfor(i in 2:length(df_joint)) {\n    missing_array[i-1] <- sum(is.na(df_joint[, i]))\n}\n\nsummary(missing_array)","metadata":{"execution":{"iopub.status.busy":"2021-09-27T22:45:14.453179Z","iopub.execute_input":"2021-09-27T22:45:14.454615Z","iopub.status.idle":"2021-09-27T22:45:18.795941Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  22956   23281   23385   23367   23465   23719 "},"metadata":{}}]},{"cell_type":"markdown","source":"All columns are missing nearly the same number of values.","metadata":{}},{"cell_type":"code","source":"#Checking how many cells are missing per row\nnas_per_row <- c(rep(0, nrow(df_joint)))\n\nfor(i in 1:length(df_joint))\nnas_per_row <- nas_per_row + as.numeric(is.na(df_joint[[i]]))\n\nsummary(nas_per_row)","metadata":{"execution":{"iopub.status.busy":"2021-09-27T22:45:18.798810Z","iopub.execute_input":"2021-09-27T22:45:18.800079Z","iopub.status.idle":"2021-09-27T22:45:19.721611Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    0.0     0.0     1.0     1.9     3.0    15.0 "},"metadata":{}}]},{"cell_type":"code","source":"#Distribution of missing rows throughout a column that was split into 10 parts\nk <- 1\nfor(i in seq(from = length(nas_per_row)/10, to = length(nas_per_row), by = length(nas_per_row)/10)) {\n    print(sum(nas_per_row[k:i]))\n    k <- i\n}\n#Sums of missing rows throughout partitions of the colum are similar","metadata":{"execution":{"iopub.status.busy":"2021-09-27T22:45:19.724432Z","iopub.execute_input":"2021-09-27T22:45:19.725687Z","iopub.status.idle":"2021-09-27T22:45:19.761537Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"[1] 274882\n[1] 276128\n[1] 275690\n[1] 275902\n[1] 276501\n[1] 276023\n[1] 276287\n[1] 276660\n[1] 273370\n[1] 275561\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Missing values will be filled feature by feature - column by column.","metadata":{}},{"cell_type":"code","source":"#Normalizing function\ndf_joint <- BBmisc::normalize(df_joint, method = \"range\", range = c(-1,1))\n\n\n#Sampling data for training\nsplitprop <- 0.25 * nrow(na.omit(df_joint))\n\nset.seed(2022)\n\nsplit <- sample(nrow(na.omit(df_joint)), splitprop)","metadata":{"execution":{"iopub.status.busy":"2021-09-27T22:45:19.764323Z","iopub.execute_input":"2021-09-27T22:45:19.765594Z","iopub.status.idle":"2021-09-27T22:45:38.399892Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"Checking the summary of the sampled data frame.","metadata":{}},{"cell_type":"code","source":"head(na.omit(df_joint)[split, ])\nsummary(na.omit(df_joint)[split, ])","metadata":{"execution":{"iopub.status.busy":"2021-09-27T22:45:38.403068Z","iopub.execute_input":"2021-09-27T22:45:38.404476Z","iopub.status.idle":"2021-09-27T22:45:43.967477Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/html":"<table class=\"dataframe\">\n<caption>A data.frame: 6 × 118</caption>\n<thead>\n\t<tr><th></th><th scope=col>f1</th><th scope=col>f2</th><th scope=col>f3</th><th scope=col>f4</th><th scope=col>f5</th><th scope=col>f6</th><th scope=col>f7</th><th scope=col>f8</th><th scope=col>f9</th><th scope=col>f10</th><th scope=col>⋯</th><th scope=col>f109</th><th scope=col>f110</th><th scope=col>f111</th><th scope=col>f112</th><th scope=col>f113</th><th scope=col>f114</th><th scope=col>f115</th><th scope=col>f116</th><th scope=col>f117</th><th scope=col>f118</th></tr>\n\t<tr><th></th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>⋯</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n</thead>\n<tbody>\n\t<tr><th scope=row>636594</th><td>-0.2150362446</td><td> 0.353508739</td><td> 0.2322631</td><td>-0.8550451</td><td>-0.97307065</td><td> 0.248604583</td><td>-0.85051846</td><td>-0.8574325</td><td>-0.6827289</td><td>-0.4541839</td><td>⋯</td><td>-0.141176117</td><td>0.06677559</td><td>-0.40727609</td><td>-0.7578823</td><td>-0.5625588</td><td>-0.6998294</td><td> 0.06389452</td><td> 0.1477368</td><td> 0.03987296</td><td>-0.6519573</td></tr>\n\t<tr><th scope=row>880120</th><td>-0.0009149131</td><td> 0.479942109</td><td>-0.6180438</td><td>-0.8641063</td><td> 0.34246603</td><td> 0.170552760</td><td> 0.01178469</td><td>-0.4167734</td><td>-0.7727326</td><td>-0.5124710</td><td>⋯</td><td>-0.007551749</td><td>0.90529564</td><td> 0.04948648</td><td>-0.3113375</td><td> 0.5183332</td><td>-0.5336194</td><td>-0.34807302</td><td>-0.8567180</td><td>-0.03531303</td><td>-0.4051444</td></tr>\n\t<tr><th scope=row>1069263</th><td>-0.0019353931</td><td>-0.025717148</td><td>-0.5860175</td><td>-0.8770543</td><td> 0.69665131</td><td> 0.184727363</td><td>-0.58892664</td><td>-0.9594640</td><td>-0.4983779</td><td>-0.2380697</td><td>⋯</td><td>-0.510281272</td><td>0.47537902</td><td>-0.42430838</td><td>-0.7547503</td><td>-0.3373712</td><td>-0.7240530</td><td> 0.07829615</td><td> 0.4318046</td><td>-0.54485304</td><td>-0.3233443</td></tr>\n\t<tr><th scope=row>263913</th><td> 0.1603561123</td><td> 0.722418080</td><td>-0.5115641</td><td>-0.6057523</td><td> 0.63632285</td><td>-0.003999501</td><td>-0.51852638</td><td> 0.1077120</td><td>-0.7977093</td><td>-0.5304801</td><td>⋯</td><td> 0.397352385</td><td>0.88184053</td><td>-0.38082266</td><td>-0.7860286</td><td>-0.4632906</td><td>-0.7252833</td><td>-0.51582150</td><td>-0.9442308</td><td>-0.53383516</td><td>-0.2002757</td></tr>\n\t<tr><th scope=row>798051</th><td>-0.1965022169</td><td>-0.009685679</td><td>-0.5341384</td><td>-0.7631007</td><td> 0.29742410</td><td> 0.177323961</td><td>-0.91458860</td><td>-0.5120324</td><td>-0.7849974</td><td>-0.3981201</td><td>⋯</td><td>-0.463950152</td><td>0.59311979</td><td>-0.60228892</td><td>-0.7672712</td><td>-0.1449009</td><td>-0.7275900</td><td>-0.18661258</td><td>-0.8689299</td><td>-0.54576385</td><td>-0.8697323</td></tr>\n\t<tr><th scope=row>373229</th><td>-0.1457315786</td><td> 0.630756671</td><td>-0.6271699</td><td>-0.8884140</td><td> 0.07690705</td><td>-0.448180344</td><td>-0.18073398</td><td>-0.8183287</td><td> 0.5040489</td><td>-0.5290302</td><td>⋯</td><td>-0.145292914</td><td>0.61546950</td><td>-0.57848083</td><td>-0.7620268</td><td>-0.3276222</td><td>-0.1951443</td><td>-0.50608519</td><td>-0.8598743</td><td> 0.26728190</td><td>-0.3715263</td></tr>\n</tbody>\n</table>\n","text/markdown":"\nA data.frame: 6 × 118\n\n| <!--/--> | f1 &lt;dbl&gt; | f2 &lt;dbl&gt; | f3 &lt;dbl&gt; | f4 &lt;dbl&gt; | f5 &lt;dbl&gt; | f6 &lt;dbl&gt; | f7 &lt;dbl&gt; | f8 &lt;dbl&gt; | f9 &lt;dbl&gt; | f10 &lt;dbl&gt; | ⋯ ⋯ | f109 &lt;dbl&gt; | f110 &lt;dbl&gt; | f111 &lt;dbl&gt; | f112 &lt;dbl&gt; | f113 &lt;dbl&gt; | f114 &lt;dbl&gt; | f115 &lt;dbl&gt; | f116 &lt;dbl&gt; | f117 &lt;dbl&gt; | f118 &lt;dbl&gt; |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| 636594 | -0.2150362446 |  0.353508739 |  0.2322631 | -0.8550451 | -0.97307065 |  0.248604583 | -0.85051846 | -0.8574325 | -0.6827289 | -0.4541839 | ⋯ | -0.141176117 | 0.06677559 | -0.40727609 | -0.7578823 | -0.5625588 | -0.6998294 |  0.06389452 |  0.1477368 |  0.03987296 | -0.6519573 |\n| 880120 | -0.0009149131 |  0.479942109 | -0.6180438 | -0.8641063 |  0.34246603 |  0.170552760 |  0.01178469 | -0.4167734 | -0.7727326 | -0.5124710 | ⋯ | -0.007551749 | 0.90529564 |  0.04948648 | -0.3113375 |  0.5183332 | -0.5336194 | -0.34807302 | -0.8567180 | -0.03531303 | -0.4051444 |\n| 1069263 | -0.0019353931 | -0.025717148 | -0.5860175 | -0.8770543 |  0.69665131 |  0.184727363 | -0.58892664 | -0.9594640 | -0.4983779 | -0.2380697 | ⋯ | -0.510281272 | 0.47537902 | -0.42430838 | -0.7547503 | -0.3373712 | -0.7240530 |  0.07829615 |  0.4318046 | -0.54485304 | -0.3233443 |\n| 263913 |  0.1603561123 |  0.722418080 | -0.5115641 | -0.6057523 |  0.63632285 | -0.003999501 | -0.51852638 |  0.1077120 | -0.7977093 | -0.5304801 | ⋯ |  0.397352385 | 0.88184053 | -0.38082266 | -0.7860286 | -0.4632906 | -0.7252833 | -0.51582150 | -0.9442308 | -0.53383516 | -0.2002757 |\n| 798051 | -0.1965022169 | -0.009685679 | -0.5341384 | -0.7631007 |  0.29742410 |  0.177323961 | -0.91458860 | -0.5120324 | -0.7849974 | -0.3981201 | ⋯ | -0.463950152 | 0.59311979 | -0.60228892 | -0.7672712 | -0.1449009 | -0.7275900 | -0.18661258 | -0.8689299 | -0.54576385 | -0.8697323 |\n| 373229 | -0.1457315786 |  0.630756671 | -0.6271699 | -0.8884140 |  0.07690705 | -0.448180344 | -0.18073398 | -0.8183287 |  0.5040489 | -0.5290302 | ⋯ | -0.145292914 | 0.61546950 | -0.57848083 | -0.7620268 | -0.3276222 | -0.1951443 | -0.50608519 | -0.8598743 |  0.26728190 | -0.3715263 |\n\n","text/latex":"A data.frame: 6 × 118\n\\begin{tabular}{r|lllllllllllllllllllll}\n  & f1 & f2 & f3 & f4 & f5 & f6 & f7 & f8 & f9 & f10 & ⋯ & f109 & f110 & f111 & f112 & f113 & f114 & f115 & f116 & f117 & f118\\\\\n  & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & ⋯ & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n\\hline\n\t636594 & -0.2150362446 &  0.353508739 &  0.2322631 & -0.8550451 & -0.97307065 &  0.248604583 & -0.85051846 & -0.8574325 & -0.6827289 & -0.4541839 & ⋯ & -0.141176117 & 0.06677559 & -0.40727609 & -0.7578823 & -0.5625588 & -0.6998294 &  0.06389452 &  0.1477368 &  0.03987296 & -0.6519573\\\\\n\t880120 & -0.0009149131 &  0.479942109 & -0.6180438 & -0.8641063 &  0.34246603 &  0.170552760 &  0.01178469 & -0.4167734 & -0.7727326 & -0.5124710 & ⋯ & -0.007551749 & 0.90529564 &  0.04948648 & -0.3113375 &  0.5183332 & -0.5336194 & -0.34807302 & -0.8567180 & -0.03531303 & -0.4051444\\\\\n\t1069263 & -0.0019353931 & -0.025717148 & -0.5860175 & -0.8770543 &  0.69665131 &  0.184727363 & -0.58892664 & -0.9594640 & -0.4983779 & -0.2380697 & ⋯ & -0.510281272 & 0.47537902 & -0.42430838 & -0.7547503 & -0.3373712 & -0.7240530 &  0.07829615 &  0.4318046 & -0.54485304 & -0.3233443\\\\\n\t263913 &  0.1603561123 &  0.722418080 & -0.5115641 & -0.6057523 &  0.63632285 & -0.003999501 & -0.51852638 &  0.1077120 & -0.7977093 & -0.5304801 & ⋯ &  0.397352385 & 0.88184053 & -0.38082266 & -0.7860286 & -0.4632906 & -0.7252833 & -0.51582150 & -0.9442308 & -0.53383516 & -0.2002757\\\\\n\t798051 & -0.1965022169 & -0.009685679 & -0.5341384 & -0.7631007 &  0.29742410 &  0.177323961 & -0.91458860 & -0.5120324 & -0.7849974 & -0.3981201 & ⋯ & -0.463950152 & 0.59311979 & -0.60228892 & -0.7672712 & -0.1449009 & -0.7275900 & -0.18661258 & -0.8689299 & -0.54576385 & -0.8697323\\\\\n\t373229 & -0.1457315786 &  0.630756671 & -0.6271699 & -0.8884140 &  0.07690705 & -0.448180344 & -0.18073398 & -0.8183287 &  0.5040489 & -0.5290302 & ⋯ & -0.145292914 & 0.61546950 & -0.57848083 & -0.7620268 & -0.3276222 & -0.1951443 & -0.50608519 & -0.8598743 &  0.26728190 & -0.3715263\\\\\n\\end{tabular}\n","text/plain":"        f1            f2           f3         f4         f5         \n636594  -0.2150362446  0.353508739  0.2322631 -0.8550451 -0.97307065\n880120  -0.0009149131  0.479942109 -0.6180438 -0.8641063  0.34246603\n1069263 -0.0019353931 -0.025717148 -0.5860175 -0.8770543  0.69665131\n263913   0.1603561123  0.722418080 -0.5115641 -0.6057523  0.63632285\n798051  -0.1965022169 -0.009685679 -0.5341384 -0.7631007  0.29742410\n373229  -0.1457315786  0.630756671 -0.6271699 -0.8884140  0.07690705\n        f6           f7          f8         f9         f10        ⋯\n636594   0.248604583 -0.85051846 -0.8574325 -0.6827289 -0.4541839 ⋯\n880120   0.170552760  0.01178469 -0.4167734 -0.7727326 -0.5124710 ⋯\n1069263  0.184727363 -0.58892664 -0.9594640 -0.4983779 -0.2380697 ⋯\n263913  -0.003999501 -0.51852638  0.1077120 -0.7977093 -0.5304801 ⋯\n798051   0.177323961 -0.91458860 -0.5120324 -0.7849974 -0.3981201 ⋯\n373229  -0.448180344 -0.18073398 -0.8183287  0.5040489 -0.5290302 ⋯\n        f109         f110       f111        f112       f113       f114      \n636594  -0.141176117 0.06677559 -0.40727609 -0.7578823 -0.5625588 -0.6998294\n880120  -0.007551749 0.90529564  0.04948648 -0.3113375  0.5183332 -0.5336194\n1069263 -0.510281272 0.47537902 -0.42430838 -0.7547503 -0.3373712 -0.7240530\n263913   0.397352385 0.88184053 -0.38082266 -0.7860286 -0.4632906 -0.7252833\n798051  -0.463950152 0.59311979 -0.60228892 -0.7672712 -0.1449009 -0.7275900\n373229  -0.145292914 0.61546950 -0.57848083 -0.7620268 -0.3276222 -0.1951443\n        f115        f116       f117        f118      \n636594   0.06389452  0.1477368  0.03987296 -0.6519573\n880120  -0.34807302 -0.8567180 -0.03531303 -0.4051444\n1069263  0.07829615  0.4318046 -0.54485304 -0.3233443\n263913  -0.51582150 -0.9442308 -0.53383516 -0.2002757\n798051  -0.18661258 -0.8689299 -0.54576385 -0.8697323\n373229  -0.50608519 -0.8598743  0.26728190 -0.3715263"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"       f1                 f2                f3                f4         \n Min.   :-1.00000   Min.   :-1.0000   Min.   :-0.9981   Min.   :-0.9784  \n 1st Qu.:-0.21344   1st Qu.: 0.1304   1st Qu.:-0.6005   1st Qu.:-0.8329  \n Median :-0.14359   Median : 0.5184   Median :-0.5662   Median :-0.6870  \n Mean   :-0.14264   Mean   : 0.3583   Mean   :-0.4548   Mean   :-0.5951  \n 3rd Qu.:-0.04944   3rd Qu.: 0.7749   3rd Qu.:-0.4407   3rd Qu.:-0.4580  \n Max.   : 0.95985   Max.   : 1.0000   Max.   : 0.9620   Max.   : 0.9518  \n       f5                f6                 f7                 f8         \n Min.   :-0.9995   Min.   :-0.98145   Min.   :-0.98921   Min.   :-0.9971  \n 1st Qu.:-0.1127   1st Qu.:-0.02572   1st Qu.:-0.75182   1st Qu.:-0.8773  \n Median : 0.1956   Median : 0.03551   Median :-0.41183   Median :-0.6737  \n Mean   : 0.1128   Mean   : 0.06079   Mean   :-0.34953   Mean   :-0.5849  \n 3rd Qu.: 0.4943   3rd Qu.: 0.14375   3rd Qu.:-0.04063   3rd Qu.:-0.3981  \n Max.   : 0.9833   Max.   : 1.00000   Max.   : 0.96422   Max.   : 0.9721  \n       f9               f10               f11                 f12         \n Min.   :-1.0000   Min.   :-0.9819   Min.   :-0.985164   Min.   :-0.9949  \n 1st Qu.:-0.7845   1st Qu.:-0.5273   1st Qu.:-0.008732   1st Qu.:-0.8986  \n Median :-0.7033   Median :-0.5097   Median : 0.099352   Median :-0.7043  \n Mean   :-0.4817   Mean   :-0.4345   Mean   : 0.045881   Mean   :-0.4945  \n 3rd Qu.:-0.2658   3rd Qu.:-0.4278   3rd Qu.: 0.138305   3rd Qu.:-0.1246  \n Max.   : 0.9726   Max.   : 0.9222   Max.   : 0.984440   Max.   : 0.9795  \n      f13                f14               f15                f16         \n Min.   :-0.95568   Min.   :-0.9928   Min.   :-0.98432   Min.   :-1.0000  \n 1st Qu.:-0.06071   1st Qu.:-0.8730   1st Qu.:-0.28565   1st Qu.:-0.8962  \n Median : 0.09564   Median :-0.6703   Median :-0.17685   Median :-0.8319  \n Mean   : 0.01422   Mean   :-0.5824   Mean   :-0.15527   Mean   :-0.5416  \n 3rd Qu.: 0.19247   3rd Qu.:-0.3873   3rd Qu.:-0.04527   3rd Qu.:-0.3352  \n Max.   : 0.98988   Max.   : 0.9768   Max.   : 0.97585   Max.   : 0.9826  \n      f17                f18               f19               f20         \n Min.   :-0.97054   Min.   :-1.0000   Min.   :-0.9965   Min.   :-0.9887  \n 1st Qu.:-0.15366   1st Qu.:-0.9128   1st Qu.:-0.6271   1st Qu.:-0.8822  \n Median :-0.08106   Median :-0.6843   Median :-0.4293   Median :-0.8095  \n Mean   :-0.07241   Mean   :-0.6430   Mean   :-0.3684   Mean   :-0.5160  \n 3rd Qu.:-0.01609   3rd Qu.:-0.4855   3rd Qu.:-0.1372   3rd Qu.:-0.3220  \n Max.   : 0.98945   Max.   : 0.9464   Max.   : 1.0000   Max.   : 0.9916  \n      f21               f22                f23               f24          \n Min.   :-0.9662   Min.   :-0.97446   Min.   :-0.9803   Min.   :-0.97251  \n 1st Qu.:-0.6970   1st Qu.:-0.09065   1st Qu.: 0.0673   1st Qu.:-0.22354  \n Median :-0.6426   Median :-0.04144   Median : 0.2942   Median : 0.07543  \n Mean   :-0.5038   Mean   :-0.04345   Mean   : 0.1904   Mean   :-0.04242  \n 3rd Qu.:-0.4280   3rd Qu.: 0.00529   3rd Qu.: 0.4571   3rd Qu.: 0.09983  \n Max.   : 0.9980   Max.   : 0.96512   Max.   : 0.9868   Max.   : 0.95614  \n      f25               f26               f27                f28          \n Min.   :-0.9900   Min.   :-0.9931   Min.   :-0.97164   Min.   :-0.98668  \n 1st Qu.:-0.6938   1st Qu.:-0.9453   1st Qu.:-0.69714   1st Qu.:-0.86880  \n Median :-0.3118   Median :-0.8867   Median :-0.60893   Median :-0.57146  \n Mean   :-0.2454   Mean   :-0.6919   Mean   :-0.31642   Mean   :-0.42379  \n 3rd Qu.: 0.1560   3rd Qu.:-0.6485   3rd Qu.:-0.04367   3rd Qu.:-0.04992  \n Max.   : 0.9793   Max.   : 1.0000   Max.   : 0.98872   Max.   : 0.97485  \n      f29               f30               f31                  f32         \n Min.   :-0.9997   Min.   :-0.9960   Min.   :-0.9753067   Min.   :-1.0000  \n 1st Qu.:-0.9870   1st Qu.:-0.8064   1st Qu.: 0.0007669   1st Qu.:-0.5231  \n Median :-0.8498   Median :-0.6101   Median : 0.1760340   Median :-0.4720  \n Mean   :-0.3991   Mean   :-0.5517   Mean   : 0.1070895   Mean   :-0.3596  \n 3rd Qu.: 0.6421   3rd Qu.:-0.3891   3rd Qu.: 0.2519614   3rd Qu.:-0.3087  \n Max.   : 0.9978   Max.   : 0.9805   Max.   : 0.9860853   Max.   : 0.9589  \n      f33               f34                 f35               f36         \n Min.   :-0.9819   Min.   :-0.985571   Min.   :-0.9912   Min.   :-0.9721  \n 1st Qu.:-0.6136   1st Qu.:-0.296607   1st Qu.:-0.7618   1st Qu.:-0.2488  \n Median :-0.5396   Median : 0.001226   Median :-0.6574   Median :-0.1998  \n Mean   :-0.4328   Mean   : 0.001178   Mean   :-0.4576   Mean   :-0.1558  \n 3rd Qu.:-0.3400   3rd Qu.: 0.300220   3rd Qu.:-0.2180   3rd Qu.:-0.1449  \n Max.   : 0.9538   Max.   : 0.983059   Max.   : 1.0000   Max.   : 0.9663  \n      f37               f38               f39               f40         \n Min.   :-0.9966   Min.   :-0.9804   Min.   :-0.9831   Min.   :-1.0000  \n 1st Qu.:-0.7478   1st Qu.:-0.5121   1st Qu.:-0.8555   1st Qu.:-0.9800  \n Median :-0.3688   Median :-0.4290   Median :-0.6857   Median :-0.8226  \n Mean   :-0.2895   Mean   :-0.3066   Mean   :-0.5876   Mean   :-0.3010  \n 3rd Qu.: 0.1208   3rd Qu.:-0.1842   3rd Qu.:-0.4225   3rd Qu.: 0.7339  \n Max.   : 0.9881   Max.   : 0.9861   Max.   : 0.9850   Max.   : 1.0000  \n      f41               f42               f43                 f44         \n Min.   :-0.9990   Min.   :-0.9953   Min.   :-0.951622   Min.   :-0.9760  \n 1st Qu.:-0.9019   1st Qu.:-0.8810   1st Qu.:-0.154463   1st Qu.:-0.8417  \n Median :-0.8153   Median :-0.7163   Median :-0.083945   Median :-0.6921  \n Mean   :-0.5452   Mean   :-0.2282   Mean   :-0.068002   Mean   :-0.5983  \n 3rd Qu.:-0.3598   3rd Qu.: 0.7563   3rd Qu.:-0.001189   3rd Qu.:-0.4443  \n Max.   : 0.9796   Max.   : 0.9991   Max.   : 0.927293   Max.   : 0.9595  \n      f45               f46               f47                f48         \n Min.   :-0.9961   Min.   :-0.9998   Min.   :-0.95866   Min.   :-0.9943  \n 1st Qu.:-0.6785   1st Qu.: 0.1060   1st Qu.:-0.26945   1st Qu.:-0.5005  \n Median :-0.5126   Median : 0.1479   Median :-0.03149   Median :-0.3653  \n Mean   :-0.3818   Mean   : 0.1474   Mean   :-0.06917   Mean   :-0.3306  \n 3rd Qu.:-0.1496   3rd Qu.: 0.2013   3rd Qu.: 0.12031   3rd Qu.:-0.2004  \n Max.   : 0.9759   Max.   : 0.9855   Max.   : 1.00000   Max.   : 0.9555  \n      f49                 f50                f51               f52         \n Min.   :-0.982378   Min.   :-0.99926   Min.   :-0.9962   Min.   :-0.9471  \n 1st Qu.:-0.585114   1st Qu.:-0.15421   1st Qu.:-0.5660   1st Qu.:-0.8197  \n Median :-0.424348   Median : 0.18309   Median :-0.5203   Median :-0.6398  \n Mean   :-0.258173   Mean   : 0.09706   Mean   :-0.3996   Mean   :-0.5500  \n 3rd Qu.:-0.003304   3rd Qu.: 0.49706   3rd Qu.:-0.3495   3rd Qu.:-0.3833  \n Max.   : 0.998640   Max.   : 0.99008   Max.   : 0.9611   Max.   : 0.9864  \n      f53               f54                 f55                f56         \n Min.   :-0.9870   Min.   :-0.996907   Min.   :-0.99484   Min.   :-0.9860  \n 1st Qu.:-0.6645   1st Qu.:-0.313555   1st Qu.:-0.10822   1st Qu.:-0.7686  \n Median :-0.6342   Median :-0.087378   Median : 0.14082   Median :-0.2975  \n Mean   :-0.5099   Mean   : 0.009739   Mean   : 0.07384   Mean   :-0.2559  \n 3rd Qu.:-0.4806   3rd Qu.: 0.382273   3rd Qu.: 0.36493   3rd Qu.: 0.2282  \n Max.   : 0.9835   Max.   : 0.975102   Max.   : 0.99589   Max.   : 0.9778  \n      f57                 f58               f59                f60         \n Min.   :-0.982941   Min.   :-0.9701   Min.   :-0.99722   Min.   :-0.9939  \n 1st Qu.:-0.287577   1st Qu.: 0.1969   1st Qu.:-0.69227   1st Qu.:-0.1026  \n Median : 0.006544   Median : 0.4715   Median :-0.49546   Median : 0.2933  \n Mean   : 0.007180   Mean   : 0.3240   Mean   :-0.32785   Mean   : 0.2031  \n 3rd Qu.: 0.302971   3rd Qu.: 0.6230   3rd Qu.:-0.08419   3rd Qu.: 0.5913  \n Max.   : 0.990907   Max.   : 0.9467   Max.   : 0.98935   Max.   : 0.9820  \n      f61               f62               f63               f64         \n Min.   :-0.9740   Min.   :-0.9692   Min.   :-0.9566   Min.   :-0.9725  \n 1st Qu.:-0.6225   1st Qu.:-0.7927   1st Qu.:-0.8141   1st Qu.:-0.7709  \n Median :-0.3982   Median :-0.6805   Median :-0.6475   Median :-0.6259  \n Mean   :-0.2636   Mean   :-0.5767   Mean   :-0.5603   Mean   :-0.5320  \n 3rd Qu.: 0.1001   3rd Qu.:-0.4635   3rd Qu.:-0.4050   3rd Qu.:-0.3925  \n Max.   : 0.9961   Max.   : 0.9855   Max.   : 1.0000   Max.   : 0.9572  \n      f65                f66                f67                f68         \n Min.   :-0.99795   Min.   :-0.95425   Min.   :-0.99730   Min.   :-1.0000  \n 1st Qu.:-0.70804   1st Qu.: 0.04957   1st Qu.:-0.79238   1st Qu.:-0.8934  \n Median : 0.65262   Median : 0.24668   Median :-0.45652   Median :-0.7265  \n Mean   : 0.07506   Mean   : 0.15540   Mean   :-0.34347   Mean   :-0.6326  \n 3rd Qu.: 0.76007   3rd Qu.: 0.38518   3rd Qu.:-0.03645   3rd Qu.:-0.4672  \n Max.   : 0.99950   Max.   : 0.98772   Max.   : 0.99167   Max.   : 0.9445  \n      f69               f70               f71               f72         \n Min.   :-0.9767   Min.   :-0.9997   Min.   :-0.9566   Min.   :-0.9884  \n 1st Qu.:-0.4486   1st Qu.:-0.9834   1st Qu.:-0.6812   1st Qu.:-0.7171  \n Median :-0.3669   Median :-0.9738   Median :-0.5403   Median :-0.3500  \n Mean   :-0.3071   Mean   :-0.1676   Mean   :-0.4779   Mean   :-0.2817  \n 3rd Qu.:-0.2496   3rd Qu.: 0.9620   3rd Qu.:-0.3458   3rd Qu.: 0.0309  \n Max.   : 0.9748   Max.   : 0.9994   Max.   : 0.9696   Max.   : 0.9866  \n      f73               f74               f75               f76           \n Min.   :-0.9808   Min.   :-0.9833   Min.   :-0.9987   Min.   :-0.941278  \n 1st Qu.:-0.9101   1st Qu.:-0.8275   1st Qu.:-0.9666   1st Qu.:-0.154768  \n Median :-0.8298   Median :-0.8015   Median :-0.8020   Median :-0.074262  \n Mean   :-0.6596   Mean   :-0.5375   Mean   :-0.2511   Mean   :-0.066135  \n 3rd Qu.:-0.5740   3rd Qu.:-0.6473   3rd Qu.: 0.7652   3rd Qu.:-0.006207  \n Max.   : 1.0000   Max.   : 0.9959   Max.   : 0.9990   Max.   : 1.000000  \n      f77               f78               f79               f80         \n Min.   :-1.0000   Min.   :-0.9917   Min.   :-0.9893   Min.   :-0.9923  \n 1st Qu.:-0.9359   1st Qu.:-0.8573   1st Qu.:-0.4042   1st Qu.:-0.8693  \n Median :-0.8641   Median :-0.6479   Median :-0.2635   Median :-0.7446  \n Mean   :-0.5853   Mean   :-0.5178   Mean   :-0.2408   Mean   :-0.6922  \n 3rd Qu.:-0.4899   3rd Qu.:-0.3522   3rd Qu.:-0.1041   3rd Qu.:-0.5968  \n Max.   : 1.0000   Max.   : 0.9828   Max.   : 0.9742   Max.   : 0.9246  \n      f81                f82               f83               f84         \n Min.   :-1.00000   Min.   :-0.9441   Min.   :-0.9964   Min.   :-0.9732  \n 1st Qu.:-0.80862   1st Qu.:-0.7634   1st Qu.:-0.9306   1st Qu.:-0.8374  \n Median :-0.48340   Median :-0.6181   Median :-0.8642   Median :-0.7899  \n Mean   :-0.28319   Mean   :-0.5579   Mean   :-0.6283   Mean   :-0.5156  \n 3rd Qu.:-0.01034   3rd Qu.:-0.4268   3rd Qu.:-0.5065   3rd Qu.:-0.2454  \n Max.   : 0.99953   Max.   : 0.9445   Max.   : 0.9836   Max.   : 0.9856  \n      f85               f86               f87               f88         \n Min.   :-0.9981   Min.   :-0.9966   Min.   :-0.9966   Min.   :-1.0000  \n 1st Qu.:-0.8173   1st Qu.:-0.8764   1st Qu.:-0.8251   1st Qu.:-0.7222  \n Median :-0.5232   Median :-0.5408   Median :-0.6541   Median :-0.3656  \n Mean   :-0.4058   Mean   :-0.4402   Mean   :-0.5605   Mean   :-0.2826  \n 3rd Qu.:-0.1542   3rd Qu.:-0.1467   3rd Qu.:-0.3971   3rd Qu.: 0.1070  \n Max.   : 0.9925   Max.   : 0.9944   Max.   : 0.9828   Max.   : 0.9914  \n      f89               f90               f91               f92         \n Min.   :-0.9953   Min.   :-0.9929   Min.   :-0.9998   Min.   :-0.9471  \n 1st Qu.:-0.8801   1st Qu.:-0.5598   1st Qu.: 0.3073   1st Qu.:-0.6225  \n Median :-0.7002   Median :-0.4179   Median : 0.6004   Median :-0.5937  \n Mean   :-0.6076   Mean   :-0.3731   Mean   : 0.4361   Mean   :-0.4832  \n 3rd Qu.:-0.4397   3rd Qu.:-0.2313   3rd Qu.: 0.8370   3rd Qu.:-0.4618  \n Max.   : 0.9611   Max.   : 1.0000   Max.   : 0.9945   Max.   : 0.9625  \n      f93               f94                f95               f96         \n Min.   :-0.9989   Min.   :-0.96216   Min.   :-0.9986   Min.   :-0.9827  \n 1st Qu.:-0.7759   1st Qu.: 0.05784   1st Qu.:-0.8886   1st Qu.:-0.6528  \n Median :-0.4849   Median : 0.19552   Median :-0.6847   Median :-0.6189  \n Mean   :-0.3715   Mean   : 0.13404   Mean   :-0.5714   Mean   :-0.4985  \n 3rd Qu.:-0.0535   3rd Qu.: 0.26616   3rd Qu.:-0.3572   3rd Qu.:-0.4811  \n Max.   : 0.9933   Max.   : 0.99090   Max.   : 0.9861   Max.   : 0.9834  \n      f97                 f98               f99               f100        \n Min.   :-0.982166   Min.   :-0.9618   Min.   :-0.9224   Min.   :-0.9912  \n 1st Qu.:-0.304459   1st Qu.:-0.8479   1st Qu.:-0.6487   1st Qu.:-0.5211  \n Median :-0.006369   Median :-0.6691   Median :-0.5573   Median :-0.2309  \n Mean   :-0.004125   Mean   :-0.4875   Mean   :-0.4005   Mean   :-0.1606  \n 3rd Qu.: 0.286624   3rd Qu.:-0.1922   3rd Qu.:-0.3563   3rd Qu.: 0.1387  \n Max.   : 1.000000   Max.   : 0.9586   Max.   : 0.9816   Max.   : 1.0000  \n      f101              f102              f103              f104        \n Min.   :-1.0000   Min.   :-0.9546   Min.   :-0.9876   Min.   :-0.9493  \n 1st Qu.:-0.8466   1st Qu.:-0.7525   1st Qu.:-0.8641   1st Qu.:-0.6019  \n Median :-0.6597   Median :-0.6328   Median :-0.8157   Median :-0.5748  \n Mean   :-0.5529   Mean   :-0.5514   Mean   :-0.5556   Mean   :-0.4768  \n 3rd Qu.:-0.3671   3rd Qu.:-0.4198   3rd Qu.:-0.4956   3rd Qu.:-0.4651  \n Max.   : 0.9765   Max.   : 0.9720   Max.   : 0.9998   Max.   : 0.9383  \n      f105              f106              f107              f108        \n Min.   :-0.9981   Min.   :-0.9494   Min.   :-1.0000   Min.   :-0.9824  \n 1st Qu.:-0.6294   1st Qu.:-0.7188   1st Qu.:-0.8115   1st Qu.:-0.8413  \n Median :-0.2839   Median :-0.5867   Median :-0.6839   Median :-0.6661  \n Mean   :-0.2062   Mean   :-0.5075   Mean   :-0.5475   Mean   :-0.4492  \n 3rd Qu.: 0.2040   3rd Qu.:-0.3553   3rd Qu.:-0.3550   3rd Qu.:-0.1266  \n Max.   : 0.9838   Max.   : 0.9732   Max.   : 0.9588   Max.   : 0.9915  \n      f109              f110              f111               f112        \n Min.   :-0.9933   Min.   :-0.9431   Min.   :-0.98028   Min.   :-1.0000  \n 1st Qu.:-0.7327   1st Qu.: 0.4370   1st Qu.:-0.43104   1st Qu.:-0.7794  \n Median :-0.3163   Median : 0.6988   Median :-0.34987   Median :-0.7600  \n Mean   :-0.2432   Mean   : 0.6012   Mean   :-0.15939   Mean   :-0.5765  \n 3rd Qu.: 0.1478   3rd Qu.: 0.8711   3rd Qu.: 0.04573   3rd Qu.:-0.6142  \n Max.   : 0.9926   Max.   : 0.9947   Max.   : 0.98065   Max.   : 0.9959  \n      f113              f114              f115              f116        \n Min.   :-0.9914   Min.   :-1.0000   Min.   :-1.0000   Min.   :-0.9933  \n 1st Qu.:-0.4051   1st Qu.:-0.7224   1st Qu.:-0.5014   1st Qu.:-0.9450  \n Median :-0.2616   Median :-0.6667   Median :-0.4398   Median :-0.8665  \n Mean   :-0.2385   Mean   :-0.5232   Mean   :-0.3748   Mean   :-0.6899  \n 3rd Qu.:-0.1017   3rd Qu.:-0.4402   3rd Qu.:-0.3085   3rd Qu.:-0.6298  \n Max.   : 0.9826   Max.   : 0.9655   Max.   : 0.9454   Max.   : 0.9824  \n      f117               f118        \n Min.   :-0.99272   Min.   :-0.9731  \n 1st Qu.:-0.74767   1st Qu.:-0.6834  \n Median :-0.46602   Median :-0.5502  \n Mean   :-0.35758   Mean   :-0.4913  \n 3rd Qu.:-0.03727   3rd Qu.:-0.3648  \n Max.   : 0.98810   Max.   : 0.9506  "},"metadata":{}}]},{"cell_type":"markdown","source":"Sampled data envelops a decent range of values for each feature.","metadata":{}},{"cell_type":"code","source":"#Filling missing values with a gbm model\n\n#Filling loop\nfor(i in 1:length(df_joint)) {\n    \n   gb_model <- h2o4gpu.gradient_boosting_regressor(loss = 'ls', learning_rate = 0.1, n_estimators = 150, random_state = 1212,\n                                                   max_depth = 4, predictor = 'gpu_predictor',\n                                                   tree_method = 'gpu_hist', backend = 'h2o4gpu') %>%\n   h2o4gpu::fit(na.omit(df_joint)[split, -i], na.omit(df_joint)[split, i])\n                                                    \n\n   df_joint[which(is.na(df_joint[, i])), i] <- predict(gb_model, df_joint[which(is.na(df_joint[, i])), -i])\n}\n\n#Removing models\nrm(gb_model)\ngc()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-09-27T22:45:43.970353Z","iopub.execute_input":"2021-09-27T22:45:43.971608Z","iopub.status.idle":"2021-09-27T22:55:53.447927Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/html":"<table class=\"dataframe\">\n<caption>A matrix: 2 × 6 of type dbl</caption>\n<thead>\n\t<tr><th></th><th scope=col>used</th><th scope=col>(Mb)</th><th scope=col>gc trigger</th><th scope=col>(Mb)</th><th scope=col>max used</th><th scope=col>(Mb)</th></tr>\n</thead>\n<tbody>\n\t<tr><th scope=row>Ncells</th><td>  2730299</td><td> 145.9</td><td>  7885907</td><td> 421.2</td><td> 12321728</td><td> 658.1</td></tr>\n\t<tr><th scope=row>Vcells</th><td>354708686</td><td>2706.3</td><td>853976422</td><td>6515.4</td><td>853972228</td><td>6515.3</td></tr>\n</tbody>\n</table>\n","text/markdown":"\nA matrix: 2 × 6 of type dbl\n\n| <!--/--> | used | (Mb) | gc trigger | (Mb) | max used | (Mb) |\n|---|---|---|---|---|---|---|\n| Ncells |   2730299 |  145.9 |   7885907 |  421.2 |  12321728 |  658.1 |\n| Vcells | 354708686 | 2706.3 | 853976422 | 6515.4 | 853972228 | 6515.3 |\n\n","text/latex":"A matrix: 2 × 6 of type dbl\n\\begin{tabular}{r|llllll}\n  & used & (Mb) & gc trigger & (Mb) & max used & (Mb)\\\\\n\\hline\n\tNcells &   2730299 &  145.9 &   7885907 &  421.2 &  12321728 &  658.1\\\\\n\tVcells & 354708686 & 2706.3 & 853976422 & 6515.4 & 853972228 & 6515.3\\\\\n\\end{tabular}\n","text/plain":"       used      (Mb)   gc trigger (Mb)   max used  (Mb)  \nNcells   2730299  145.9   7885907   421.2  12321728  658.1\nVcells 354708686 2706.3 853976422  6515.4 853972228 6515.3"},"metadata":{}}]},{"cell_type":"code","source":"#Checking imputation results\nhead(df_joint, 20)","metadata":{"execution":{"iopub.status.busy":"2021-09-27T22:55:53.450731Z","iopub.execute_input":"2021-09-27T22:55:53.452002Z","iopub.status.idle":"2021-09-27T22:55:53.504449Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/html":"<table class=\"dataframe\">\n<caption>A data.frame: 20 × 118</caption>\n<thead>\n\t<tr><th></th><th scope=col>f1</th><th scope=col>f2</th><th scope=col>f3</th><th scope=col>f4</th><th scope=col>f5</th><th scope=col>f6</th><th scope=col>f7</th><th scope=col>f8</th><th scope=col>f9</th><th scope=col>f10</th><th scope=col>⋯</th><th scope=col>f109</th><th scope=col>f110</th><th scope=col>f111</th><th scope=col>f112</th><th scope=col>f113</th><th scope=col>f114</th><th scope=col>f115</th><th scope=col>f116</th><th scope=col>f117</th><th scope=col>f118</th></tr>\n\t<tr><th></th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>⋯</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n</thead>\n<tbody>\n\t<tr><th scope=row>1</th><td>-0.07882328</td><td>-0.91325973</td><td>-0.6185370</td><td>-0.858082113</td><td> 0.05283966</td><td>-0.784552676</td><td>-0.87262625</td><td>-0.7954838</td><td>-0.7182980100</td><td>-0.5270653</td><td>⋯</td><td>-0.73706532</td><td> 0.74242185</td><td>-0.30981608</td><td>-0.75888503</td><td>-0.47614398</td><td>-0.7159475</td><td>-0.373427992</td><td>-0.9412715</td><td>-0.91827087</td><td>-0.1778636</td></tr>\n\t<tr><th scope=row>2</th><td>-0.10588359</td><td> 0.18258062</td><td>-0.1371970</td><td>-0.488421722</td><td> 0.65914599</td><td>-0.001483176</td><td>-0.31107336</td><td>-0.8459998</td><td>-0.1265252599</td><td>-0.3509078</td><td>⋯</td><td> 0.74806918</td><td>-0.08624832</td><td> 0.81338729</td><td>-0.77160456</td><td>-0.17329782</td><td>-0.7273469</td><td>-0.298782961</td><td>-0.9010614</td><td>-0.66546205</td><td>-0.6788668</td></tr>\n\t<tr><th scope=row>3</th><td> 0.16552889</td><td>-0.95517126</td><td>-0.5801893</td><td>-0.494641311</td><td> 0.65836389</td><td> 0.082582872</td><td>-0.10719468</td><td>-0.5981638</td><td>-0.7842037143</td><td>-0.2460500</td><td>⋯</td><td>-0.58253106</td><td> 0.86262275</td><td>-0.56228391</td><td>-0.77229841</td><td>-0.06598338</td><td>-0.5819396</td><td>-0.440770791</td><td>-0.9445028</td><td>-0.37500147</td><td>-0.5947123</td></tr>\n\t<tr><th scope=row>4</th><td> 0.07519882</td><td>-0.90233013</td><td>-0.5853507</td><td>-0.846933928</td><td> 0.87162872</td><td> 0.691336356</td><td>-0.88062781</td><td>-0.7022623</td><td>-0.7730372364</td><td>-0.5292671</td><td>⋯</td><td>-0.94762234</td><td> 0.32129439</td><td>-0.16074868</td><td>-0.76795346</td><td>-0.72418153</td><td>-0.7140607</td><td>-0.436713996</td><td>-0.9464030</td><td>-0.22061900</td><td>-0.5210335</td></tr>\n\t<tr><th scope=row>5</th><td>-0.05193891</td><td> 0.93698742</td><td>-0.6214423</td><td>-0.457880119</td><td> 0.25103126</td><td> 0.034120534</td><td>-0.02194147</td><td>-0.9020603</td><td>-0.4614906462</td><td>-0.5260365</td><td>⋯</td><td> 0.66343470</td><td> 0.71612697</td><td>-0.41117449</td><td>-0.76510176</td><td>-0.29632368</td><td>-0.7176655</td><td> 0.181135903</td><td>-0.9464840</td><td>-0.35715652</td><td>-0.7151056</td></tr>\n\t<tr><th scope=row>6</th><td>-0.10366669</td><td>-0.93896241</td><td>-0.6624166</td><td>-0.716948807</td><td> 0.15845890</td><td> 0.028374995</td><td>-0.08072336</td><td>-0.9711506</td><td> 0.2045402294</td><td>-0.3782658</td><td>⋯</td><td>-0.03009121</td><td> 0.14964261</td><td> 0.54087055</td><td>-0.55810061</td><td>-0.45883302</td><td>-0.5837455</td><td>-0.438336714</td><td>-0.9446348</td><td>-0.12377922</td><td>-0.8950364</td></tr>\n\t<tr><th scope=row>7</th><td>-0.45012527</td><td> 0.40494304</td><td> 0.2317761</td><td>-0.877909619</td><td>-0.02213542</td><td>-0.033739550</td><td>-0.45995682</td><td>-0.7293366</td><td>-0.4846175461</td><td>-0.5294679</td><td>⋯</td><td> 0.01720049</td><td> 0.22705711</td><td>-0.42983112</td><td>-0.76391331</td><td> 0.62007371</td><td>-0.6770604</td><td>-0.337931034</td><td>-0.7991618</td><td>-0.79005218</td><td>-0.4301908</td></tr>\n\t<tr><th scope=row>8</th><td>-0.11841087</td><td>-0.01669945</td><td>-0.1391046</td><td>-0.005747413</td><td> 0.72907297</td><td>-0.055483925</td><td> 0.20465725</td><td>-0.8417498</td><td>-0.5047982357</td><td>-0.5196044</td><td>⋯</td><td>-0.51177361</td><td> 0.54205599</td><td>-0.39163608</td><td> 0.05600515</td><td>-0.24395125</td><td>-0.7266750</td><td>-0.297971602</td><td>-0.9468477</td><td>-0.72118311</td><td>-0.6655042</td></tr>\n\t<tr><th scope=row>9</th><td>-0.49242628</td><td> 0.32511968</td><td>-0.5965643</td><td>-0.024777072</td><td>-0.04453196</td><td> 0.129341596</td><td>-0.34925859</td><td>-0.5494074</td><td>-0.6554228152</td><td>-0.5285133</td><td>⋯</td><td>-0.40046571</td><td> 0.79292364</td><td>-0.09196976</td><td>-0.78452008</td><td>-0.09449601</td><td>-0.7269602</td><td>-0.384787018</td><td>-0.9147455</td><td>-0.64972852</td><td>-0.7728148</td></tr>\n\t<tr><th scope=row>10</th><td>-0.01674995</td><td>-0.28370505</td><td>-0.5495573</td><td>-0.828529082</td><td>-0.97336721</td><td> 0.073153018</td><td>-0.81566691</td><td>-0.9775588</td><td>-0.0154548319</td><td>-0.5302216</td><td>⋯</td><td>-0.91450785</td><td> 0.93334537</td><td> 0.71643779</td><td>-0.61829667</td><td>-0.29131305</td><td>-0.7275931</td><td>-0.399391481</td><td> 0.5909975</td><td>-0.21682885</td><td>-0.5176895</td></tr>\n\t<tr><th scope=row>11</th><td>-0.03314800</td><td> 0.09373956</td><td>-0.3817343</td><td>-0.874009823</td><td> 0.70898718</td><td> 0.116582789</td><td>-0.19704852</td><td> 0.1801571</td><td>-0.7796542886</td><td> 0.2398372</td><td>⋯</td><td>-0.31529948</td><td> 0.16683756</td><td> 0.58440269</td><td>-0.70946153</td><td>-0.83644043</td><td>-0.5031109</td><td> 0.004868154</td><td>-0.5141836</td><td>-0.86199303</td><td>-0.2839594</td></tr>\n\t<tr><th scope=row>12</th><td>-0.18417904</td><td> 0.50855383</td><td>-0.5753745</td><td>-0.764985143</td><td>-0.02565488</td><td>-0.054627126</td><td>-0.15245780</td><td>-0.4408120</td><td>-0.7846343038</td><td>-0.5286079</td><td>⋯</td><td>-0.80202495</td><td> 0.65173894</td><td>-0.38072984</td><td> 0.21134195</td><td>-0.53579211</td><td>-0.5801371</td><td>-0.293509128</td><td>-0.9161290</td><td>-0.14228925</td><td>-0.4827405</td></tr>\n\t<tr><th scope=row>13</th><td>-0.17493138</td><td> 0.19263740</td><td>-0.5625856</td><td>-0.145545505</td><td> 0.42618643</td><td>-0.060400116</td><td>-0.88307322</td><td> 0.1069917</td><td>-0.7867451017</td><td>-0.3740275</td><td>⋯</td><td> 0.23844402</td><td> 0.93756595</td><td>-0.28781796</td><td>-0.78128741</td><td>-0.51196029</td><td>-0.7103546</td><td>-0.407910751</td><td>-0.8410323</td><td> 0.01921811</td><td>-0.2230427</td></tr>\n\t<tr><th scope=row>14</th><td>-0.20869519</td><td> 0.33740305</td><td>-0.6031195</td><td>-0.887285078</td><td>-0.96073495</td><td>-0.019914320</td><td>-0.52178221</td><td>-0.8391875</td><td>-0.7852816696</td><td>-0.5240787</td><td>⋯</td><td>-0.74555622</td><td> 0.91483660</td><td>-0.57119453</td><td> 0.06594279</td><td> 0.52777554</td><td>-0.2315243</td><td>-0.391886410</td><td>-0.9120468</td><td>-0.17437330</td><td>-0.4024009</td></tr>\n\t<tr><th scope=row>15</th><td>-0.20586248</td><td> 0.82755038</td><td>-0.6179163</td><td>-0.878792630</td><td> 0.39166731</td><td> 0.363340681</td><td>-0.93469477</td><td>-0.2078868</td><td>-0.0002809969</td><td>-0.5268206</td><td>⋯</td><td>-0.48799911</td><td> 0.77776082</td><td>-0.37154075</td><td> 0.81688667</td><td>-0.25639308</td><td>-0.7244089</td><td>-0.346450304</td><td>-0.9072770</td><td>-0.85196089</td><td>-0.6674083</td></tr>\n\t<tr><th scope=row>16</th><td>-0.14376100</td><td> 0.90076817</td><td>-0.5977576</td><td>-0.114290646</td><td> 0.34004862</td><td> 0.335706859</td><td>-0.75581980</td><td>-0.7735342</td><td>-0.7434038096</td><td>-0.5341130</td><td>⋯</td><td>-0.90262574</td><td> 0.91036666</td><td>-0.13833288</td><td>-0.55918336</td><td>-0.35553493</td><td>-0.4396433</td><td>-0.437525355</td><td>-0.4862260</td><td>-0.85694537</td><td>-0.5110696</td></tr>\n\t<tr><th scope=row>17</th><td>-0.13626927</td><td> 0.35673730</td><td>-0.4952037</td><td>-0.699188743</td><td>-0.07908663</td><td>-0.004132596</td><td> 0.42916799</td><td>-0.5788279</td><td>-0.7841384889</td><td>-0.4454059</td><td>⋯</td><td>-0.26106924</td><td> 0.14314798</td><td>-0.41344855</td><td>-0.26493192</td><td>-0.44131224</td><td>-0.7285337</td><td>-0.239756592</td><td>-0.8106131</td><td> 0.66255920</td><td>-0.4573460</td></tr>\n\t<tr><th scope=row>18</th><td>-0.03610388</td><td> 0.78754592</td><td>-0.5521548</td><td>-0.874234926</td><td> 0.82516479</td><td> 0.078431976</td><td>-0.32890965</td><td>-0.9456940</td><td>-0.7901623822</td><td>-0.5148985</td><td>⋯</td><td>-0.81239070</td><td> 0.53541248</td><td> 0.71332835</td><td>-0.79015603</td><td>-0.19952923</td><td>-0.6937833</td><td>-0.520892495</td><td>-0.9185858</td><td>-0.87402602</td><td>-0.6455763</td></tr>\n\t<tr><th scope=row>19</th><td>-0.48572595</td><td> 0.28400193</td><td>-0.3530084</td><td>-0.880638820</td><td>-0.03653320</td><td>-0.054627126</td><td>-0.35768128</td><td>-0.5416690</td><td>-0.7862701539</td><td>-0.5258365</td><td>⋯</td><td>-0.93194231</td><td> 0.36437481</td><td> 0.62176236</td><td>-0.75629228</td><td>-0.03039572</td><td>-0.7040684</td><td>-0.799269777</td><td>-0.9466798</td><td>-0.81226422</td><td>-0.7045752</td></tr>\n\t<tr><th scope=row>20</th><td>-0.34607291</td><td> 0.60804542</td><td> 0.1902965</td><td>-0.891802839</td><td>-0.08555674</td><td>-0.223657613</td><td>-0.47450189</td><td>-0.4496412</td><td>-0.7344561858</td><td>-0.5287437</td><td>⋯</td><td>-0.62433370</td><td> 0.93944176</td><td>-0.30313311</td><td>-0.77459457</td><td>-0.62198370</td><td>-0.7033619</td><td>-0.309330629</td><td>-0.9436164</td><td>-0.74905099</td><td>-0.6390860</td></tr>\n</tbody>\n</table>\n","text/markdown":"\nA data.frame: 20 × 118\n\n| <!--/--> | f1 &lt;dbl&gt; | f2 &lt;dbl&gt; | f3 &lt;dbl&gt; | f4 &lt;dbl&gt; | f5 &lt;dbl&gt; | f6 &lt;dbl&gt; | f7 &lt;dbl&gt; | f8 &lt;dbl&gt; | f9 &lt;dbl&gt; | f10 &lt;dbl&gt; | ⋯ ⋯ | f109 &lt;dbl&gt; | f110 &lt;dbl&gt; | f111 &lt;dbl&gt; | f112 &lt;dbl&gt; | f113 &lt;dbl&gt; | f114 &lt;dbl&gt; | f115 &lt;dbl&gt; | f116 &lt;dbl&gt; | f117 &lt;dbl&gt; | f118 &lt;dbl&gt; |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| 1 | -0.07882328 | -0.91325973 | -0.6185370 | -0.858082113 |  0.05283966 | -0.784552676 | -0.87262625 | -0.7954838 | -0.7182980100 | -0.5270653 | ⋯ | -0.73706532 |  0.74242185 | -0.30981608 | -0.75888503 | -0.47614398 | -0.7159475 | -0.373427992 | -0.9412715 | -0.91827087 | -0.1778636 |\n| 2 | -0.10588359 |  0.18258062 | -0.1371970 | -0.488421722 |  0.65914599 | -0.001483176 | -0.31107336 | -0.8459998 | -0.1265252599 | -0.3509078 | ⋯ |  0.74806918 | -0.08624832 |  0.81338729 | -0.77160456 | -0.17329782 | -0.7273469 | -0.298782961 | -0.9010614 | -0.66546205 | -0.6788668 |\n| 3 |  0.16552889 | -0.95517126 | -0.5801893 | -0.494641311 |  0.65836389 |  0.082582872 | -0.10719468 | -0.5981638 | -0.7842037143 | -0.2460500 | ⋯ | -0.58253106 |  0.86262275 | -0.56228391 | -0.77229841 | -0.06598338 | -0.5819396 | -0.440770791 | -0.9445028 | -0.37500147 | -0.5947123 |\n| 4 |  0.07519882 | -0.90233013 | -0.5853507 | -0.846933928 |  0.87162872 |  0.691336356 | -0.88062781 | -0.7022623 | -0.7730372364 | -0.5292671 | ⋯ | -0.94762234 |  0.32129439 | -0.16074868 | -0.76795346 | -0.72418153 | -0.7140607 | -0.436713996 | -0.9464030 | -0.22061900 | -0.5210335 |\n| 5 | -0.05193891 |  0.93698742 | -0.6214423 | -0.457880119 |  0.25103126 |  0.034120534 | -0.02194147 | -0.9020603 | -0.4614906462 | -0.5260365 | ⋯ |  0.66343470 |  0.71612697 | -0.41117449 | -0.76510176 | -0.29632368 | -0.7176655 |  0.181135903 | -0.9464840 | -0.35715652 | -0.7151056 |\n| 6 | -0.10366669 | -0.93896241 | -0.6624166 | -0.716948807 |  0.15845890 |  0.028374995 | -0.08072336 | -0.9711506 |  0.2045402294 | -0.3782658 | ⋯ | -0.03009121 |  0.14964261 |  0.54087055 | -0.55810061 | -0.45883302 | -0.5837455 | -0.438336714 | -0.9446348 | -0.12377922 | -0.8950364 |\n| 7 | -0.45012527 |  0.40494304 |  0.2317761 | -0.877909619 | -0.02213542 | -0.033739550 | -0.45995682 | -0.7293366 | -0.4846175461 | -0.5294679 | ⋯ |  0.01720049 |  0.22705711 | -0.42983112 | -0.76391331 |  0.62007371 | -0.6770604 | -0.337931034 | -0.7991618 | -0.79005218 | -0.4301908 |\n| 8 | -0.11841087 | -0.01669945 | -0.1391046 | -0.005747413 |  0.72907297 | -0.055483925 |  0.20465725 | -0.8417498 | -0.5047982357 | -0.5196044 | ⋯ | -0.51177361 |  0.54205599 | -0.39163608 |  0.05600515 | -0.24395125 | -0.7266750 | -0.297971602 | -0.9468477 | -0.72118311 | -0.6655042 |\n| 9 | -0.49242628 |  0.32511968 | -0.5965643 | -0.024777072 | -0.04453196 |  0.129341596 | -0.34925859 | -0.5494074 | -0.6554228152 | -0.5285133 | ⋯ | -0.40046571 |  0.79292364 | -0.09196976 | -0.78452008 | -0.09449601 | -0.7269602 | -0.384787018 | -0.9147455 | -0.64972852 | -0.7728148 |\n| 10 | -0.01674995 | -0.28370505 | -0.5495573 | -0.828529082 | -0.97336721 |  0.073153018 | -0.81566691 | -0.9775588 | -0.0154548319 | -0.5302216 | ⋯ | -0.91450785 |  0.93334537 |  0.71643779 | -0.61829667 | -0.29131305 | -0.7275931 | -0.399391481 |  0.5909975 | -0.21682885 | -0.5176895 |\n| 11 | -0.03314800 |  0.09373956 | -0.3817343 | -0.874009823 |  0.70898718 |  0.116582789 | -0.19704852 |  0.1801571 | -0.7796542886 |  0.2398372 | ⋯ | -0.31529948 |  0.16683756 |  0.58440269 | -0.70946153 | -0.83644043 | -0.5031109 |  0.004868154 | -0.5141836 | -0.86199303 | -0.2839594 |\n| 12 | -0.18417904 |  0.50855383 | -0.5753745 | -0.764985143 | -0.02565488 | -0.054627126 | -0.15245780 | -0.4408120 | -0.7846343038 | -0.5286079 | ⋯ | -0.80202495 |  0.65173894 | -0.38072984 |  0.21134195 | -0.53579211 | -0.5801371 | -0.293509128 | -0.9161290 | -0.14228925 | -0.4827405 |\n| 13 | -0.17493138 |  0.19263740 | -0.5625856 | -0.145545505 |  0.42618643 | -0.060400116 | -0.88307322 |  0.1069917 | -0.7867451017 | -0.3740275 | ⋯ |  0.23844402 |  0.93756595 | -0.28781796 | -0.78128741 | -0.51196029 | -0.7103546 | -0.407910751 | -0.8410323 |  0.01921811 | -0.2230427 |\n| 14 | -0.20869519 |  0.33740305 | -0.6031195 | -0.887285078 | -0.96073495 | -0.019914320 | -0.52178221 | -0.8391875 | -0.7852816696 | -0.5240787 | ⋯ | -0.74555622 |  0.91483660 | -0.57119453 |  0.06594279 |  0.52777554 | -0.2315243 | -0.391886410 | -0.9120468 | -0.17437330 | -0.4024009 |\n| 15 | -0.20586248 |  0.82755038 | -0.6179163 | -0.878792630 |  0.39166731 |  0.363340681 | -0.93469477 | -0.2078868 | -0.0002809969 | -0.5268206 | ⋯ | -0.48799911 |  0.77776082 | -0.37154075 |  0.81688667 | -0.25639308 | -0.7244089 | -0.346450304 | -0.9072770 | -0.85196089 | -0.6674083 |\n| 16 | -0.14376100 |  0.90076817 | -0.5977576 | -0.114290646 |  0.34004862 |  0.335706859 | -0.75581980 | -0.7735342 | -0.7434038096 | -0.5341130 | ⋯ | -0.90262574 |  0.91036666 | -0.13833288 | -0.55918336 | -0.35553493 | -0.4396433 | -0.437525355 | -0.4862260 | -0.85694537 | -0.5110696 |\n| 17 | -0.13626927 |  0.35673730 | -0.4952037 | -0.699188743 | -0.07908663 | -0.004132596 |  0.42916799 | -0.5788279 | -0.7841384889 | -0.4454059 | ⋯ | -0.26106924 |  0.14314798 | -0.41344855 | -0.26493192 | -0.44131224 | -0.7285337 | -0.239756592 | -0.8106131 |  0.66255920 | -0.4573460 |\n| 18 | -0.03610388 |  0.78754592 | -0.5521548 | -0.874234926 |  0.82516479 |  0.078431976 | -0.32890965 | -0.9456940 | -0.7901623822 | -0.5148985 | ⋯ | -0.81239070 |  0.53541248 |  0.71332835 | -0.79015603 | -0.19952923 | -0.6937833 | -0.520892495 | -0.9185858 | -0.87402602 | -0.6455763 |\n| 19 | -0.48572595 |  0.28400193 | -0.3530084 | -0.880638820 | -0.03653320 | -0.054627126 | -0.35768128 | -0.5416690 | -0.7862701539 | -0.5258365 | ⋯ | -0.93194231 |  0.36437481 |  0.62176236 | -0.75629228 | -0.03039572 | -0.7040684 | -0.799269777 | -0.9466798 | -0.81226422 | -0.7045752 |\n| 20 | -0.34607291 |  0.60804542 |  0.1902965 | -0.891802839 | -0.08555674 | -0.223657613 | -0.47450189 | -0.4496412 | -0.7344561858 | -0.5287437 | ⋯ | -0.62433370 |  0.93944176 | -0.30313311 | -0.77459457 | -0.62198370 | -0.7033619 | -0.309330629 | -0.9436164 | -0.74905099 | -0.6390860 |\n\n","text/latex":"A data.frame: 20 × 118\n\\begin{tabular}{r|lllllllllllllllllllll}\n  & f1 & f2 & f3 & f4 & f5 & f6 & f7 & f8 & f9 & f10 & ⋯ & f109 & f110 & f111 & f112 & f113 & f114 & f115 & f116 & f117 & f118\\\\\n  & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & ⋯ & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n\\hline\n\t1 & -0.07882328 & -0.91325973 & -0.6185370 & -0.858082113 &  0.05283966 & -0.784552676 & -0.87262625 & -0.7954838 & -0.7182980100 & -0.5270653 & ⋯ & -0.73706532 &  0.74242185 & -0.30981608 & -0.75888503 & -0.47614398 & -0.7159475 & -0.373427992 & -0.9412715 & -0.91827087 & -0.1778636\\\\\n\t2 & -0.10588359 &  0.18258062 & -0.1371970 & -0.488421722 &  0.65914599 & -0.001483176 & -0.31107336 & -0.8459998 & -0.1265252599 & -0.3509078 & ⋯ &  0.74806918 & -0.08624832 &  0.81338729 & -0.77160456 & -0.17329782 & -0.7273469 & -0.298782961 & -0.9010614 & -0.66546205 & -0.6788668\\\\\n\t3 &  0.16552889 & -0.95517126 & -0.5801893 & -0.494641311 &  0.65836389 &  0.082582872 & -0.10719468 & -0.5981638 & -0.7842037143 & -0.2460500 & ⋯ & -0.58253106 &  0.86262275 & -0.56228391 & -0.77229841 & -0.06598338 & -0.5819396 & -0.440770791 & -0.9445028 & -0.37500147 & -0.5947123\\\\\n\t4 &  0.07519882 & -0.90233013 & -0.5853507 & -0.846933928 &  0.87162872 &  0.691336356 & -0.88062781 & -0.7022623 & -0.7730372364 & -0.5292671 & ⋯ & -0.94762234 &  0.32129439 & -0.16074868 & -0.76795346 & -0.72418153 & -0.7140607 & -0.436713996 & -0.9464030 & -0.22061900 & -0.5210335\\\\\n\t5 & -0.05193891 &  0.93698742 & -0.6214423 & -0.457880119 &  0.25103126 &  0.034120534 & -0.02194147 & -0.9020603 & -0.4614906462 & -0.5260365 & ⋯ &  0.66343470 &  0.71612697 & -0.41117449 & -0.76510176 & -0.29632368 & -0.7176655 &  0.181135903 & -0.9464840 & -0.35715652 & -0.7151056\\\\\n\t6 & -0.10366669 & -0.93896241 & -0.6624166 & -0.716948807 &  0.15845890 &  0.028374995 & -0.08072336 & -0.9711506 &  0.2045402294 & -0.3782658 & ⋯ & -0.03009121 &  0.14964261 &  0.54087055 & -0.55810061 & -0.45883302 & -0.5837455 & -0.438336714 & -0.9446348 & -0.12377922 & -0.8950364\\\\\n\t7 & -0.45012527 &  0.40494304 &  0.2317761 & -0.877909619 & -0.02213542 & -0.033739550 & -0.45995682 & -0.7293366 & -0.4846175461 & -0.5294679 & ⋯ &  0.01720049 &  0.22705711 & -0.42983112 & -0.76391331 &  0.62007371 & -0.6770604 & -0.337931034 & -0.7991618 & -0.79005218 & -0.4301908\\\\\n\t8 & -0.11841087 & -0.01669945 & -0.1391046 & -0.005747413 &  0.72907297 & -0.055483925 &  0.20465725 & -0.8417498 & -0.5047982357 & -0.5196044 & ⋯ & -0.51177361 &  0.54205599 & -0.39163608 &  0.05600515 & -0.24395125 & -0.7266750 & -0.297971602 & -0.9468477 & -0.72118311 & -0.6655042\\\\\n\t9 & -0.49242628 &  0.32511968 & -0.5965643 & -0.024777072 & -0.04453196 &  0.129341596 & -0.34925859 & -0.5494074 & -0.6554228152 & -0.5285133 & ⋯ & -0.40046571 &  0.79292364 & -0.09196976 & -0.78452008 & -0.09449601 & -0.7269602 & -0.384787018 & -0.9147455 & -0.64972852 & -0.7728148\\\\\n\t10 & -0.01674995 & -0.28370505 & -0.5495573 & -0.828529082 & -0.97336721 &  0.073153018 & -0.81566691 & -0.9775588 & -0.0154548319 & -0.5302216 & ⋯ & -0.91450785 &  0.93334537 &  0.71643779 & -0.61829667 & -0.29131305 & -0.7275931 & -0.399391481 &  0.5909975 & -0.21682885 & -0.5176895\\\\\n\t11 & -0.03314800 &  0.09373956 & -0.3817343 & -0.874009823 &  0.70898718 &  0.116582789 & -0.19704852 &  0.1801571 & -0.7796542886 &  0.2398372 & ⋯ & -0.31529948 &  0.16683756 &  0.58440269 & -0.70946153 & -0.83644043 & -0.5031109 &  0.004868154 & -0.5141836 & -0.86199303 & -0.2839594\\\\\n\t12 & -0.18417904 &  0.50855383 & -0.5753745 & -0.764985143 & -0.02565488 & -0.054627126 & -0.15245780 & -0.4408120 & -0.7846343038 & -0.5286079 & ⋯ & -0.80202495 &  0.65173894 & -0.38072984 &  0.21134195 & -0.53579211 & -0.5801371 & -0.293509128 & -0.9161290 & -0.14228925 & -0.4827405\\\\\n\t13 & -0.17493138 &  0.19263740 & -0.5625856 & -0.145545505 &  0.42618643 & -0.060400116 & -0.88307322 &  0.1069917 & -0.7867451017 & -0.3740275 & ⋯ &  0.23844402 &  0.93756595 & -0.28781796 & -0.78128741 & -0.51196029 & -0.7103546 & -0.407910751 & -0.8410323 &  0.01921811 & -0.2230427\\\\\n\t14 & -0.20869519 &  0.33740305 & -0.6031195 & -0.887285078 & -0.96073495 & -0.019914320 & -0.52178221 & -0.8391875 & -0.7852816696 & -0.5240787 & ⋯ & -0.74555622 &  0.91483660 & -0.57119453 &  0.06594279 &  0.52777554 & -0.2315243 & -0.391886410 & -0.9120468 & -0.17437330 & -0.4024009\\\\\n\t15 & -0.20586248 &  0.82755038 & -0.6179163 & -0.878792630 &  0.39166731 &  0.363340681 & -0.93469477 & -0.2078868 & -0.0002809969 & -0.5268206 & ⋯ & -0.48799911 &  0.77776082 & -0.37154075 &  0.81688667 & -0.25639308 & -0.7244089 & -0.346450304 & -0.9072770 & -0.85196089 & -0.6674083\\\\\n\t16 & -0.14376100 &  0.90076817 & -0.5977576 & -0.114290646 &  0.34004862 &  0.335706859 & -0.75581980 & -0.7735342 & -0.7434038096 & -0.5341130 & ⋯ & -0.90262574 &  0.91036666 & -0.13833288 & -0.55918336 & -0.35553493 & -0.4396433 & -0.437525355 & -0.4862260 & -0.85694537 & -0.5110696\\\\\n\t17 & -0.13626927 &  0.35673730 & -0.4952037 & -0.699188743 & -0.07908663 & -0.004132596 &  0.42916799 & -0.5788279 & -0.7841384889 & -0.4454059 & ⋯ & -0.26106924 &  0.14314798 & -0.41344855 & -0.26493192 & -0.44131224 & -0.7285337 & -0.239756592 & -0.8106131 &  0.66255920 & -0.4573460\\\\\n\t18 & -0.03610388 &  0.78754592 & -0.5521548 & -0.874234926 &  0.82516479 &  0.078431976 & -0.32890965 & -0.9456940 & -0.7901623822 & -0.5148985 & ⋯ & -0.81239070 &  0.53541248 &  0.71332835 & -0.79015603 & -0.19952923 & -0.6937833 & -0.520892495 & -0.9185858 & -0.87402602 & -0.6455763\\\\\n\t19 & -0.48572595 &  0.28400193 & -0.3530084 & -0.880638820 & -0.03653320 & -0.054627126 & -0.35768128 & -0.5416690 & -0.7862701539 & -0.5258365 & ⋯ & -0.93194231 &  0.36437481 &  0.62176236 & -0.75629228 & -0.03039572 & -0.7040684 & -0.799269777 & -0.9466798 & -0.81226422 & -0.7045752\\\\\n\t20 & -0.34607291 &  0.60804542 &  0.1902965 & -0.891802839 & -0.08555674 & -0.223657613 & -0.47450189 & -0.4496412 & -0.7344561858 & -0.5287437 & ⋯ & -0.62433370 &  0.93944176 & -0.30313311 & -0.77459457 & -0.62198370 & -0.7033619 & -0.309330629 & -0.9436164 & -0.74905099 & -0.6390860\\\\\n\\end{tabular}\n","text/plain":"   f1          f2          f3         f4           f5          f6          \n1  -0.07882328 -0.91325973 -0.6185370 -0.858082113  0.05283966 -0.784552676\n2  -0.10588359  0.18258062 -0.1371970 -0.488421722  0.65914599 -0.001483176\n3   0.16552889 -0.95517126 -0.5801893 -0.494641311  0.65836389  0.082582872\n4   0.07519882 -0.90233013 -0.5853507 -0.846933928  0.87162872  0.691336356\n5  -0.05193891  0.93698742 -0.6214423 -0.457880119  0.25103126  0.034120534\n6  -0.10366669 -0.93896241 -0.6624166 -0.716948807  0.15845890  0.028374995\n7  -0.45012527  0.40494304  0.2317761 -0.877909619 -0.02213542 -0.033739550\n8  -0.11841087 -0.01669945 -0.1391046 -0.005747413  0.72907297 -0.055483925\n9  -0.49242628  0.32511968 -0.5965643 -0.024777072 -0.04453196  0.129341596\n10 -0.01674995 -0.28370505 -0.5495573 -0.828529082 -0.97336721  0.073153018\n11 -0.03314800  0.09373956 -0.3817343 -0.874009823  0.70898718  0.116582789\n12 -0.18417904  0.50855383 -0.5753745 -0.764985143 -0.02565488 -0.054627126\n13 -0.17493138  0.19263740 -0.5625856 -0.145545505  0.42618643 -0.060400116\n14 -0.20869519  0.33740305 -0.6031195 -0.887285078 -0.96073495 -0.019914320\n15 -0.20586248  0.82755038 -0.6179163 -0.878792630  0.39166731  0.363340681\n16 -0.14376100  0.90076817 -0.5977576 -0.114290646  0.34004862  0.335706859\n17 -0.13626927  0.35673730 -0.4952037 -0.699188743 -0.07908663 -0.004132596\n18 -0.03610388  0.78754592 -0.5521548 -0.874234926  0.82516479  0.078431976\n19 -0.48572595  0.28400193 -0.3530084 -0.880638820 -0.03653320 -0.054627126\n20 -0.34607291  0.60804542  0.1902965 -0.891802839 -0.08555674 -0.223657613\n   f7          f8         f9            f10        ⋯ f109        f110       \n1  -0.87262625 -0.7954838 -0.7182980100 -0.5270653 ⋯ -0.73706532  0.74242185\n2  -0.31107336 -0.8459998 -0.1265252599 -0.3509078 ⋯  0.74806918 -0.08624832\n3  -0.10719468 -0.5981638 -0.7842037143 -0.2460500 ⋯ -0.58253106  0.86262275\n4  -0.88062781 -0.7022623 -0.7730372364 -0.5292671 ⋯ -0.94762234  0.32129439\n5  -0.02194147 -0.9020603 -0.4614906462 -0.5260365 ⋯  0.66343470  0.71612697\n6  -0.08072336 -0.9711506  0.2045402294 -0.3782658 ⋯ -0.03009121  0.14964261\n7  -0.45995682 -0.7293366 -0.4846175461 -0.5294679 ⋯  0.01720049  0.22705711\n8   0.20465725 -0.8417498 -0.5047982357 -0.5196044 ⋯ -0.51177361  0.54205599\n9  -0.34925859 -0.5494074 -0.6554228152 -0.5285133 ⋯ -0.40046571  0.79292364\n10 -0.81566691 -0.9775588 -0.0154548319 -0.5302216 ⋯ -0.91450785  0.93334537\n11 -0.19704852  0.1801571 -0.7796542886  0.2398372 ⋯ -0.31529948  0.16683756\n12 -0.15245780 -0.4408120 -0.7846343038 -0.5286079 ⋯ -0.80202495  0.65173894\n13 -0.88307322  0.1069917 -0.7867451017 -0.3740275 ⋯  0.23844402  0.93756595\n14 -0.52178221 -0.8391875 -0.7852816696 -0.5240787 ⋯ -0.74555622  0.91483660\n15 -0.93469477 -0.2078868 -0.0002809969 -0.5268206 ⋯ -0.48799911  0.77776082\n16 -0.75581980 -0.7735342 -0.7434038096 -0.5341130 ⋯ -0.90262574  0.91036666\n17  0.42916799 -0.5788279 -0.7841384889 -0.4454059 ⋯ -0.26106924  0.14314798\n18 -0.32890965 -0.9456940 -0.7901623822 -0.5148985 ⋯ -0.81239070  0.53541248\n19 -0.35768128 -0.5416690 -0.7862701539 -0.5258365 ⋯ -0.93194231  0.36437481\n20 -0.47450189 -0.4496412 -0.7344561858 -0.5287437 ⋯ -0.62433370  0.93944176\n   f111        f112        f113        f114       f115         f116      \n1  -0.30981608 -0.75888503 -0.47614398 -0.7159475 -0.373427992 -0.9412715\n2   0.81338729 -0.77160456 -0.17329782 -0.7273469 -0.298782961 -0.9010614\n3  -0.56228391 -0.77229841 -0.06598338 -0.5819396 -0.440770791 -0.9445028\n4  -0.16074868 -0.76795346 -0.72418153 -0.7140607 -0.436713996 -0.9464030\n5  -0.41117449 -0.76510176 -0.29632368 -0.7176655  0.181135903 -0.9464840\n6   0.54087055 -0.55810061 -0.45883302 -0.5837455 -0.438336714 -0.9446348\n7  -0.42983112 -0.76391331  0.62007371 -0.6770604 -0.337931034 -0.7991618\n8  -0.39163608  0.05600515 -0.24395125 -0.7266750 -0.297971602 -0.9468477\n9  -0.09196976 -0.78452008 -0.09449601 -0.7269602 -0.384787018 -0.9147455\n10  0.71643779 -0.61829667 -0.29131305 -0.7275931 -0.399391481  0.5909975\n11  0.58440269 -0.70946153 -0.83644043 -0.5031109  0.004868154 -0.5141836\n12 -0.38072984  0.21134195 -0.53579211 -0.5801371 -0.293509128 -0.9161290\n13 -0.28781796 -0.78128741 -0.51196029 -0.7103546 -0.407910751 -0.8410323\n14 -0.57119453  0.06594279  0.52777554 -0.2315243 -0.391886410 -0.9120468\n15 -0.37154075  0.81688667 -0.25639308 -0.7244089 -0.346450304 -0.9072770\n16 -0.13833288 -0.55918336 -0.35553493 -0.4396433 -0.437525355 -0.4862260\n17 -0.41344855 -0.26493192 -0.44131224 -0.7285337 -0.239756592 -0.8106131\n18  0.71332835 -0.79015603 -0.19952923 -0.6937833 -0.520892495 -0.9185858\n19  0.62176236 -0.75629228 -0.03039572 -0.7040684 -0.799269777 -0.9466798\n20 -0.30313311 -0.77459457 -0.62198370 -0.7033619 -0.309330629 -0.9436164\n   f117        f118      \n1  -0.91827087 -0.1778636\n2  -0.66546205 -0.6788668\n3  -0.37500147 -0.5947123\n4  -0.22061900 -0.5210335\n5  -0.35715652 -0.7151056\n6  -0.12377922 -0.8950364\n7  -0.79005218 -0.4301908\n8  -0.72118311 -0.6655042\n9  -0.64972852 -0.7728148\n10 -0.21682885 -0.5176895\n11 -0.86199303 -0.2839594\n12 -0.14228925 -0.4827405\n13  0.01921811 -0.2230427\n14 -0.17437330 -0.4024009\n15 -0.85196089 -0.6674083\n16 -0.85694537 -0.5110696\n17  0.66255920 -0.4573460\n18 -0.87402602 -0.6455763\n19 -0.81226422 -0.7045752\n20 -0.74905099 -0.6390860"},"metadata":{}}]},{"cell_type":"code","source":"#Adding number of missing rows as a column\ndf_joint <- df_joint %>%\n    cbind(nas_sum = nas_per_row)","metadata":{"execution":{"iopub.status.busy":"2021-09-27T22:55:53.507174Z","iopub.execute_input":"2021-09-27T22:55:53.508561Z","iopub.status.idle":"2021-09-27T22:55:53.519022Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# Model creation","metadata":{}},{"cell_type":"markdown","source":"Preparing data for modelling and performing a grid search of hyperparameters.","metadata":{}},{"cell_type":"code","source":"#Isolating training data from the joint dataset\nrf_train <- df_joint[1:nrow(df_train), ]\n\n#Adding claim and NAs per row variables to the training set\nrf_train <- rf_train %>%\n    mutate(claim = df_train$claim)\n\nholdout_prop <- round(0.2 *nrow(rf_train))\nholdout_sampler <- sample(nrow(rf_train), holdout_prop)\n\nrf_train_holdout <- rf_train[holdout_sampler, ]\n\nrf_train <- rf_train %>%\n    anti_join(rf_train_holdout, by = colnames(rf_train_holdout))\n\n\n#Removing original train and test sets\nrm(df_train, df_test)\ngc()","metadata":{"execution":{"iopub.status.busy":"2021-09-27T22:55:53.521748Z","iopub.execute_input":"2021-09-27T22:55:53.522994Z","iopub.status.idle":"2021-09-27T22:55:56.240025Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/html":"<table class=\"dataframe\">\n<caption>A matrix: 2 × 6 of type dbl</caption>\n<thead>\n\t<tr><th></th><th scope=col>used</th><th scope=col>(Mb)</th><th scope=col>gc trigger</th><th scope=col>(Mb)</th><th scope=col>max used</th><th scope=col>(Mb)</th></tr>\n</thead>\n<tbody>\n\t<tr><th scope=row>Ncells</th><td>  2734987</td><td> 146.1</td><td>  7885907</td><td> 421.2</td><td> 12321728</td><td> 658.1</td></tr>\n\t<tr><th scope=row>Vcells</th><td>296912617</td><td>2265.3</td><td>853976422</td><td>6515.4</td><td>853972228</td><td>6515.3</td></tr>\n</tbody>\n</table>\n","text/markdown":"\nA matrix: 2 × 6 of type dbl\n\n| <!--/--> | used | (Mb) | gc trigger | (Mb) | max used | (Mb) |\n|---|---|---|---|---|---|---|\n| Ncells |   2734987 |  146.1 |   7885907 |  421.2 |  12321728 |  658.1 |\n| Vcells | 296912617 | 2265.3 | 853976422 | 6515.4 | 853972228 | 6515.3 |\n\n","text/latex":"A matrix: 2 × 6 of type dbl\n\\begin{tabular}{r|llllll}\n  & used & (Mb) & gc trigger & (Mb) & max used & (Mb)\\\\\n\\hline\n\tNcells &   2734987 &  146.1 &   7885907 &  421.2 &  12321728 &  658.1\\\\\n\tVcells & 296912617 & 2265.3 & 853976422 & 6515.4 & 853972228 & 6515.3\\\\\n\\end{tabular}\n","text/plain":"       used      (Mb)   gc trigger (Mb)   max used  (Mb)  \nNcells   2734987  146.1   7885907   421.2  12321728  658.1\nVcells 296912617 2265.3 853976422  6515.4 853972228 6515.3"},"metadata":{}}]},{"cell_type":"markdown","source":"***Grid search***","metadata":{}},{"cell_type":"code","source":"#Deciding whether to perform a grid search or not\ngrid_search <- FALSE\n\n\nif(grid_search == TRUE) {\n#Grid search\ndf_holder <- rf_train\nk_grid <- 4\n\n#Splitting holdout set into 10 parts\nset.seed(2077)\nrf_train <- df_holder %>%\n  group_by((row_number()-1) %/% (n()/k_grid)) %>%\n  nest %>% pull(data)    \n\n#Isolating holdout\ngrid_holdout <- rf_train[[k_grid]]\n\n#Merging back list of dataframes\nrf_train <- rf_train %>%\n    bind_rows()\n\n#Excluding holdout\nrf_train <- rf_train %>%\n    anti_join(grid_holdout, by = colnames(grid_holdout))\n\n\n\n#Brute grid search loops\nfor(i in c(1)) {\n    for(j in c(1)) {\n        for(k in c(1)){\n    \n    \n    tic(\"Creating and fitting the model\")\n\n    rf_model_train <- h2o4gpu.random_forest_classifier(n_estimators = 20, criterion = \"entropy\",\n    max_depth = 7, min_samples_split = 3, min_samples_leaf = 1, colsample_bytree = 0.3,\n    max_features = \"auto\", random_state = 1212, subsample = 1, \n    min_impurity_decrease = 2, min_impurity_split = 1, verbose = 0, tree_method = \"gpu_hist\",\n    predictor = \"gpu_predictor\", backend = \"h2o4gpu\") %>% fit(x = rf_train[, -length(rf_train)],\n                                                              y = rf_train[, length(rf_train)])\n    toc()\n    \n    \n    \n    \n    tic(\"Predicting on validation data\")\n    \n    grid_pred <- predict(rf_model_train,  grid_holdout[, -(length(grid_holdout))], type = \"prob\")\n    \n    toc()\n    \n    \n    cat(sprintf(\"colsample_bytree: %s\\n\", i))\n    cat(sprintf(\"subsample: %s\\n\", j))\n    #cat(sprintf(\"min_samples_leaf: %s\\n\", k))\n    cat(sprintf(\"Grid search AUC: %s\\n\",AUC(predictions = grid_pred[, 2], labels = grid_holdout$claim)))\n\n            }\n    }\n}\n\n    \n}","metadata":{"execution":{"iopub.status.busy":"2021-09-27T22:55:56.242937Z","iopub.execute_input":"2021-09-27T22:55:56.244175Z","iopub.status.idle":"2021-09-27T22:55:56.257469Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"Creating a GPU based random forest classifier model to complement gradient boosting classifier model and NN. Model is checked and developed in a two-step process.\nModels are used in a mix similar to [stacking](http://www.kaggle.com/general/18793).\nSince it's a GPU based model, other strategies could bring better results and not cause a strain in computation time. ","metadata":{}},{"cell_type":"markdown","source":"***Simple cv***","metadata":{}},{"cell_type":"code","source":"cv <- FALSE\n\nif(cv == TRUE) {\n#Splitting data into 10 parts for training\nk_cv <- 10\n\nauc_vec <- c()\npred_list <- list()\n\ndf_holder <- rf_train\n    \nfor(i in 1:k_cv) {\nset.seed(2077)\nrf_train <- df_holder %>%\n  group_by((row_number()-1) %/% (n()/k_cv)) %>%\n  nest %>% pull(data)    \n\n#Isolating holdout\nk_inner_holdout <- rf_train[[i]]\n\n#Merging back list of dataframes\nrf_train <- rf_train %>%\n    bind_rows()\n\n\n#Excluding inner holdout\nrf_train <- rf_train %>%\n    anti_join(k_inner_holdout, by = colnames(rf_train))\n    \n    tic(\"Creating and fitting the model\")\n\n    rf_model_train <- h2o4gpu.random_forest_classifier(n_estimators = 20, criterion = \"entropy\",\n    max_depth = 7, min_samples_split = 3, min_samples_leaf = 1, colsample_bytree = 0.3,\n    subsample = 1, random_state = 1212, max_features = \"auto\",\n    verbose = 0, tree_method = \"gpu_hist\",\n    predictor = \"gpu_predictor\", backend = \"h2o4gpu\") %>% fit(x = rf_train[, -length(rf_train)],\n                                                              y = rf_train[, length(rf_train)])\n    toc()\n    \n    \n    \n    \n    tic(\"Predicting on validation data\")\n    \n    pred_list[[i]] <- predict(rf_model_train,  k_inner_holdout[, -length(k_inner_holdout)], type = \"prob\")\n    \n    toc()\n\n    print(i)\n    cat(sprintf(\"Split AUC: %s\\n\",AUC(predictions = pred_list[[i]][, 2], labels = k_inner_holdout$claim)))\n    auc_vec[i] <- AUC(predictions = pred_list[[i]][, 2], labels = k_inner_holdout$claim)\n}\n\n#Mean AUC for CV\ncat(sprintf(\"Mean AUC: %s\\n\", mean(auc_vec)))\n\n\n\n#Removing rf_model\nrm(rf_model_train)\ngc()\n\n}","metadata":{"execution":{"iopub.status.busy":"2021-09-27T22:55:56.260264Z","iopub.execute_input":"2021-09-27T22:55:56.261512Z","iopub.status.idle":"2021-09-27T22:55:56.273658Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"1: \nCreating and fitting the model: 5.031 sec elapsed\nPredicting on validation data: 0.266 sec elapsed\nSplit AUC: 0.807258974201438\n\n2: \nCreating and fitting the model: 3.455 sec elapsed\nPredicting on validation data: 0.253 sec elapsed\nSplit AUC: 0.806473453160845\n\n3:\nCreating and fitting the model: 3.398 sec elapsed\nPredicting on validation data: 0.226 sec elapsed\nSplit AUC: 0.805474566846782\n\n4:\nCreating and fitting the model: 2.943 sec elapsed\nPredicting on validation data: 0.211 sec elapsed\nSplit AUC: 0.802924746024181\n\n5:\nCreating and fitting the model: 2.613 sec elapsed\nPredicting on validation data: 0.199 sec elapsed\nSplit AUC: 0.806402562997926\n\n6:\nCreating and fitting the model: 2.398 sec elapsed\nPredicting on validation data: 0.177 sec elapsed\nSplit AUC: 0.805294984207043\n\n7:\nCreating and fitting the model: 2.225 sec elapsed\nPredicting on validation data: 0.168 sec elapsed\nSplit AUC: 0.804420483910461\n\n8:\nCreating and fitting the model: 2.012 sec elapsed\nPredicting on validation data: 0.164 sec elapsed\nSplit AUC: 0.806963979164778\n\n9:\nCreating and fitting the model: 1.69 sec elapsed\nPredicting on validation data: 0.143 sec elapsed\nSplit AUC: 0.80400435784427\n\n10:\nCreating and fitting the model: 1.394 sec elapsed\nPredicting on validation data: 0.132 sec elapsed\nSplit AUC: 0.802016595796033\n\n\nMean AUC: **0.805123470415376**","metadata":{}},{"cell_type":"markdown","source":"# Impure CV predictions function","metadata":{}},{"cell_type":"markdown","source":"Function generates semi-out-of-fold predictions. Data is split into n parts and k parts out of those n are chosen as holdout. Final predictions share half the knowledge of holdout data with one another.","metadata":{}},{"cell_type":"code","source":"impure_cv_predictor <- function(dataset, model, package, test, is_validation) {\n    #Spltting the data and setting up cv\nk_num <- 10\n\ntemp_dataset <- dataset\n\ncombinations <- combn(rep(1:10), 2)\nlist_comb <- lapply(seq_len(ncol(combinations)), function(i) combinations[,i])\n\npredictions <- list()\n\n\ndf_of_predictions <- data.frame(id = 1:nrow(test))                    \n\n#Pseudo cv loop\nfor(i in 1:length(list_comb)) {\n\nset.seed(2040)\ndataset <- temp_dataset %>%\n  group_by((row_number()-1) %/% (n()/k_num)) %>%\n  nest %>% pull(data)\n\ntemp_exclusion <- dataset[[list_comb[[i]][1]]] %>%\n    rbind(dataset[[list_comb[[i]][2]]])\n    \n    dataset <- dataset %>%\n        bind_rows()\n    \ndataset <- dataset %>%\n    anti_join(temp_exclusion, by = colnames(dataset))\n    \n    \n    if(package == 'keras'){\n        \n        if(is_validation == TRUE) {\n        model %>% keras::fit(\n  x = list(data.matrix(dataset[, length(dataset)-1], rownames.force = NA),\n           data.matrix(dataset[, -c((length(dataset)-1):length(dataset))], rownames.force = NA)),\n  data.matrix(dataset[, length(dataset)], rownames.force = NA),\n  epochs = 10,\n  batch_size = 478)\n        \n        \n    predictions <- predict(model, list(data.matrix(test[, length(test)-1],\n                                                               rownames.force = NA),\n                                         data.matrix(test[, -c((length(test)-1):length(test))],\n                                                               rownames.force = NA)))\n     df_of_predictions <- df_of_predictions %>%\n        cbind(predictions)\n        \n        \n        \n        }\n         if(is_validation == FALSE) {\n        \n                model %>% keras::fit(\n  x = list(data.matrix(dataset[, length(dataset)-1], rownames.force = NA),\n           data.matrix(dataset[, -c((length(dataset)-1):length(dataset))], rownames.force = NA)),\n  data.matrix(dataset[, length(dataset)], rownames.force = NA),\n  epochs = 10,\n  batch_size = 478)\n        \n        \n    predictions <- predict(model, list(data.matrix(test[, length(test)],\n                                                               rownames.force = NA),\n                                         data.matrix(test[, -length(test)],\n                                                               rownames.force = NA)))\n     df_of_predictions <- df_of_predictions %>%\n        cbind(predictions)\n        \n    }\n    \n        }\n    \n    if(package == 'h2o4gpu') {\n        \n        if(is_validation == TRUE) {\n        model_fit <- model %>% h2o4gpu::fit(x = dataset[, -length(dataset)],\n                                   y = dataset[, length(dataset)])\n        \n        predictions <- predict(model_fit, test[, -length(test)], type = \"prob\")\n    \n        df_of_predictions <- df_of_predictions %>%\n            cbind(predictions)\n        \n                            }\n    \n        if(is_validation == FALSE) {\n           model_fit <- model %>% h2o4gpu::fit(x = dataset[, -length(dataset)],\n                                   y = dataset[, length(dataset)])\n        \n           predictions <- predict(model_fit, test, type = \"prob\")\n    \n           df_of_predictions <- df_of_predictions %>%\n              cbind(predictions)\n        \n        }\n}\n    \n    }\n\ndf_of_predictions <- df_of_predictions[, -1]\n       \nreturn(df_of_predictions)       \n                                        }","metadata":{"execution":{"iopub.status.busy":"2021-09-27T22:55:56.276488Z","iopub.execute_input":"2021-09-27T22:55:56.277717Z","iopub.status.idle":"2021-09-27T22:55:56.288569Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"***Random forest model***","metadata":{}},{"cell_type":"code","source":"#Predicting with rf on test and holdout data - model assembly\n\n       rf_model <- h2o4gpu.random_forest_classifier(n_estimators = 20, criterion = \"entropy\",\n    max_depth = 7, min_samples_split = 3, min_samples_leaf = 1, colsample_bytree = 0.3,\n    max_features = \"auto\", random_state = 1212, subsample = 1, \n    min_impurity_decrease = 2, min_impurity_split = 1, verbose = 0, tree_method = \"gpu_hist\",\n    predictor = \"gpu_predictor\", backend = \"h2o4gpu\")\n\nrf_validation_pred <- impure_cv_predictor(dataset = rf_train, model = rf_model,\n                                         package = 'h2o4gpu', test = rf_train_holdout, is_validation = TRUE)\n\nrf_test_pred <- impure_cv_predictor(dataset = rf_train, model = rf_model, package = 'h2o4gpu',\n                                    test = df_joint[957919:nrow(df_joint), ], is_validation = FALSE)\n\n#Removing used model\nrm(rf_model)\ngc()","metadata":{"execution":{"iopub.status.busy":"2021-09-27T22:55:56.291322Z","iopub.execute_input":"2021-09-27T22:55:56.292624Z","iopub.status.idle":"2021-09-27T23:04:32.632342Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/html":"<table class=\"dataframe\">\n<caption>A matrix: 2 × 6 of type dbl</caption>\n<thead>\n\t<tr><th></th><th scope=col>used</th><th scope=col>(Mb)</th><th scope=col>gc trigger</th><th scope=col>(Mb)</th><th scope=col>max used</th><th scope=col>(Mb)</th></tr>\n</thead>\n<tbody>\n\t<tr><th scope=row>Ncells</th><td>  2749431</td><td> 146.9</td><td>  7885907</td><td> 421.2</td><td> 12321728</td><td> 658.1</td></tr>\n\t<tr><th scope=row>Vcells</th><td>358607753</td><td>2736.0</td><td>853976422</td><td>6515.4</td><td>853972228</td><td>6515.3</td></tr>\n</tbody>\n</table>\n","text/markdown":"\nA matrix: 2 × 6 of type dbl\n\n| <!--/--> | used | (Mb) | gc trigger | (Mb) | max used | (Mb) |\n|---|---|---|---|---|---|---|\n| Ncells |   2749431 |  146.9 |   7885907 |  421.2 |  12321728 |  658.1 |\n| Vcells | 358607753 | 2736.0 | 853976422 | 6515.4 | 853972228 | 6515.3 |\n\n","text/latex":"A matrix: 2 × 6 of type dbl\n\\begin{tabular}{r|llllll}\n  & used & (Mb) & gc trigger & (Mb) & max used & (Mb)\\\\\n\\hline\n\tNcells &   2749431 &  146.9 &   7885907 &  421.2 &  12321728 &  658.1\\\\\n\tVcells & 358607753 & 2736.0 & 853976422 & 6515.4 & 853972228 & 6515.3\\\\\n\\end{tabular}\n","text/plain":"       used      (Mb)   gc trigger (Mb)   max used  (Mb)  \nNcells   2749431  146.9   7885907   421.2  12321728  658.1\nVcells 358607753 2736.0 853976422  6515.4 853972228 6515.3"},"metadata":{}}]},{"cell_type":"markdown","source":"**Gradient boosting classifier**","metadata":{}},{"cell_type":"code","source":"#Predicting with gb on test and holdout data - model assembly\n\n    gb_model_class <- h2o4gpu.gradient_boosting_classifier(loss = \"deviance\", learning_rate = 0.3,\n                n_estimators = 200, subsample = 1, criterion = \"friedman_mse\",\n                min_samples_split = 2, min_samples_leaf = 1,\n                min_weight_fraction_leaf = 0, max_depth = 5, min_impurity_decrease = 0,random_state = 1212,\n                max_features = \"auto\", verbose = 0,\n                warm_start = FALSE, presort = \"auto\", colsample_bytree = 0.21,\n                tree_method = \"gpu_hist\", \n                predictor = \"gpu_predictor\", objective = \"binary:logistic\",\n                booster = \"gbtree\", gamma = 0, colsample_bylevel = 1, reg_alpha = 0,\n                reg_lambda = 1, scale_pos_weight = 1, base_score = 0.5,\n                backend = \"h2o4gpu\")\n\n\ngb_validation_pred <- impure_cv_predictor(dataset = rf_train, model = gb_model_class,\n                                         package = 'h2o4gpu', test = rf_train_holdout, is_validation = TRUE)\n\ngb_test_pred <- impure_cv_predictor(dataset = rf_train, model = gb_model_class, package = 'h2o4gpu',\n                                    test = df_joint[957919:nrow(df_joint), ], is_validation = FALSE)\n\n#Removing used model\nrm(gb_model_class)\ngc()","metadata":{"execution":{"iopub.status.busy":"2021-09-27T23:05:15.456427Z","iopub.execute_input":"2021-09-27T23:05:15.457850Z","iopub.status.idle":"2021-09-27T23:17:50.907140Z"},"trusted":true},"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/html":"<table class=\"dataframe\">\n<caption>A matrix: 2 × 6 of type dbl</caption>\n<thead>\n\t<tr><th></th><th scope=col>used</th><th scope=col>(Mb)</th><th scope=col>gc trigger</th><th scope=col>(Mb)</th><th scope=col>max used</th><th scope=col>(Mb)</th></tr>\n</thead>\n<tbody>\n\t<tr><th scope=row>Ncells</th><td>  2751019</td><td> 147.0</td><td>   7885907</td><td> 421.2</td><td>  12321728</td><td> 658.1</td></tr>\n\t<tr><th scope=row>Vcells</th><td>420266958</td><td>3206.4</td><td>1024851706</td><td>7819.0</td><td>1024851235</td><td>7819.0</td></tr>\n</tbody>\n</table>\n","text/markdown":"\nA matrix: 2 × 6 of type dbl\n\n| <!--/--> | used | (Mb) | gc trigger | (Mb) | max used | (Mb) |\n|---|---|---|---|---|---|---|\n| Ncells |   2751019 |  147.0 |    7885907 |  421.2 |   12321728 |  658.1 |\n| Vcells | 420266958 | 3206.4 | 1024851706 | 7819.0 | 1024851235 | 7819.0 |\n\n","text/latex":"A matrix: 2 × 6 of type dbl\n\\begin{tabular}{r|llllll}\n  & used & (Mb) & gc trigger & (Mb) & max used & (Mb)\\\\\n\\hline\n\tNcells &   2751019 &  147.0 &    7885907 &  421.2 &   12321728 &  658.1\\\\\n\tVcells & 420266958 & 3206.4 & 1024851706 & 7819.0 & 1024851235 & 7819.0\\\\\n\\end{tabular}\n","text/plain":"       used      (Mb)   gc trigger (Mb)   max used   (Mb)  \nNcells   2751019  147.0    7885907  421.2   12321728  658.1\nVcells 420266958 3206.4 1024851706 7819.0 1024851235 7819.0"},"metadata":{}}]},{"cell_type":"code","source":"#NN model\nfeatures_input <- layer_input(shape = c(dim(rf_train)[[-1]]-2), name = 'features_input')\n\nnas_input <- layer_input(shape = c(1), name = 'nas_input')\n\n\nfeatures_output <- features_input %>%\n    layer_dense(units = 48, activation = 'hard_sigmoid') %>%\n    layer_dropout(rate = 0.3) %>%\n    layer_dense(units = 32, activation = 'hard_sigmoid') %>%\n    layer_dropout(rate = 0.3) %>%\n    layer_dense(units = 24, activation = 'hard_sigmoid') %>%\n    layer_dropout(rate = 0.3) %>%\n    layer_dense(units = 16, activation = 'hard_sigmoid') %>%\n    layer_batch_normalization(\n    momentum = 0.99,\n    epsilon = 0.001,\n    center = TRUE,\n    scale = TRUE) %>%\n    layer_flatten()\n\nnas_output <- nas_input %>%\n    layer_dense(units = 48, activation = 'hard_sigmoid') %>%\n    layer_dropout(rate = 0.18) %>%\n    layer_dense(units = 32, activation = 'hard_sigmoid') %>%\n    layer_dropout(rate = 0.18) %>%\n    layer_dense(units = 32, activation = 'hard_sigmoid') %>%\n    layer_dropout(rate = 0.18) %>%\n    layer_dense(units = 16, activation = 'hard_sigmoid')\n\n\nnn_output <- layer_concatenate(c(nas_output, features_output)) %>%\n    layer_dense(units = 32, activation = 'hard_sigmoid') %>%\n    layer_dense(units = 16, activation = 'hard_sigmoid') %>%\n    layer_dense(units = 16, activation = 'hard_sigmoid') %>%\n    layer_dense(units = 1, activation = 'hard_sigmoid', name = 'nn_output')\n\n\n\nnn_model <- keras_model(\n        inputs = c(nas_input, features_input),\n        outputs = c(nn_output))\n\n\nnn_model %>% compile(\n  optimizer = 'adam',\n  loss = \"binary_crossentropy\",\n  metrics = 'AUC')\n\n\nnn_model %>% summary()","metadata":{"execution":{"iopub.status.busy":"2021-09-27T23:17:50.910042Z","iopub.execute_input":"2021-09-27T23:17:50.911228Z","iopub.status.idle":"2021-09-27T23:17:56.784136Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Model: \"functional_1\"\n________________________________________________________________________________\nLayer (type)              Output Shape      Param #  Connected to               \n================================================================================\nfeatures_input (InputLaye [(None, 118)]     0                                   \n________________________________________________________________________________\ndense_3 (Dense)           (None, 48)        5712     features_input[0][0]       \n________________________________________________________________________________\nnas_input (InputLayer)    [(None, 1)]       0                                   \n________________________________________________________________________________\ndropout_2 (Dropout)       (None, 48)        0        dense_3[0][0]              \n________________________________________________________________________________\ndense_7 (Dense)           (None, 48)        96       nas_input[0][0]            \n________________________________________________________________________________\ndense_2 (Dense)           (None, 32)        1568     dropout_2[0][0]            \n________________________________________________________________________________\ndropout_5 (Dropout)       (None, 48)        0        dense_7[0][0]              \n________________________________________________________________________________\ndropout_1 (Dropout)       (None, 32)        0        dense_2[0][0]              \n________________________________________________________________________________\ndense_6 (Dense)           (None, 32)        1568     dropout_5[0][0]            \n________________________________________________________________________________\ndense_1 (Dense)           (None, 24)        792      dropout_1[0][0]            \n________________________________________________________________________________\ndropout_4 (Dropout)       (None, 32)        0        dense_6[0][0]              \n________________________________________________________________________________\ndropout (Dropout)         (None, 24)        0        dense_1[0][0]              \n________________________________________________________________________________\ndense_5 (Dense)           (None, 32)        1056     dropout_4[0][0]            \n________________________________________________________________________________\ndense (Dense)             (None, 16)        400      dropout[0][0]              \n________________________________________________________________________________\ndropout_3 (Dropout)       (None, 32)        0        dense_5[0][0]              \n________________________________________________________________________________\nbatch_normalization (Batc (None, 16)        64       dense[0][0]                \n________________________________________________________________________________\ndense_4 (Dense)           (None, 16)        528      dropout_3[0][0]            \n________________________________________________________________________________\nflatten (Flatten)         (None, 16)        0        batch_normalization[0][0]  \n________________________________________________________________________________\nconcatenate (Concatenate) (None, 32)        0        dense_4[0][0]              \n                                                     flatten[0][0]              \n________________________________________________________________________________\ndense_10 (Dense)          (None, 32)        1056     concatenate[0][0]          \n________________________________________________________________________________\ndense_9 (Dense)           (None, 16)        528      dense_10[0][0]             \n________________________________________________________________________________\ndense_8 (Dense)           (None, 16)        272      dense_9[0][0]              \n________________________________________________________________________________\nnn_output (Dense)         (None, 1)         17       dense_8[0][0]              \n================================================================================\nTotal params: 13,657\nTrainable params: 13,625\nNon-trainable params: 32\n________________________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"#Neural network predictions\n\nnn_validation_pred <- impure_cv_predictor(dataset = rf_train, model = nn_model, package = 'keras',\n                                          is_validation = TRUE, test = rf_train_holdout)\n\nnn_test_pred <- impure_cv_predictor(dataset = rf_train, model = nn_model, package = 'keras',\n                                    test = df_joint[957919:nrow(df_joint), ], is_validation = FALSE)\n","metadata":{"execution":{"iopub.status.busy":"2021-09-27T23:17:56.787511Z","iopub.execute_input":"2021-09-27T23:17:56.788928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Second level model - NN with concatenated layers\n\nrf_input <- layer_input(shape = c(dim(rf_predictions_df)[[-1]]), name = 'rf_input')\n\ngb_input <- layer_input(shape = c(dim(gb_predictions_df)[[-1]]), name = 'gb_input')\n\nnn_input <- layer_input(shape = c(dim(nn_predictions_df)[[-1]]), name = 'nn_input')                     \n\nrf_output <- rf_input %>%\n    layer_dense(units = 24, activation = 'sigmoid') %>%\n    layer_dropout(rate = 0.2) %>%\n    layer_dense(units = 16, activation = 'sigmoid') %>%\n    layer_dropout(rate = 0.2) %>%\n    layer_dense(units = 12, activation = 'sigmoid') %>%\n    layer_dropout(rate = 0.2) %>%\n    layer_dense(units = 8, activation = 'sigmoid') %>%\n    layer_batch_normalization(\n    momentum = 0.99,\n    epsilon = 0.001,\n    center = TRUE,\n    scale = TRUE)\n                        \n                        \ngb_output <- gb_input %>%\n    layer_dense(units = 24, activation = 'sigmoid') %>%\n    layer_dropout(rate = 0.15) %>%\n    layer_dense(units = 16, activation = 'sigmoid') %>%\n    layer_dropout(rate = 0.15) %>%\n    layer_dense(units = 16, activation = 'sigmoid') %>%\n    layer_dropout(rate = 0.15) %>%\n    layer_dense(units = 8, activation = 'sigmoid') %>%\n    layer_batch_normalization(\n    momentum = 0.99,\n    epsilon = 0.001,\n    center = TRUE,\n    scale = TRUE)\n\nnn_concat_output <- nn_input %>%\n    layer_dense(units = 16, activation = 'sigmoid') %>%\n    layer_dropout(rate = 0.25) %>%\n    layer_dense(units = 12, activation = 'sigmoid') %>%\n    layer_dropout(rate = 0.25) %>%\n    layer_dense(units = 8, activation = 'sigmoid') %>%\n    layer_dropout(rate = 0.25) %>%\n    layer_dense(units = 4, activation = 'sigmoid') %>%\n    layer_batch_normalization(\n    momentum = 0.99,\n    epsilon = 0.001,\n    center = TRUE,\n    scale = TRUE)\n\n                        \nconc_output <- layer_concatenate(c(rf_output, gb_output, nn_concat_output)) %>%\n    layer_dense(units = 16, activation = 'sigmoid') %>%\n    layer_dense(units = 12, activation = 'sigmoid') %>%\n    layer_dense(units = 4, activation = 'sigmoid') %>%\n    layer_dense(units = 1, activation = 'sigmoid', name = 'conc_output')\n                        \n                        \n\nconc_model <- keras_model(\n        inputs = c(rf_input, gb_input, nn_input),\n        outputs = c(conc_output))\n\n\nconc_model %>% compile(\n  optimizer = 'adam',\n  loss = \"binary_crossentropy\",\n  metrics = 'AUC')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Fitting the model\nconc_history <- conc_model %>% keras::fit(\n  x = list(data.matrix(rf_validation_pred, rownames.force = NA),\n           data.matrix(gb_validation_pred, rownames.force = NA),\n           data.matrix(nn_validation_pred, rownames.force = NA)),\n  data.matrix(rf_train_holdout[, length(rf_train_holdout)], rownames.force = NA),\n  epochs = 10,\n  batch_size = 128)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Predicting on 'predicted' data\nconc_nn_pred <- predict(conc_model, list(data.matrix(rf_test_pred, rownames.force = NA),\n                                    data.matrix(gb_test_pred, rownames.force = NA),\n                                    data.matrix(nn_test_pred, rownames.force = NA)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Creating a df for submission\ndf_pred <- data.frame(id = 957919:1451392, claim = conc_nn_pred)\n\n\n#Creating a submission file\nwrite.csv(df_pred, \"./conc_nn_submission.csv\", quote = FALSE, row.names = FALSE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}